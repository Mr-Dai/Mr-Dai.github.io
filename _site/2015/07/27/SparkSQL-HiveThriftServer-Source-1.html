<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <link href="/css/bootstrap.min.css" rel="stylesheet">
    <link href="/css/default.css" rel="stylesheet">
	<link href="/css/post.css" rel="stylesheet">
    <script type="text/javascript" src="/js/jquery-2.1.1.min.js"></script>
    
    <script type="text/javascript" src="/js/syntaxhighlighters/shCore.js"></script>
    <link href="/css/syntaxhighlighters/shCore.css" rel="stylesheet" type="text/css" />
    <link href="/css/syntaxhighlighters/shThemeDefault.css" rel="stylesheet" type="text/css" />
	
	<link rel="shortcut icon" href="/img/favicon.ico" >
    
    <title>SparkSQL Hive ThriftServer 源码解析（一）：Intro - Robert Peng</title>
</head>
<body>
    <div id="main_wrapper">
    <div id="banner_wrapper">
    <h1 style="padding-bottom:0">Robert P.'s Blog</h1>
	<p style="margin-top:8px; color: #999999; font-size: 22px">Blog is how I learn.</p>
    </div>
    <div id="content_wrapper">
        <div id="right_wrapper" style="margin-left: 10px; width: 30%; font-size:15px; line-height:25px;">
            <ul id="JumpList">
                <li><h4>跳转目录</h4></li>
            </ul>
        </div>
		<div id="left_wrapper">
			<p style="color: rgb(50, 93, 114); font-size:30px; font-weight:300; margin-top: 0; padding-top: 30px;">SparkSQL Hive ThriftServer 源码解析（一）：Intro</p>
			<p style="font-size:15px; margin-top: 0; color: #BBB"><em>By Robert Peng</em>, 27 Jul 2015</p>
			<!-- content starts here -->
			<script type="text/javascript" src="/js/syntaxhighlighters/shBrushBash.js"></script>
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushJava.js"></script>
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushScala.js"></script>

<h2 class="jump">写作背景</h2>
<p>
	本人的第一个实习工作是在一家小公司做研发工作。这家公司以Spark平台为基础开发出了一款大数据分析平台作为其核心产品。
	工作性质使然，我需要掌握Spark的运行原理，工作更要求我去阅读和理解Spark的源代码。
	这篇博文只是我的一时心血来潮：一来可以巩固我所学的知识，二来也希望我的理解能够帮到后来的人。
</p>
<p>
	首先，必须承认Spark本身是一个十分复杂的系统，Scala作为Spark的主要开发语言，其相较于Java差得多的可读性也为源代码阅读带来了相当大的挑战。
	SparkSQL作为Spark的一个模块，也相当的复杂，我并不认为自己有能力在如此短时间的源代码阅读过程中就能够把SparkSQL模块琢磨透。因此这篇文章更像是
	我开的一个坑：我会慢慢地更新这篇文章，不断地修正我对SparkSQL本身的理解。同时也希望读者不要过于相信我的一家之言，因为我很有可能是错的。
	如果您在某些问题上有比我更好的见解，随时欢迎您用电子邮件与我联系进行深入交流。
</p>
<p>本文中所出现的源代码皆为写作时最新的Spark 1.4.1中的源代码。</p>

<h2 class="jump">SparkSQL模块综述</h2>
<p>
	Spark的主要开发语言是Scala，同时包含部分Java代码。以模块为单位的话，不去管其他模块，在Spark 1.4.1中的SparkSQL模块全部由Scala编写而成，因此
	本文要求读者拥有阅读Scala源代码的能力。对于并未学习过Scala语言的读者，我由衷地建议您在学习过Scala后再在此文的指导下阅读SparkSQL的源代码。
</p>
<p>
	在利用IntelliJ构建完Spark源码阅读环境后，打开项目的sql文件夹，就会看到有四个文件夹：catalyst、core、hive、hive-thriftserver。这四个文件夹
	分别属于SparkSQL的四个项目：spark-catalyst_2.10、spark-sql_2.10、spark-hive_2.10、spark-hive-thriftserver_2.10。初来乍到很容易被这四个文件夹
	吓晕，因为这四个文件夹下面各自都是一个Maven项目，而光从项目名称上很难看出每个项目到底有什么用途。但不用担心，Apache Spark长期以来一直都在
	<a href="https://github.com/apache/spark">Github</a>上开源，因此sql文件夹以及这四个文件夹下都有README.md文件对项目进行详细说明。
</p>
<p>我们首先看一下<code>sql/README.md</code>。其中有这么一段话：
<pre>
This module provides support for executing relational queries expressed in either SQL or a LINQ-like Scala DSL.

Spark SQL is broken up into four subprojects:
 - Catalyst (sql/catalyst) - An implementation-agnostic framework for manipulating trees of relational operators and expressions.
 - Execution (sql/core) - A query planner / execution engine for translating Catalyst’s logical query plans into Spark RDDs.
   This component also includes a new public interface, SQLContext, that allows users to execute SQL or LINQ statements against
   existing RDDs and Parquet files.
 - Hive Support (sql/hive) - Includes an extension of SQLContext called HiveContext that allows users to write queries using a subset of
   HiveQL and access data from a Hive Metastore using Hive SerDes.  There are also wrappers that allows users to run queries that include
   Hive UDFs, UDAFs, and UDTFs.
 - HiveServer and CLI support (sql/hive-thriftserver) - Includes support for the SQL CLI (bin/spark-sql) and a HiveServer2 (for JDBC/ODBC)
   compatible server.
</pre>
<p>（如果你不喜欢这个样式，还可以在<a href="https://github.com/apache/spark/tree/master/sql">这里</a>看到由Github渲染过的Readme说明）</p>
<p>
	对于有一定英文基础的人来讲，上述说明并不难理解，但在这里我会再详细解释一下，可能会有点啰嗦。
</p>
<p>
	熟悉SparkSQL的人都知道，SparkSQL接受用户输入的SQL语句，并将其
	解析为对应的Spark操作，执行计算后返回结果。SparkSQL的存在让大量的DBA找到存在感，同时也大大加快了各大企业开发Spark应用的速度，原因在于用Java、
	Python、Scala或R写出来的Spark应用脚本的专用性过强，业务逻辑中的每个运算都需要程序员明确写明步骤，而且这样的运算脚本复用性极差，基本上完全无法复用。
	这样的脚本也被部分开发者称为“一次性的Spark脚本”。毋庸置疑，开发这样的脚本，效率是极低的。SparkSQL模块接收程序员输入的SQL语句并自动转化为对应的Spark运算。
	突然之间，程序员们从维护一次性脚本变成了维护SQL语句，而这正是DBA们的专业领域。维护成本大大降低，开发速度大大提高，加之大部分企业以关系型数据库组织自己的
	业务数据，SparkSQL可以说为企业的传统数据业务提供了无缝转接。以笔者所在公司为例，该公司核心产品的大部分业务逻辑都是通过JDBC发送SQL语句至SparkSQL模块完成查询的。
</p>
<p>
	说回SparkSQL的模块划分。首先最引人注目的，应该是Hive Support和HiveServer项目。众所周知，Apache Hive的功能在于将数据文件以表的形式存储在HDFS之上。原本的Hive与
	Hadoop紧密结合，Hive通过JDBC接收SQL语句并将SQL解析为Hadoop的MapReduce操作，处理完毕后返回结果。Spark本身也可以使用Hadoop HDFS存储数据文件，所以SparkSQL对Hive
	做出兼容似乎也是合情合理，只是这次Hive不再将SQL语句变成MapReduce操作，而是变成Spark操作。其中，hive-thriftserver项目除了实现了一个HiveServer2（负责JDBC/ODBC连接）
	还实现了CLI support功能，也就是bin/spark-sql这个SQL Shell。而hive这个项目，不妨理解为hive-core。它实现了SparkSQL与Hive之间的桥梁：HiveContext。它继承自SQLContext，
	而且并未改写其中最核心的sql方法。
</p>
<p>
	Catalyst，“催化剂”，即SQL解析器。它接收SQL语句，将其解析为抽象语法树并进一步解析为对应的Spark操作树，供执行模块执行。执行模块Execution，或称sql core，负责管理和
	调度接收到的每个查询，同时也是这些查询的执行引擎。在Catalyst为每个查询生成了对应的查询计划以后，Execution便将执行对应操作，将这些查询计划变成结果RDD（DataFrame）。
	SparkSQL核心类SQLContext正是位于这个项目之中。
</p>
<p>以上便是SparkSQL四个模块的介绍。接下来我们先从Hive ThriftServer开始。</p>

<h2 class="jump">Start ThriftServer</h2>
<p>
	正如上文所述，Spark ThriftServer项目负责接受JDBC连接，将JDBC客户端发来的SQL语句转发至SparkSQL，并在SparkSQL计算完毕后将结果DataFrame以ResultSet
	的形式返回给客户端。ThriftServer本身基于Apache Hive项目进行开发，大量使用了Hive本身的代码，仅在转发至Hadoop MapReduce的部分通过继承的方式，改而将
	SQL语句转发至SparkSQL。因此，我们不难将整个ThriftServer分成两个模块进行理解：
</p>
<ul>
	<li>
		首先是Hive Server模块，角色比较类似于Tomcat这样的Servlet Container。
		Hive Server负责监听套接字（0.0.0.0:10000）、在接收到客户端请求后维护与客户端的连接、接收客户端的请求转发至
		执行模块并将执行模块的结果（可能已经是ResultSet，也可能仍然是DataFrame）以ResultSet的形式返回。
	</li>
	<li>
		然后是Hive Service模块，角色类似于Servlet。它包含真正的业务逻辑或对真正的业务逻辑的直接调用。它正是上文提到的
		thriftserver的执行模块，它调用SparkSQL的接口（极有可能就是SQLContext.sql方法）并将结果返回给Server模块。
	</li>
</ul>
<p>
	当然，上述只是对ThriftServer模块划分及分工的大概猜测，也有可能并不准确，加之如数据表缓存等与上下文（context）有关
	的功能也暂时无法确定具体是哪个模块负责。But, after all, talk is cheap, show me the code.
</p>
<p>
	首先，我们从ThriftServer的启动入口开始。在Spark的sbin文件夹下有一个名为<code>start-thriftserver.sh</code>的脚本文件，
	通过执行该脚本便可启动Thrift Server。我们不妨先看看它的内容：
</p>
<pre class="brush: bash" id="start-thriftserver">
# Usage打印以及注释等无关语句已被删去

CLASS="org.apache.spark.sql.hive.thriftserver.HiveThriftServer2"

exec "$FWDIR"/sbin/spark-daemon.sh submit $CLASS 1 "$@"
</pre>
<p>
	可见，该脚本利用<code>spark-daemon.sh</code>，在后台调用了<code>spark-submit</code>接口，执行了<code>org.apache.spark.sql.hive.thriftserver.HiveThriftServer2</code>。
</p>
<p>
	入口确定，于是我们去找HiveThriftServer2的main函数吧！
</p>
<pre class="brush: scala" id="HiveThriftServer2#main">
/**
 * The main entry point for the Spark SQL port of HiveServer2.  Starts up a `SparkSQLContext` and a
 * `HiveThriftServer2` thrift server.
 */
object HiveThriftServer2 extends Logging {
  var LOG = LogFactory.getLog(classOf[HiveServer2])
  var uiTab: Option[ThriftServerTab] = _
  var listener: HiveThriftServer2Listener = _

  // ...

  def main(args: Array[String]) {
    // 使用ServerOptionsProcessor解析用户启动服务器时输入的参数
    val optionsProcessor = new ServerOptionsProcessor("HiveThriftServer2")
    if (!optionsProcessor.process(args)) {
      System.exit(-1)
    }

    // 通过SparkSQLEnv初始化SparkContext和HiveContext
    logInfo("Starting SparkContext")
    SparkSQLEnv.init()

    // 为Spark添加一个关闭时的任务
    Utils.addShutdownHook { () =&gt;
      SparkSQLEnv.stop()    // 关闭SparkSQLEnv
      uiTab.foreach(_.detach())
    }

    try {
      // 启动HiveThriftServer2, 包括一个SparkSQLCLIService和一个ThriftCliService
      val server = new HiveThriftServer2(SparkSQLEnv.hiveContext)
      server.init(SparkSQLEnv.hiveContext.hiveconf)  // 调用其所有service的init(HiveContext)方法
      server.start()      							 // 调用其所有service的start方法
      logInfo("HiveThriftServer2 started")

      // 为启动的HiveThriftServer2设置一个listener
      listener = new HiveThriftServer2Listener(server, SparkSQLEnv.hiveContext.conf)
      SparkSQLEnv.sparkContext.addSparkListener(listener)

      // Web UI页面
      uiTab = if (SparkSQLEnv.sparkContext.getConf.getBoolean("spark.ui.enabled", true)) {
        Some(new ThriftServerTab(SparkSQLEnv.sparkContext))
      } else {
        None
      }
    } catch {
      case e: Exception =&gt;
        logError("Error starting HiveThriftServer2", e)
        System.exit(-1)
    }
  }
  
  // ...
  
}
</pre>
<p>
	可见，main函数创建了一个HiveThriftServer2实例，传入HiveContext与HiveConf实例对其进行初始化并启动。于是我们来看看HiveThriftServer2：
</p>
<pre class="brush: scala" id="HiveThriftServer2">
/**
 * 继承自Apache Hive的HiveServer2。
 * 注意HiveServer2里，cliService和thriftCLIService为private，
 * 所以该类在初始化时利用反射机制对这两个变量进行设置
 */
private[hive] class HiveThriftServer2(hiveContext: HiveContext)
  extends HiveServer2 with ReflectedCompositeService {

  /** 使用给定的HiveConf初始化HiveThriftServer2 */
  override def init(hiveConf: HiveConf) {
    // 初始化SparkSqlCliService
    val sparkSqlCliService = new SparkSQLCLIService(hiveContext)
    // super.cliService = sparkSqlCliService
    setSuperField(this, "cliService", sparkSqlCliService)
    addService(sparkSqlCliService)

    // 初始化Thrift的CliService
    val thriftCliService = if (isHTTPTransportMode(hiveConf)) {
      new ThriftHttpCLIService(sparkSqlCliService)
    } else {
      new ThriftBinaryCLIService(sparkSqlCliService)
    }
    // super.thriftCLISerivce = thriftCliService
    setSuperField(this, "thriftCLIService", thriftCliService)
    addService(thriftCliService)

    // 启动所有Service (cliService、thriftCLIService)
    initCompositeService(hiveConf)
  }

  private def isHTTPTransportMode(hiveConf: HiveConf): Boolean = {
    val transportMode: String = hiveConf.getVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE)
    // 该属性的默认值是binary
    transportMode.equalsIgnoreCase("http")
  }

}
</pre>
<p>
	我们可以拿上述代码对比一下HiveServer2原本的代码：
</p>
<pre class="brush: java" id="HiveServer2">
public class HiveServer2 extends CompositeService {
  private static final Log LOG = LogFactory.getLog(HiveServer2.class);

  private CLIService cliService;
  private ThriftCLIService thriftCLIService;
  
  // ...
  
  @Override
  public synchronized void init(HiveConf hiveConf) {
    cliService = new CLIService();
    addService(cliService);

    String transportMode = System.getenv("HIVE_SERVER2_TRANSPORT_MODE");
    if(transportMode == null) {
      transportMode = hiveConf.getVar(HiveConf.ConfVars.HIVE_SERVER2_TRANSPORT_MODE);
    }
    if(transportMode != null && (transportMode.equalsIgnoreCase("http"))) {
      thriftCLIService = new ThriftHttpCLIService(cliService);
    }
    else {
      thriftCLIService = new ThriftBinaryCLIService(cliService);
    }

    addService(thriftCLIService);
    super.init(hiveConf);
  }
  
  // ...
}
</pre>
<p>
	仔细看就会发现，这两个方法仅在处理启动参数和<code>cliService</code>变量的设置上有所差别。在Spark ThriftServer中，<code>cliService</code>从原本的
	<code>org.apache.hive.service.cli.CLIService</code>变成了<code>org.apache.spark.sql.hive.thriftserver</code>。同时HiveThriftServer2启动了另一个名为
	<code>thriftCLISerivce</code>的服务，这一点上与原本的HiveServer2保持一致。这恰恰证明了我们先前的猜想，这个<code>thriftCLIService</code>就代表着Hive Server模块，
	<code>cliService</code>则代表着Servlet模块。
</p>
<p>
	除此之外，为了能够顺利的复用Hive的功能，Thrift Server大量的使用了Java反射机制。
	HiveThriftServer2除了继承自HiveServer2，还混入了ReflectedCompositeService特质，
	而HiveServer2继承自CompositeService特质。我们可以看一下ReflectedCompositeService特质：
</p>
<pre class="brush: scala" id="ReflectedCompositeService">
/** Reflected，反射 */
private[thriftserver] trait ReflectedCompositeService {
  this: AbstractService =>
  /** 相当于调用CompositeService的init(HiveConf) */
  def initCompositeService(hiveConf: HiveConf) {
    // 模拟 CompositeService.init(hiveConf) 方法
    val serviceList = getAncestorField[JList[Service]](this, 2, "serviceList") // 获取到CompositeService的serviceList
    serviceList.foreach(_.init(hiveConf))   								   // 启动serviceList里的所有serivce

    // 模拟 AbstractService.init(hiveConf) 方法 
    // CompositeSerivce 继承自 AbstractService，而CompositeService.init()的末尾调用了super.init()
    invoke(classOf[AbstractService], this, "ensureCurrentState", classOf[STATE] -> STATE.NOTINITED)
    // ensureCurrentState(STATE.NOTINITED)
    setAncestorField(this, 3, "hiveConf", hiveConf)
    // this.hiveConf = hiveConf
    invoke(classOf[AbstractService], this, "changeState", classOf[STATE] -> STATE.INITED)
    // changeState(STATE.INITED)
    getAncestorField[Log](this, 3, "LOG").info(s"Service: $getName is inited.")
    // LOG.info("Service:" + getName() + " is inited.")
  }
}
</pre>
<p>可以对比一下CompositleService的源代码</p>
<pre class="brush: java">
public class CompositeService extends AbstractService {

  // ...

  private final List&lt;Service> serviceList = new ArrayList&lt;Service>();

  // ...

  protected synchronized void addService(Service service) {
    serviceList.add(service);
  }

  protected synchronized boolean removeService(Service service) {
    return serviceList.remove(service);
  }

  @Override
  public synchronized void init(HiveConf hiveConf) {
    for (Service service : serviceList) {
      service.init(hiveConf);
    }
    super.init(hiveConf);
  }

  @Override
  public synchronized void start() {
    int i = 0;
    try {
      for (int n = serviceList.size(); i &lt; n; i++) {
        Service service = serviceList.get(i);
        service.start();
      }
      super.start();
    } catch (Throwable e) {
      LOG.error("Error starting services " + getName(), e);
      stop(i);
      throw new ServiceException("Failed to Start " + getName(), e);
    }

  }

  // ...
}
</pre>
<p>
	可以看到，CompositeService维护着一个由Service组成的ArrayList（Composite意为“复合的”、“混合的”），
	调用CompositeService的<code>addService</code>和<code>removeService</code>可以向其中添加和删除Service，
	而调用<code>init</code>和<code>start</code>则可以分别调用其中所有Service的init和start方法。
	稍微对比ReflectedCompositeService的代码和CompositeService的代码即可得出结论，
	<code>HiveThriftServer2</code>中的<code>initCompositeService(hiveConf)</code>和<code>HiveServer2</code>中的<code>super.init(hiveConf)</code>是等价的。</p>
<p>
	ReflectedCompositeService是一处。细心的读者还会注意到在<code>HiveThriftServer2</code>中还出现了<code>setSuperField</code>方法。
	<code>setSuperField</code>方法是来自于<code>org.apache.spark.sql.hive.thriftserver.RefectionUtils</code>的静态方法。该工具类包含的所有
	反射工具方法如下：
</p>
<ul>
	<li><code>setSuperField(obj : Object, fieldName: String, fieldValue: Object)</code>：将obj的直接父类的指定变量置为指定值</li>
	<li><code>setAncestorField(obj: AnyRef, level: Int, fieldName: String, fieldValue: AnyRef)</code>：将obj上level级的父类的指定变量置为指定值</li>
	<li><code>getSuperField[T](obj: AnyRef, fieldName: String): T</code>：获取obj的直接父类的指定变量</li>
	<li><code>getAncestorField[T](obj: Object, level: Int, fieldName: String): T</code>：获取obj上level级的父类的指定变量</li>
	<li><code>invokeStatic(clazz: Class[_], methodName: String, args: (Class[_], AnyRef)*): AnyRef</code>：调用某个类的静态函数</li>
	<li><code>invoke(clazz: Class[_], obj: AnyRef, methodName: String, args: (Class[_], AnyRef)*): AnyRef</code>：调用某个对象的指定函数</li>
</ul>
<p>
	为了能够顺利复用<code>HiveServer2</code>的其他方法，<code>HiveThriftServer2</code>必须设置其父类的<code>cliService</code>变量和<code>thriftCLIService</code>变量，
	无奈这两个变量都是<code>private</code>的，所以这里才使用了反射机制对其进行设置。
	包括ReflectedCompositeService以及ReflectionUtils前4个变量相关的方法，希望各位读者能铭记于心。这几个工具方法在整个Thrift Server项目中被多次用到。
</p>
<p>
	在<code>HiveThriftServer2</code>的<code>init</code>方法执行完毕后，Thrift Server初始化完毕。<code>main</code>函数接下来便调用了它的<code>start</code>方法。
	<code>start</code>方法调用其所有通过<code>addService</code>注册的服务的<code>start</code>方法，服务器正式启动。
</p>

<h2 class="jump">总结</h2>
<p>
	感谢您能细心读完本文。如果没有意外的话，您应该已对SparkSQL ThriftServer的启动流程有了大致的了解。该流程可用如此表示：
</p>
<p class="center"><img alt="" src="/img/SparkSQL@1.jpg"></p>
<p>
	同时，您也了解到，<code>ThriftCliService</code>充当着Servlet Container的角色，维护着与客户端的连接，接收客户端的请求、为客户端发送结果，但主要的业务逻辑并不在
	里面，而是在充当Servlet角色的执行模块<code>SparkSQLCLIService</code>内。
</p>
<p>在接下来的文章中，我将分两个方向，分别讲解这两个模块的工作原理。敬请期待。</p>
<p style="height: 20px"></p>
<a href="/applications/2015/07/30/SparkSQL-HiveThriftServer-Source-2.html">SparkSQL Hive ThriftServer 源码解析（二）：SparkSQLCLIService</a>
			<!-- and ends here -->
			<div class="blank" style="height: 80px"></div>
			<!-- Disqus Block starts here -->
			<div id="disqus_thread"></div>
			<script type="text/javascript">
				/* * * CONFIGURATION VARIABLES * * */
				var disqus_shortname = 'robertpsblog';
				var disqus_identifier = 'SparkSQL Hive ThriftServer 源码解析（一）：Intro';
    
				/* * * DON'T EDIT BELOW THIS LINE * * */
				(function() {
					var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
					dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
					(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
				})();
			</script>
			<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
			<!-- and ends here -->
		</div>
    </div>
    </div>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/mrdai.js"></script>
    <script>SyntaxHighlighter.all()</script>
</body>
</html>