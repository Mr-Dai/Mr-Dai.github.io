<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <link href="/css/bootstrap.min.css" rel="stylesheet">
    <link href="/css/mrdai.css" rel="stylesheet">
    <script type="text/javascript" src="/js/jquery-2.1.1.min.js"></script>
    
    <script type="text/javascript" src="/js/syntaxhighlighters/shCore.js"></script>
    <link href="/css/syntaxhighlighters/shCore.css" rel="stylesheet" type="text/css" />
    <link href="/css/syntaxhighlighters/shThemeDefault.css" rel="stylesheet" type="text/css" />
    
    <title>Hadoop、Hive、ZooKeeper配置 - Robert Peng</title>
</head>
<body>
    <div id="main_wrapper">
    <div id="banner_wrapper">
    <h1>Mr.Dai's Blog</h1>
    </div>
    <nav id="navbar" class="navbar navbar-default" role="navigation">
    <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">首页</a>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown">分类<span class="caret"></span></a>
          <ul class="dropdown-menu" role="menu">
            <li><a href="/courses.html">本科学习</a></li>
            <li><a href="/application.html">应用开发</a></li>
            <li><a href="/algorithm.html">算法研究</a></li>
            <li><a href="/languages.html">编程语言</a></li>
            <li class="divider"></li>
            <li><a href="/mobile">切换至手机版</a></li>
          </ul>
        </li>
      </ul>
    </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
    </nav>
    <div id="content_wrapper" style="padding:20px; line-height:30px">
        <div id="right_wrapper" style="float:right; margin-left: 10px; width: 30%; font-size:15px; line-height:25px;">
            <ul id="JumpList">
                <li><h4>跳转目录</h4></li>
            </ul>
        </div>
                <h1 style="margin-top:0; font-size: 25px;"><strong>Hadoop、Hive、ZooKeeper配置</strong></h1>
                <p style="color: gray; font-size: 14px">By Robert Peng</p>
                <p style="color: gray; font-size:14px;">02 Apr 2015</p>
                <!-- content begins here -->
                <script type="text/javascript" src="/js/syntaxhighlighters/shBrushJava.js"></script>
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushBash.js"></script>
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushXml.js"></script>
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushSql.js"></script>

<h2 class="jump">配置伪分布Hadoop</h2>

<h3>实验环境</h3>
<blockquote>
    <p>Lenovo Y480n-i5-3210</p>
    <p>Hadoop 2.6.0</p>
    <p>Hive 1.1.0</p>
    <p>ZooKeeper 3.4.6</p>
    <p>Ubuntu 14.04 RTS amd64 desktop</p>
    <p>Java-7-openjdk</p>
</blockquote>

<h3>准备工作</h3>
安装ssh-server和openjdk7：
<pre class="brush: bash">
$sudo apt-get install ssh openssh-server openjdk-7-jdk  
</pre>
JDK安装路径为/usr/lib/jvm/java-7-openjdk-amd64。记住这个路径。

创建hadoop用户，并配置ssh登录：
<pre class="brush: bash">
$sudo useradd -g root hadoop
$sudo passwd hadoop
$su - hadoop
$ssh-keygen -t rsa -P ""
$cd ~/.ssh
$cat id_rsa.pub >> authorized_keys
</pre>
<p>运行<code>ssh hadoop@localhost</code>不提示输入密码则配置成功。</p>

<h3>安装Hadoop</h3>
<p>下载hadoop并解压缩</p>
<pre class="brush: bash">
$cd ~/Downloads
$wget http://apache.fayea.com/hadoop/common/stable/hadoop-2.6.0.tar.gz
$sudo tar -zxf hadoop-2.6.0.tar.gz
$sudo mv hadoop-2.6.0 /usr/local/
$sudo chmod -R 775 /usr/local/hadoop
$sudo chown -R root:hadoop /usr/local/hadoop-2.6.0
</pre>
<p>配置环境变量：</p>
<p>运行<code>$sudo vim /etc/profile.d/hadoop.sh</code>并输入如下内容。</p>
<pre class="brush: bash">
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop-2.6.0
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
</pre>
<p>然后运行指令<code>source /etc/profile</code>使配置生效。</p>
<p>然后配置Hadoop的分布式参数：</p>
<p><code>sudo vim /usr/local/hadoop-2.6.0/etc/hadoop/core-site.xml</code></p>
<pre class="brush: xml">
<configuration>  
    <property>  
        <name>hadoop.tmp.dir</name>  
        <value>/usr/local/hadoop-2.6.0/tmp</value>  
        <description>Abase for other temporary directories.</description>  
    </property>  
    <property>  
        <name>fs.defaultFS</name>  
        <value>hdfs://localhost:9000</value>  
    </property>  
</configuration>
</pre>
<p><code>sudo vim /usr/local/hadoop-2.6.0/etc/hadoop/yarn-site.xml</code></p>
<pre class="brush: xml">
<configuration>  
    <property>  
        <name>mapreduce.framework.name</name>  
        <value>yarn</value>  
    </property>  
  
    <property>  
        <name>yarn.nodemanager.aux-services</name>  
        <value>mapreduce_shuffle</value>  
    </property>  
</configuration>
</pre>
<p><code>sudo vim /usr/local/hadoop-2.6.0/etc/hadoop/hdfs-site.xml</code></p>
<pre class="brush: xml">
<configuration>  
    <property>  
        <name>dfs.replication</name>  
        <value>1</value>  
    </property>  
    <property>  
        <name>dfs.namenode.name.dir</name>  
        <value>file:/usr/local/hadoop/dfs/name</value>  
    </property>  
    <property>  
        <name>dfs.datanode.data.dir</name>  
        <value>file:/usr/local/hadoop/dfs/data</value>  
    </property>  
    <property>
        <name>dfs.permissions</name>  
        <value>false</value>  
    </property>  
 </configuration>
</pre>
<p>配置完成后，首先在Hadoop目录下创建所需的临时目录：</p>
<pre class="brush: bash">
$cd $HADOOP_HOME
$mkdir tmp dfs/{name, data}
</pre>
<p>然后初始化并启动集群</p>
<pre class="brush: bash">
$bin/hdfs namenode -format
$sbin/start-dfs.sh
$sbin/start-yarn.sh
</pre>
<h3>运行WordCount案例</h3>
<pre class="brush: bash">
$cd $HADOOP_HOME
$bin/hdfs dfs -mkdir -p /user/hadoop/input
$bin/hdfs dfs -put etc/hadoop/* /user/hadoop/input
$bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount \
     /user/ha1/input/hadoop \
     /user/ha1/output/hadoop
$bin/hdfs dfs -cat /user/hadoop/output/hadoop/* #查看结果
</pre>
<h2 class="jump">配置ZooKeeper</h2>
<h3>下载并解压缩ZooKeeper</h3>
<pre class="brush: bash">
$cd ~/Downloads
$wget http://apache.mirrors.ionfish.org/zookeeper/stable/zookeeper-3.4.6.tar.gz
$tar zxf zookeeper-3.4.6.tar.gz
$sudo mv zookeeper-3.4.6 /usr/local/
</pre>
<p>运行<code>$sudo vim /etc/profile.d/zookeeper.sh</code>并输入：</p>
<pre class="brush: bash">
export ZOOKEEPER_INSTALL=/usr/local/zookeeper-3.4.6
export PATH=$PATH:$ZOOKEEPER_INSTALL/bin
</pre>
<p>运行<code>$source /etc/profile</code>使配置生效。</p>
<h3>配置ZooKeeper</h3>
进入ZooKeeper安装目录的conf子目录中，新建一个zoo.cfg文件，并输入如下内容：
<pre>
tickTime=2000
dataDir=/usr/local/zookeeper-3.4.6/pData
clientPort=2181
</pre>
<p>进入bin子目录并运行<code>$./zkServer.sh start</code>启动ZooKeeper。</p>
<p>向服务器发送Hello指令<code>$echo ruok | nc localhost 2181</code>，若回显<code>imok</code>即说明ZooKeeper已正常运行。</p>

<h2 class="jump">配置Hive</h2>
<h3>下载安装Hive</h3>
<pre class="brush: bash">
$cd ~/Downloads
$wget http://apache.arvixe.com/hive/stable/apache-hive-1.1.0-bin.tar.gz
$wget tar zxf hive-1.1.0-bin.tar.gz
$sudo mv hive-1.1.0-bin /usr/local/hive
</pre>
<p>运行<code>$sudo vim /etc/profile.d/hive.sh</code>并输入：</p>
<pre class="brush: bash">
export HIVE_INSTALL=/usr/local/hive
export PATH=$PATH:$HIVE_INSTALL/bin
</pre>
<p>运行<code>$source /etc/profile</code>使配置生效。</p>
<p>一般来讲，如果变量<code>$HADOOP_HOME</code>配置正确，这个时候直接运行<code>hive</code>即可进入HIVE的Shell了。
<h2 class="jump">测试Hive</h2>
<h3>编写Java程序产生数据</h3>
<pre class="brush: java">
import java.io.BufferedOutputStream;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Date;

/**
 * 随机产生指定数量的学生信息记录到指定文件中
 * 
 * @author Robert Peng
 */
public class DataGenerator {

	public static void main(String[] args) {
		if (args.length != 2) {
			System.err
					.println("Usage: DataGenerator &lt;output path> &lt;record count>");
			System.exit(-1);
		}

		File mFile = null;
		BufferedOutputStream fop = null;
		Date currentDate = new Date();			// 当前日期
		Date enrollDate = null;
		SimpleDateFormat dateFormat = null;
		try {
			dateFormat = new SimpleDateFormat("yyyy-MM-dd");
			enrollDate = dateFormat.parse("1970-01-01");	// 从1970年开始
		} catch (ParseException e) {
			e.printStackTrace();
			System.exit(-1);
		}
		try {
			mFile = new File(args[0]);
			if (mFile.exists())
				mFile.delete();			// 删除原有文件
			mFile.createNewFile();
			fop = new BufferedOutputStream(new FileOutputStream(mFile));
		} catch (Exception e) {
			e.printStackTrace();
			System.exit(-1);
		}

		int recordCount = Integer.valueOf(args[1]);
		int stepCount = (int) (recordCount / ((currentDate.getTime() - enrollDate
				.getTime()) / (24 * 60 * 60 * 1000)));    // 计算步长
		stepCount = stepCount == 0 ? 1 : stepCount;		  // 防止stepCount为0
		try {
			// 输出记录格式： &lt;student-id>\t&lt;enroll-date>\t&lt;gender>\n
			for (int i = 1; i &lt; recordCount; i++) {
				fop.write(new String(i + "\t" + dateFormat.format(enrollDate)
						+ "\t" + (int) (Math.random() * 2) + "\n").getBytes());
				if (i % stepCount == 0)
					enrollDate = new Date(enrollDate.getTime() + 24 * 60 * 60 * 1000); 	// 自增一天
			}
			fop.flush();
			fop.close(); // 写入并关闭文件
		} catch (IOException e) {
			e.printStackTrace();
			System.exit(-1);
		}

	}

}
</pre>
<p>运行：</p>
<pre class="brush: bash">
$javac DataGenerator.java
$java DataGenerator ./student.txt 10000000
</pre>
<p class="center"><img width="1200px" alt="" src="/img/hive@1.jpg"></p>
<p>在当前目录下输入<code>hive</code>启动HIVE命令行，并输入：
<pre class="brush: sql">
> CREATE TABLE student (id INT, enrollDate STRING, gender INT)
.
. ROW FORMAT DELIMITED
.   FILEDS TERMINATED BY '\t';                  -- 建表
> LOAD DATA LOCAL INPUT 'student.txt'
. OVERWRITE INTO TABLE student;                 -- 导入数据
> SELECT gender, COUNT(*) AS gender_num FROM student
.   WHERE UNIX_TIMESTAMP(enrollDate,'yyyy-MM-dd') > UNIX_TIMESTAMP('1985-01-01','yyyy-MM-dd')
.   GROUP BY gender;                            -- 统计1985年后入学的男女学生数
</pre>
<p>等待MapReduce处理结束后，Hive便会返回查询结果</p>
<p class="center"><img width="1200px" alt="" src="/img/hive@2.jpg"></p>
<p class="center"><img width="1200px" alt="" src="/img/hive@3.jpg"></p>

                <!-- and end here -->
    </div>
    </div>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/mrdai.js"></script>
    <script>SyntaxHighlighter.all()</script>
</body>
</html>