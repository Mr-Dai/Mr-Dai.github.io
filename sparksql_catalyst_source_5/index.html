<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Spark Catalyst 源码解析：Planner 与 RDD - Robert Peng&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="呆呆的博客"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="呆呆的博客"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="在上一篇文章中，我们详细了解了 SparkSQL 如何利用 Analyzer 和 Optimizer，一步一步将 Unresolved Logical Plan 变为 Analyzed Logical Plan 再变为 Optimized Logical Plan。到了这一步，Logical Plan 的生命历程就走到了终点。 在这篇文章中，我将开始讲解 SparkSQL 如何通过 Planner"><meta property="og:type" content="blog"><meta property="og:title" content="Spark Catalyst 源码解析：Planner 与 RDD"><meta property="og:url" content="https://mr-dai.github.io/sparksql_catalyst_source_5/"><meta property="og:site_name" content="Robert Peng&#039;s Blog"><meta property="og:description" content="在上一篇文章中，我们详细了解了 SparkSQL 如何利用 Analyzer 和 Optimizer，一步一步将 Unresolved Logical Plan 变为 Analyzed Logical Plan 再变为 Optimized Logical Plan。到了这一步，Logical Plan 的生命历程就走到了终点。 在这篇文章中，我将开始讲解 SparkSQL 如何通过 Planner"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://mr-dai.github.io/img/Spark-Catalyst@11.jpg"><meta property="og:image" content="https://mr-dai.github.io/img/Spark-Catalyst@12.jpg"><meta property="og:image" content="https://mr-dai.github.io/img/Spark-Catalyst@13.jpg"><meta property="og:image" content="https://mr-dai.github.io/img/Spark-Catalyst@14.jpg"><meta property="og:image" content="https://mr-dai.github.io/img/Spark-Catalyst@15.jpg"><meta property="article:published_time" content="2015-08-20T16:00:00.000Z"><meta property="article:modified_time" content="2015-08-20T16:00:00.000Z"><meta property="article:author" content="Robert Peng"><meta property="article:tag" content="Spark"><meta property="article:tag" content="SparkSQL"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/Spark-Catalyst@11.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://mr-dai.github.io/sparksql_catalyst_source_5/"},"headline":"Spark Catalyst 源码解析：Planner 与 RDD","image":["https://mr-dai.github.io/img/Spark-Catalyst@11.jpg","https://mr-dai.github.io/img/Spark-Catalyst@12.jpg","https://mr-dai.github.io/img/Spark-Catalyst@13.jpg","https://mr-dai.github.io/img/Spark-Catalyst@14.jpg","https://mr-dai.github.io/img/Spark-Catalyst@15.jpg"],"datePublished":"2015-08-20T16:00:00.000Z","dateModified":"2015-08-20T16:00:00.000Z","author":{"@type":"Person","name":"Robert Peng"},"publisher":{"@type":"Organization","name":"Robert Peng's Blog","logo":{"@type":"ImageObject","url":"https://mr-dai.github.io/img/avatar.png"}},"description":"在上一篇文章中，我们详细了解了 SparkSQL 如何利用 Analyzer 和 Optimizer，一步一步将 Unresolved Logical Plan 变为 Analyzed Logical Plan 再变为 Optimized Logical Plan。到了这一步，Logical Plan 的生命历程就走到了终点。 在这篇文章中，我将开始讲解 SparkSQL 如何通过 Planner"}</script><link rel="canonical" href="https://mr-dai.github.io/sparksql_catalyst_source_5/"><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/avatar.png" alt="Robert Peng&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="我的 Github" href="https://github.com/Mr-Dai"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2015-08-20T16:00:00.000Z" title="8/20/2015, 4:00:00 PM">2015-08-21</time>发表</span><span class="level-item"><time dateTime="2015-08-20T16:00:00.000Z" title="8/20/2015, 4:00:00 PM">2015-08-21</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Spark/">Spark</a></span></div></div><h1 class="title is-3 is-size-4-mobile">Spark Catalyst 源码解析：Planner 与 RDD</h1><div class="content"><p>在<a href="/sparksql_catalyst_source_4">上一篇文章</a>中，我们详细了解了 SparkSQL 如何利用 Analyzer 和 Optimizer，一步一步将 Unresolved Logical Plan 变为 Analyzed Logical Plan 再变为 Optimized Logical Plan。到了这一步，Logical Plan 的生命历程就走到了终点。</p>
<p>在这篇文章中，我将开始讲解 SparkSQL 如何通过 Planner 将 Optimized Logical Plan 变为 Physical Plan，再变为结果 RDD。</p>
<span id="more"></span>

<p><img src="/img/Spark-Catalyst@11.jpg"></p>
<h2 id="SparkPlanner"><a href="#SparkPlanner" class="headerlink" title="SparkPlanner"></a>SparkPlanner</h2><p>到了这一步，我们就不能期待 Planner 和 Optimizer 他们一样继承自 <code>RuleExecutor</code> 了。为了了解这个过程的入口，我们先回到之前提到过的 <code>SQLContext#QueryExecution</code>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> optimizedPlan: <span class="type">LogicalPlan</span> = optimizer.execute(withCachedData)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 生成 PhysicalPlan</span></span><br><span class="line"><span class="comment">// Optimized Logical Plan -&gt; Physical Plan</span></span><br><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> sparkPlan: <span class="type">SparkPlan</span> = &#123;</span><br><span class="line">  <span class="type">SparkPlan</span>.currentContext.set(self)</span><br><span class="line">  planner.plan(optimizedPlan).next()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>总结下来就是这样的一个流程：<code>optimizedPlan -&gt; planner.plan -&gt; sparkPlan</code>。由此一来，我们首先锁定了入口方法 <code>planner.plan</code>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SQLContext.scala</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>[sql] <span class="keyword">val</span> planner = <span class="keyword">new</span> <span class="type">SparkPlanner</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>[sql] <span class="class"><span class="keyword">class</span> <span class="title">SparkPlanner</span> <span class="keyword">extends</span> <span class="title">SparkStrategies</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 从外部的 SQLContext 实例中导入相关设定参数</span></span><br><span class="line">  <span class="keyword">val</span> sparkContext: <span class="type">SparkContext</span> = self.sparkContext</span><br><span class="line">  <span class="keyword">val</span> sqlContext: <span class="type">SQLContext</span> = self</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">codegenEnabled</span></span>: <span class="type">Boolean</span> = self.conf.codegenEnabled</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">unsafeEnabled</span></span>: <span class="type">Boolean</span> = self.conf.unsafeEnabled</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = self.conf.numShufflePartitions</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">strategies</span></span>: <span class="type">Seq</span>[<span class="type">Strategy</span>] =</span><br><span class="line">    experimental.extraStrategies ++ (</span><br><span class="line">    <span class="type">DataSourceStrategy</span> ::</span><br><span class="line">    <span class="type">DDLStrategy</span> ::</span><br><span class="line">    <span class="type">TakeOrdered</span> ::</span><br><span class="line">    <span class="type">HashAggregation</span> ::</span><br><span class="line">    <span class="type">LeftSemiJoin</span> ::</span><br><span class="line">    <span class="type">HashJoin</span> ::</span><br><span class="line">    <span class="type">InMemoryScans</span> ::</span><br><span class="line">    <span class="type">ParquetOperations</span> ::</span><br><span class="line">    <span class="type">BasicOperators</span> ::</span><br><span class="line">    <span class="type">CartesianProduct</span> ::</span><br><span class="line">    <span class="type">BroadcastNestedLoopJoin</span> :: <span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<p>我们这次看到了一个 <code>strategies</code> 变量，其形式与之前在 <code>Analyzer</code> 和 <code>Optimizer</code> 里看到的 <code>batches</code> 变量十分相似。除此之外，我们并未看到 <code>SparkPlanner</code> 实现 <code>plan</code> 方法。这并不奇怪，毕竟 <code>Analyzer</code> 和 <code>Optimizer</code> 也没有实现 <code>execute</code> 方法。那我们先去看看 <code>SparkPlanner</code> 的父类 <code>SparkStrategies</code>：</p>
<p><img src="/img/Spark-Catalyst@12.jpg"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[sql] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStrategies</span> <span class="keyword">extends</span> <span class="title">QueryPlanner</span>[<span class="type">SparkPlan</span>] </span>&#123;</span><br><span class="line">  self: <span class="type">SQLContext</span>#<span class="type">SparkPlanner</span> =&gt;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">LeftSemiJoin</span> <span class="keyword">extends</span> <span class="title">Strategy</span> <span class="keyword">with</span> <span class="title">PredicateHelper</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">HashJoin</span> <span class="keyword">extends</span> <span class="title">Strategy</span> <span class="keyword">with</span> <span class="title">PredicateHelper</span> </span>&#123;</span><br><span class="line">	<span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">HashAggregation</span> <span class="keyword">extends</span> <span class="title">Strategy</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">BroadcastNestedLoopJoin</span> <span class="keyword">extends</span> <span class="title">Strategy</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">CartesianProduct</span> <span class="keyword">extends</span> <span class="title">Strategy</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">lazy</span> <span class="keyword">val</span> singleRowRdd =</span><br><span class="line">    sparkContext.parallelize(<span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">GenericRow</span>(<span class="type">Array</span>[<span class="type">Any</span>]()): <span class="type">Row</span>), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">TakeOrdered</span> <span class="keyword">extends</span> <span class="title">Strategy</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">ParquetOperations</span> <span class="keyword">extends</span> <span class="title">Strategy</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">InMemoryScans</span> <span class="keyword">extends</span> <span class="title">Strategy</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">BasicOperators</span> <span class="keyword">extends</span> <span class="title">Strategy</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">DDLStrategy</span> <span class="keyword">extends</span> <span class="title">Strategy</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>似乎 <code>SparkStrategies</code> 并未定义任何函数，倒是定义了大量的 <code>Strategy</code> 子类，这些子类都被应用在了 <code>SQLContext#SparkPlanner</code> 中。那么看来，这个类确实是名符其实的 <code>SparkStrategies</code>。我们继续去看它的父类吧！</p>
<p><img src="/img/Spark-Catalyst@13.jpg"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 可以看到 Strategy 与之前的 Rule 很类似，差别只在与 apply 函数返回的是 Seq[PhysicalPlan]</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">GenericStrategy</span>[<span class="type">PhysicalPlan</span> &lt;: <span class="type">TreeNode</span>[<span class="type">PhysicalPlan</span>]] <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">Seq</span>[<span class="type">PhysicalPlan</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 相对的，QueryPlanner 也和 RuleExecutor 十分相似</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">QueryPlanner</span>[<span class="type">PhysicalPlan</span> &lt;: <span class="type">TreeNode</span>[<span class="type">PhysicalPlan</span>]] </span>&#123;</span><br><span class="line">  <span class="comment">/** A list of execution strategies that can be used by the planner */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">strategies</span></span>: <span class="type">Seq</span>[<span class="type">GenericStrategy</span>[<span class="type">PhysicalPlan</span>]]</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 返回一个占位符。该占位符将由 QueryPlanner 使用其它可用的 Strategy 替换掉</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">planLater</span></span>(plan: <span class="type">LogicalPlan</span>) = <span class="keyword">this</span>.plan(plan).next()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">plan</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">Iterator</span>[<span class="type">PhysicalPlan</span>] = &#123;</span><br><span class="line">    <span class="comment">// Lazy 地在 LogicalPlan 上 apply 所有 Strategy</span></span><br><span class="line">    <span class="keyword">val</span> iter = strategies.view.flatMap(_(plan)).toIterator</span><br><span class="line">    assert(iter.hasNext, <span class="string">s&quot;No plan for <span class="subst">$plan</span>&quot;</span>)</span><br><span class="line">    iter</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/img/Spark-Catalyst@14.jpg"></p>
<p>从执行引擎这边能看到的似乎就只有这些了，我们甚至无法知道模板参数 <code>PhysicalPlan</code> 具体会是什么类型。通过查看之前出现过的 <code>Strategy</code> 类型，会在 <code>sql</code> 的包对象中发现这样一句：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">Strategy</span> </span>= org.apache.spark.sql.catalyst.planning.<span class="type">GenericStrategy</span>[<span class="type">SparkPlan</span>]</span><br></pre></td></tr></table></figure>

<p>至此我们就了解到，PhysicalPlan 树结点的类型为 <code>SparkPlan</code>。于是我们查看它的源代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkPlan</span> </span>&#123;</span><br><span class="line">  <span class="keyword">protected</span>[sql] <span class="keyword">val</span> currentContext = <span class="keyword">new</span> <span class="type">ThreadLocal</span>[<span class="type">SQLContext</span>]()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 与 LogicalPlan 相同，继承自 QueryPlan</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkPlan</span> <span class="keyword">extends</span> <span class="title">QueryPlan</span>[<span class="type">SparkPlan</span>] <span class="keyword">with</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  self: <span class="type">Product</span> =&gt;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@transient</span></span><br><span class="line">  <span class="keyword">protected</span>[spark] <span class="keyword">final</span> <span class="keyword">val</span> sqlContext = <span class="type">SparkPlan</span>.currentContext.get()</span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">sparkContext</span> </span>= sqlContext.sparkContext</span><br><span class="line"></span><br><span class="line">  <span class="comment">// sqlContext will be null when we are being deserialized on the slaves.  In this instance</span></span><br><span class="line">  <span class="comment">// the value of codegenEnabled will be set by the desserializer after the constructor has run.</span></span><br><span class="line">  <span class="keyword">val</span> codegenEnabled: <span class="type">Boolean</span> = <span class="keyword">if</span> (sqlContext != <span class="literal">null</span>) &#123;</span><br><span class="line">    sqlContext.conf.codegenEnabled</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Overridden make copy also propogates sqlContext to copied plan. */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">makeCopy</span></span>(newArgs: <span class="type">Array</span>[<span class="type">AnyRef</span>]): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">    <span class="type">SparkPlan</span>.currentContext.set(sqlContext)</span><br><span class="line">    <span class="keyword">super</span>.makeCopy(newArgs)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义计算结果在各个节点上的 partition 规则</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">outputPartitioning</span></span>: <span class="type">Partitioning</span> = <span class="type">UnknownPartitioning</span>(<span class="number">0</span>) <span class="comment">// <span class="doctag">TODO:</span> WRONG WIDTH!</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义输入数据的若干个节点分布要求</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">requiredChildDistribution</span></span>: <span class="type">Seq</span>[<span class="type">Distribution</span>] =</span><br><span class="line">    <span class="type">Seq</span>.fill(children.size)(<span class="type">UnspecifiedDistribution</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义计算结果在各个节点上的排序规则</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">outputOrdering</span></span>: <span class="type">Seq</span>[<span class="type">SortOrder</span>] = <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义输入数据的每个 partition 的若干个排序要求</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">requiredChildOrdering</span></span>: <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">SortOrder</span>]] = <span class="type">Seq</span>.fill(children.size)(<span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 在 withScope 内调用 doExecute 方法来得出结果</span></span><br><span class="line">  <span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    <span class="type">RDDOperationScope</span>.withScope(sparkContext, nodeName, <span class="literal">false</span>, <span class="literal">true</span>) &#123;</span><br><span class="line">      doExecute()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 由子类重载该方法返回计算结果</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">doExecute</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment">// execute + collect</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">executeCollect</span></span>(): <span class="type">Array</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    execute().mapPartitions &#123; iter =&gt;</span><br><span class="line">      <span class="keyword">val</span> converter = <span class="type">CatalystTypeConverters</span>.createToScalaConverter(schema)</span><br><span class="line">      iter.map(converter(_).asInstanceOf[<span class="type">Row</span>])</span><br><span class="line">    &#125;.collect()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// execute + take(n)</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">executeTake</span></span>(n: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Row</span>](<span class="number">0</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 先获得代表完整结果的 RDD</span></span><br><span class="line">    <span class="keyword">val</span> childRDD = execute().map(_.copy())</span><br><span class="line"></span><br><span class="line">	<span class="comment">// result buffer</span></span><br><span class="line">    <span class="keyword">val</span> buf = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Row</span>]</span><br><span class="line">	<span class="comment">// partition 总数</span></span><br><span class="line">    <span class="keyword">val</span> totalParts = childRDD.partitions.length</span><br><span class="line">	<span class="comment">// 已扫描的 partition 数</span></span><br><span class="line">    <span class="keyword">var</span> partsScanned = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (buf.size &lt; n &amp;&amp; partsScanned &lt; totalParts) &#123;</span><br><span class="line">      <span class="comment">// 本次迭代尝试扫描的 partition 数</span></span><br><span class="line">      <span class="keyword">var</span> numPartsToTry = <span class="number">1</span></span><br><span class="line">      <span class="keyword">if</span> (partsScanned &gt; <span class="number">0</span>) &#123; <span class="comment">// 从第二次迭代开始</span></span><br><span class="line">        <span class="keyword">if</span> (buf.size == <span class="number">0</span>) &#123; <span class="comment">// 如果第一次迭代完全没有获取到结果，直接扫描剩下所有 partition</span></span><br><span class="line">          numPartsToTry = totalParts - <span class="number">1</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// 1.5 * n / (buf.size / partsScanned)</span></span><br><span class="line">          numPartsToTry = (<span class="number">1.5</span> * n * partsScanned / buf.size).toInt</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      numPartsToTry = math.max(<span class="number">0</span>, numPartsToTry)  <span class="comment">// guard against negative num of partitions</span></span><br><span class="line"></span><br><span class="line">	  <span class="comment">// 剩余所需结果数</span></span><br><span class="line">      <span class="keyword">val</span> left = n - buf.size</span><br><span class="line">	  <span class="comment">// 即将进行尝试的 partition 集</span></span><br><span class="line">      <span class="keyword">val</span> p = partsScanned until math.min(partsScanned + numPartsToTry, totalParts)</span><br><span class="line">      <span class="keyword">val</span> sc = sqlContext.sparkContext</span><br><span class="line">      <span class="keyword">val</span> res =</span><br><span class="line">        sc.runJob(childRDD, (it: <span class="type">Iterator</span>[<span class="type">Row</span>]) =&gt; it.take(left).toArray, p, allowLocal = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">	  <span class="comment">// 将结果放入 buf</span></span><br><span class="line">      res.foreach(buf ++= _.take(n - buf.size))</span><br><span class="line">      partsScanned += numPartsToTry</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 改变结果类型并返回。此步同 takeCollect</span></span><br><span class="line">    <span class="keyword">val</span> converter = <span class="type">CatalystTypeConverters</span>.createToScalaConverter(schema)</span><br><span class="line">    buf.toArray.map(converter(_).asInstanceOf[<span class="type">Row</span>])</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>真是万万没想到，<code>SparkPlan</code> 与 <code>LogicalPlan</code> 同样继承自 <code>QueryPlan</code>，但仔细想想确实很合理。通过观察 <code>SparkPlan</code> 类便能发现，其实现类需要通过重载 <code>doExecute</code> 方法来定义自己的计算逻辑。在了解到这个主要入口以后，剩下的问题就变得轻松很多了。</p>
<p><img src="/img/Spark-Catalyst@15.jpg"></p>
<p>但实际上，有一个难题我们并没有解决，有可能各位还没注意到这个问题。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">QueryPlanner</span>[<span class="type">PhysicalPlan</span> &lt;: <span class="type">TreeNode</span>[<span class="type">PhysicalPlan</span>]] </span>&#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 返回一个占位符。该占位符将由 QueryPlanner 使用其它可用的 Strategy 替换掉</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">planLater</span></span>(plan: <span class="type">LogicalPlan</span>) = <span class="keyword">this</span>.plan(plan).next()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">plan</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">Iterator</span>[<span class="type">PhysicalPlan</span>] = &#123;</span><br><span class="line">    <span class="comment">// Lazy 地在 LogicalPlan 上 apply 所有 Strategy</span></span><br><span class="line">    <span class="keyword">val</span> iter = strategies.view.flatMap(_(plan)).toIterator</span><br><span class="line">    assert(iter.hasNext, <span class="string">s&quot;No plan for <span class="subst">$plan</span>&quot;</span>)</span><br><span class="line">    iter</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>plan</code> 函数的 <code>iter = strategies.view.flatMap(_(plan)).toIterator</code> 这句是不是有点问题？为什么 <code>planLater</code> 那个实现返回的是一个占位符？这个问题我们先不着急回答，我们先看看 <code>Strategy</code> 实现类是怎么使用 <code>planLater</code> 的：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 笛卡尔积，由 SQL 语句的 JOIN 操作触发</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CartesianProduct</span> <span class="keyword">extends</span> <span class="title">Strategy</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">Seq</span>[<span class="type">SparkPlan</span>] = plan <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> logical.<span class="type">Join</span>(left, right, _, <span class="type">None</span>) =&gt;</span><br><span class="line">      execution.joins.<span class="type">CartesianProduct</span>(planLater(left), planLater(right)) :: <span class="type">Nil</span></span><br><span class="line">    <span class="keyword">case</span> logical.<span class="type">Join</span>(left, right, <span class="type">Inner</span>, <span class="type">Some</span>(condition)) =&gt;</span><br><span class="line">      execution.<span class="type">Filter</span>(condition,</span><br><span class="line">        execution.joins.<span class="type">CartesianProduct</span>(planLater(left), planLater(right))) :: <span class="type">Nil</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="type">Nil</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意：这个类属于 execution.joins 包，放在这里只是方便参考</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CartesianProduct</span>(<span class="params">left: <span class="type">SparkPlan</span>, right: <span class="type">SparkPlan</span></span>) <span class="keyword">extends</span> <span class="title">BinaryNode</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">output</span></span>: <span class="type">Seq</span>[<span class="type">Attribute</span>] = left.output ++ right.output</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doExecute</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> leftResults = left.execute().map(_.copy())</span><br><span class="line">    <span class="keyword">val</span> rightResults = right.execute().map(_.copy())</span><br><span class="line"></span><br><span class="line">    leftResults.cartesian(rightResults).mapPartitions &#123; iter =&gt;</span><br><span class="line">      <span class="keyword">val</span> joinedRow = <span class="keyword">new</span> <span class="type">JoinedRow</span></span><br><span class="line">      iter.map(r =&gt; joinedRow(r._1, r._2))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们注意到，在 <code>object CartesianProduct</code> 的 <code>apply</code> 中，当遇到标记为 <code>Join</code> 的 Logical Plan 时，它的做法是先对左右子树分别调用 <code>planLater</code> 得到结果后，再构造 <code>execution.joins.CartesianProduct</code>。而 <code>planLater</code> 又会调用 <code>plan</code>，这意味着每一次调用 <code>planLater</code> 实际上都是一次递归，这是一个先序遍历。<code>planLater</code> 的实现是 <code>this.plan(plan).next()</code>，意味着即使 strategies 中可应用于传入子树的策略不止一个，返回的 Physical Plan 数也可能不止一个（注意 <code>Strategy</code> 的 <code>apply</code> 函数返回的是个 <code>Seq</code>），但 <code>planLater</code> 都只取第一个。</p>
<p>我们回到最初启动 plan 过程的入口：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> sparkPlan: <span class="type">SparkPlan</span> = &#123;</span><br><span class="line">  <span class="type">SparkPlan</span>.currentContext.set(self)</span><br><span class="line">  planner.plan(optimizedPlan).next()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里就是这个先序遍历开始的地方，同样使用了和 <code>planLater</code> 一样的调用方式，这就证明了我的猜想。这同时说明，尽管 Spark 可以为同一个 Logical Plan 生成多个 Physical Plan，但本该在这些 Physical Plan 中选出最低代价执行计划的功能并未实现。在 <code>LogicalPlan</code> 中我们有看到过疑似要用于 cost-based 优化的 <code>Statistics</code> 变量，但在 Physical Plan 这边实际上我们并未见到它的身影，而且 <code>Statistics</code> 类本身的设计也过于简单（它是一个只包含了一个 <code>BigInt</code> 变量的 case class，并未继承任何类），显得有些许儿戏。</p>
<p>但这毕竟是不能怪 SparkSQL 的，查询代价受环境的影响很大，比起 rule-based 优化来说，cost-based 太过不稳定，实现起来也复杂很多。不过不管怎么说，SparkSQL 仍然留下了可用于实现 cost-based 优化的接口，也许有朝一日这个功能真的会实现。</p>
<h2 id="toRDD"><a href="#toRDD" class="headerlink" title="toRDD"></a>toRDD</h2><p>我们接着往下走：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> executedPlan: <span class="type">SparkPlan</span> = prepareForExecution.execute(sparkPlan)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行并返回结果</span></span><br><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> toRdd: <span class="type">RDD</span>[<span class="type">Row</span>] = executedPlan.execute()</span><br></pre></td></tr></table></figure>

<p>上文中出现的 <code>prepareForExecution</code> 实际上是一个 <code>RuleExecutor</code> 的子类，它唯一的 rule 是 <code>EnsureRequirements</code>，它会确保输入数据的 <code>Partitioning</code> 满足 <code>SparkPlan</code> 中规定的 childDistribution，如果不满足则会通过添加子结点等方式尝试修复。</p>
<p>最终，<code>toRDD</code> 通过调用 <code>SparkPlan</code> 的 <code>execute</code> 方法，获取到计算结果。</p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>至此，我们大概了解了 SparkSQL 是如何处理用户的 SQL 语句，如何一步一步把它解析成 Logical Plan 再解析成 Physical Plan 再变成结果 RDD。如此粗略的介绍实在很难让你就此成为 SparkSQL 大师，因为 Catalyst 还有相当多的代码量用于定义优化规则即 Logical/Physical Plan 转换规则。以后我会考虑出一些进阶篇来讲讲这之中一些进阶级的细节实现，敬请期待咯。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Spark Catalyst 源码解析：Planner 与 RDD</p><p><a href="https://mr-dai.github.io/sparksql_catalyst_source_5/">https://mr-dai.github.io/sparksql_catalyst_source_5/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Robert Peng</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2015-08-21</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2015-08-21</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Spark/">Spark</a><a class="link-muted mr-2" rel="tag" href="/tags/SparkSQL/">SparkSQL</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/alipay-qrcode.png" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wechat-qrcode.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/sparksql_catalyst_source_6/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Spark Catalyst 进阶：Parser 词素</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/sparksql_catalyst_source_4/"><span class="level-item">Spark Catalyst 源码解析：Analyzer 与 Optimizer</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://mr-dai.github.io/sparksql_catalyst_source_5/';
            this.page.identifier = 'sparksql_catalyst_source_5/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'robertpsblog' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="呆呆"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">呆呆</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>中国广州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">49</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">35</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://www.zhihu.com/people/robert.peng" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="知乎" href="https://www.zhihu.com/people/robert.peng"><i class="fab fa-zhihu"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Mr-Dai"><i class="fab fa-github"></i></a></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Bash/"><span class="tag">Bash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bigtable/"><span class="tag">Bigtable</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GC/"><span class="tag">GC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Go/"><span class="tag">Go</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gradle/"><span class="tag">Gradle</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Groovy/"><span class="tag">Groovy</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HBase/"><span class="tag">HBase</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hadoop/"><span class="tag">Hadoop</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hive-ThriftServer/"><span class="tag">Hive ThriftServer</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JVM/"><span class="tag">JVM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MIT-6-824/"><span class="tag">MIT 6.824</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mesos/"><span class="tag">Mesos</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MongoDB/"><span class="tag">MongoDB</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Paxos/"><span class="tag">Paxos</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Raft/"><span class="tag">Raft</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spark/"><span class="tag">Spark</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spark-SQL/"><span class="tag">Spark SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SparkSQL/"><span class="tag">SparkSQL</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Windows/"><span class="tag">Windows</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Yarn/"><span class="tag">Yarn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ZooKeeper/"><span class="tag">ZooKeeper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/etcd/"><span class="tag">etcd</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%BB%E4%BB%8E%E5%A4%87%E4%BB%BD/"><span class="tag">主从备份</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%85%B1%E8%AF%86/"><span class="tag">分布式共识</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%86%85%E5%AD%98/"><span class="tag">分布式内存</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"><span class="tag">分布式存储</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"><span class="tag">分布式系统</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/"><span class="tag">分布式计算</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%A2%E9%98%9F%E5%8D%8F%E4%BD%9C/"><span class="tag">团队协作</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/"><span class="tag">集群资源调度</span><span class="tag">2</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#SparkPlanner"><span class="level-left"><span class="level-item">1</span><span class="level-item">SparkPlanner</span></span></a></li><li><a class="level is-mobile" href="#toRDD"><span class="level-left"><span class="level-item">2</span><span class="level-item">toRDD</span></span></a></li><li><a class="level is-mobile" href="#结语"><span class="level-left"><span class="level-item">3</span><span class="level-item">结语</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/avatar.png" alt="Robert Peng&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Robert Peng</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="我的 Github" href="https://github.com/Mr-Dai"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>