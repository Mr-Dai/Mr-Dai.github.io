---
layout: post_original
title: Spark Catalyst 进阶：Join
author: Robert Peng
category: Spark
---
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushScala.js"></script>
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushSql.js"></script>
<h2 class="jump">写作背景</h2>
<p>
	在之前的文章中，我们已经了解了SparkSQL把SQL语句变为SparkJob的过程。这个过程我们只是做了一个Overview，
	具体不同的语句会变为怎样的Job我们并未一一列举。实际上列举起来是一件相当大工程的事。
</p>
<p>
	在那么多的SQL操作中，有那么一个操作十分常用，但又十分耗时，那就是Join操作。在这篇文章里，我们将深入探讨SparkSQL会对不同的Join做出怎样的操作。
</p>
<h2 class="jump">什么是Join？</h2>
<p>在SQL中，Join用于根据两个或多个表中的列之间的关系，从这些表中查询数据。表达Join的方式有两种：</p>
<pre class="brush: sql">
SELECT Persons.LastName, Persons.FirstName, Orders.OrderNo
FROM Persons, Orders
WHERE Persons.Id_P = Orders.Id_P;

-- 或

SELECT Persons.LastName, Persons.FirstName, Orders.OrderNo
FROM Persons
INNER JOIN Orders
ON Persons.Id_P = Orders.Id_P
ORDER BY Persons.LastName;
</pre>
<p>
	实际上，第一种方式更像是SQL的语法糖，理论上而言我们更偏向后一种写法。这种使用关键字<code>JOIN</code>的规范写法使用<code>ON</code>关键字表明了Join的条件，
	同时在<code>JOIN</code>前面加上了一个<code>INNER</code>来表明要执行的Join的类型。SparkSQL支持的SQL操作有以下几种：
</p>
<table class="table table-bordered table-striped">
  <tr>
	<td>Inner Join</td>
	<td>使用比较运算符根据每个表共有的列的值匹配两个表中的行</td>
  </tr>
  <tr>
	<td>Left Semi Join</td>
	<td>对于左表的每个键值，在右表中找到第一个匹配的键值便返回</td>
  </tr>
  <tr>
	<td>Left Outer Join</td>
	<td>左向外联接的结果集包括LEFT OUTER子句中指定的左表的所有行，而不仅仅是联接列所匹配的行。如果左表的某行在右表中没有匹配行，则在相关联的结果集行中右表的所有选择列表列均为空值。</td>
  </tr>
  <tr>
	<td>Right Outer Join</td>
	<td>右向外联接是左向外联接的反向联接。将返回右表的所有行。如果右表的某行在左表中没有匹配行，则将为左表返回空值。</td>
  </tr>
  <tr>
	<td>Full Outer Join</td>
	<td>完整外部联接返回左表和右表中的所有行。当某行在另一个表中没有匹配行时，则另一个表的选择列表列包含空值。如果表之间有匹配行，则整个结果集行包含基表的数据值。   </td>
  </tr>
</table>
<p>接下来我们就开始看看SparkSQL会怎么处理这些JOIN语句。</p>
<h2 class="jump">Parser</h2>
<p>
	首先JOIN语句要变成Logical Plan就需要先经过Parser。根据我们之前学习过的内容来判断，JOIN语句相关的解析规则在<code>SqlParser</code>类中：
</p>
<pre class="brush: scala">
class SqlParser extends AbstractSparkSQLParser with DataTypeParser {

  // ...
  
  // 直接查找关键字`Join`，发现在relations中创建了这样一个实例
  protected lazy val relations: Parser[LogicalPlan] =
  // 我们知道relation指代的是一张表，那么在遇到像`Table1, Table2`这样的语句就会进入这里
    ( relation ~ rep1("," ~> relation) ^^ {
        case r1 ~ joins => joins.foldLeft(r1) { case(lhs, r) => Join(lhs, r, Inner, None) } }
		// 对于这样的语句，这里的做法是foldLeft地形成了一个由Inner Join组成的单边二叉树
    | relation
    )
	
  protected lazy val select: Parser[LogicalPlan] =
    SELECT ~> DISTINCT.? ~
      repsep(projection, ",") ~
	  // FROM这里引用了上面的relations，由此可见SparkSQL支持我们提到的第一种SQL写法，产生的是一个Inner Join
      (FROM   ~> relations).? ~
      (WHERE  ~> expression).? ~
      (GROUP  ~  BY ~> rep1sep(expression, ",")).? ~
      (HAVING ~> expression).? ~
      sortType.? ~
      (LIMIT  ~> expression).? ^^ {
        case d ~ p ~ r ~ f ~ g ~ h ~ o ~ l =>
          val base = r.getOrElse(OneRowRelation)
          val withFilter = f.map(Filter(_, base)).getOrElse(base)
          val withProjection = g
            .map(Aggregate(_, assignAliases(p), withFilter))
            .getOrElse(Project(assignAliases(p), withFilter))
          val withDistinct = d.map(_ => Distinct(withProjection)).getOrElse(withProjection)
          val withHaving = h.map(Filter(_, withDistinct)).getOrElse(withDistinct)
          val withOrder = o.map(_(withHaving)).getOrElse(withHaving)
          val withLimit = l.map(Limit(_, withOrder)).getOrElse(withOrder)
          withLimit
      }

  // ...	  
	  
  // Join实例另一次出现的位置在这里	  
  protected lazy val joinedRelation: Parser[LogicalPlan] =
  // 这里对应的语句便是 JOIN `table` [ON ...]
    relationFactor ~ rep1(joinType.? ~ (JOIN ~> relationFactor) ~ joinConditions.?) ^^ {
      case r1 ~ joins =>
        joins.foldLeft(r1) { case (lhs, jt ~ rhs ~ cond) =>
          Join(lhs, rhs, joinType = jt.getOrElse(Inner), cond)
		  // 注意这里Join类型在未指定时为Inner Join。同时注意这里的cond是个Option[Expression]
        }
		// 这里同样是foldLeft地形成了一个Join的二叉树
    }

  protected lazy val joinConditions: Parser[Expression] =
    ON ~> expression

  // 通过在JOIN关键字前加入如下关键字可以改变Join的类型	
  protected lazy val joinType: Parser[JoinType] =
    ( INNER           ^^^ Inner
    | LEFT  ~ SEMI    ^^^ LeftSemi
    | LEFT  ~ OUTER.? ^^^ LeftOuter
    | RIGHT ~ OUTER.? ^^^ RightOuter
    | FULL  ~ OUTER.? ^^^ FullOuter
    )

  // ...	

}
</pre>
<p>由此，输入到SparkSQL中的SQL语句与Join类型的关系可以总结如下：</p>
<table class="table table-bordered table-striped">
  <tr>
    <td>Inner Join</td>
	<td>
	  SELECT ... FROM table1, table2[, ...] ...
	  <hr />
	  SELECT ... FROM ... JOIN ... [ON ...]
	</td>
  </tr>
  <tr>
    <td>Left Semi Join</td>
	<td>SELECT ... FROM ... LEFT SEMI JOIN ... [ON ...]
  </tr>
  <tr>
    <td>Left Outer Join</td>
	<td>SELECT ... FROM ... LEFT [OUTER] JOIN ... [ON ...]
  </tr>
  <tr>
    <td>Right Outer Join</td>
	<td>SELECT ... FROM ... RIGHT [OUTER] JOIN ... [ON ...]
  </tr>
  <tr>
    <td>Full Outer Join</td>
	<td>SELECT ... FROM ... FULL [OUTER] JOIN ... [ON ...]
  </tr>
</table>
<p>接下来我们来看一下表示Logical Plan的<code>Join</code>类：</p>
<pre class="brush: scala">
case class Join(
  left: LogicalPlan,
  right: LogicalPlan,
  joinType: JoinType, // JoinType包括5个case object，对应5个Join类型
  condition: Option[Expression]) extends BinaryNode {

  override def output: Seq[Attribute] = {
    joinType match {
      case LeftSemi =>
        left.output
      case LeftOuter =>
        left.output ++ right.output.map(_.withNullability(true))
      case RightOuter =>
        left.output.map(_.withNullability(true)) ++ right.output
      case FullOuter =>
        left.output.map(_.withNullability(true)) ++ right.output.map(_.withNullability(true))
      case _ =>
        left.output ++ right.output
    }
  }

  // 防止用户构成了一些根本无法Join的左右子树
  private def selfJoinResolved: Boolean = left.outputSet.intersect(right.outputSet).isEmpty

  override lazy val resolved: Boolean = {
    childrenResolved && !expressions.exists(!_.resolved) && selfJoinResolved
  }
}
</pre>
<p>Join的Logical Plan本身只有一个类，显得十分简单。</p>
<h2 class="jump">Analyzer</h2>
<p>
	在通过Parser得到Unresolved Logical Plan以后，下一步就轮到Analyzer了。经过之前的学习，我们知道Analyzer所应用的全部规则都位于<code>Analyzer.scala</code>中：
</p>
<pre class="brush: scala">
class Analyzer(
    catalog: Catalog,
    registry: FunctionRegistry,
    conf: CatalystConf,
    maxIterations: Int = 100)
  extends RuleExecutor[LogicalPlan] with HiveTypeCoercion with CheckAnalysis {
  
  // ...
  
  object ResolveReferences extends Rule[LogicalPlan] {
    def apply(plan: LogicalPlan): LogicalPlan = plan transformUp {
	
	  // ...
	
	  // 同样，经过搜索，Join仅出现在该分支中
	  // 该处用于处理之前的selfJoinResolved为false的情况
      case j @ Join(left, right, _, _) if left.outputSet.intersect(right.outputSet).nonEmpty =>
	    // 找出冲突的Attribute
        val conflictingAttributes = left.outputSet.intersect(right.outputSet)
        logDebug(s"Conflicting attributes ${conflictingAttributes.mkString(",")} in $j")

        // ...
		
		// 根据右子树类型的不同将右子树进行了替换
        val newRight = right transformUp {
          case r if r == oldRelation => newRelation
        } transformUp {
          case other => other transformExpressions {
            case a: Attribute => attributeRewrites.get(a).getOrElse(a)
          }
        }
        j.copy(right = newRight)
		
		// ...
	}
  // ...
}	
</pre>
<p>看起来，Analyzer对Join树做的操作仅在于解决一些很奇怪的属性冲突。这种问题属于少数派，相信大多数时候SparkSQL都不会进入这个分支。</p>
<h2 class="jump">Optimizer</h2>
<p>接下来我们来看一下Optimizer是否有与Join相关的优化逻辑：</p>
<pre class="brush: scala">
// Join首先出现在了这个Rule中
object ColumnPruning extends Rule[LogicalPlan] {
  
  // 对c进行剪枝，只需要包含在allReferences中的属性
  // 通过在c之上加上一个Project计划来实现
  private def prunedChild(c: LogicalPlan, allReferences: AttributeSet) =
    if ((c.outputSet -- allReferences.filter(c.outputSet.contains)).nonEmpty) {
      Project(allReferences.filter(c.outputSet.contains).toSeq, c)
    } else {
      c
    }

  def apply(plan: LogicalPlan): LogicalPlan = plan transform {
    // ...

    // Join后只SELECT了少部分属性
    case Project(projectList, Join(left, right, joinType, condition)) =>
      // Collect the list of all references required either above or to evaluate the condition.
      val allReferences: AttributeSet =
        AttributeSet(
          projectList.flatMap(_.references.iterator)) ++
          condition.map(_.references).getOrElse(AttributeSet(Seq.empty))
	  // 包括SELECT了的属性以及出现在了ON中的属性  

      /** Applies a projection only when the child is producing unnecessary attributes */
      def pruneJoinChild(c: LogicalPlan): LogicalPlan = prunedChild(c, allReferences)
      // 先对左右子树进行Project再Join
      Project(projectList, Join(pruneJoinChild(left), pruneJoinChild(right), joinType, condition))

    // 消除LeftSemiJoin中右子树中不必要的属性
    case Join(left, right, LeftSemi, condition) =>
      // Collect the list of all references required to evaluate the condition.
      val allReferences: AttributeSet =
        condition.map(_.references).getOrElse(AttributeSet(Seq.empty))
      // 包括出现在ON中的属性
      Join(left, prunedChild(right, allReferences), LeftSemi, condition)

    // ...
  }
}
</pre>
<p>
	由此可见，Optimizer对Join操作做出的优化，在于将SELECT以及ON所包含的属性考虑进去后，将左右子树中不需要的属性先删去再Join，
	以此来优化Join的性能。
</p>
<p>至此，Logical Plan的处理过程就全部完成了。接下来就是重中之重了。</p>
<h2 class="jump">Planner</h2>
<p>我们知道，Planner将Optimized Logical Plan变为Physical Plan的规则全都位于<code>SparkStrategies</code>类中，那我们直接看吧：</p>
<pre class="brush: scala">
private[sql] abstract class SparkStrategies extends QueryPlanner[SparkPlan] {
  self: SQLContext#SparkPlanner =>
  
  object LeftSemiJoin extends Strategy with PredicateHelper {
    def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {
	  // ExtractEquiJoinKeys用于将出现在condition的相等条件中的属性拆分出来
	  // leftKeys和rightKeys分别对应属于左子树和属于右子树的Attribute
	  // 相同索引值的leftKey和rightKey构成原本的condition中的一对相等条件，即`leftKey(i) = rightKey(i)`
	  // 剩余的非相等条件会被放入到结果的condition中
	  // 该unapply函数当且仅当leftKeys和rightKeys不为空时会有返回
      case ExtractEquiJoinKeys(LeftSemi, leftKeys, rightKeys, condition, left, right)
	    // 该参数默认为 10 * 1024 * 1024，即10mb
        if sqlContext.conf.autoBroadcastJoinThreshold > 0 &&
          right.statistics.sizeInBytes &lt;= sqlContext.conf.autoBroadcastJoinThreshold =>
		// 右子树 &lt;= 10 MB
        // 产生一个BroadcastLeftSemiJoinHash实例		
        val semiJoin = joins.BroadcastLeftSemiJoinHash(
          leftKeys, rightKeys, planLater(left), planLater(right))
		// 再把剩下的非相等条件以Filter的形式覆盖上去  
        condition.map(Filter(_, semiJoin)).getOrElse(semiJoin) :: Nil
      case ExtractEquiJoinKeys(LeftSemi, leftKeys, rightKeys, condition, left, right) =>
	    // 情况基本同上，只是这里改为使用LeftSemiJoinHash实例
        val semiJoin = joins.LeftSemiJoinHash(
          leftKeys, rightKeys, planLater(left), planLater(right))
        condition.map(Filter(_, semiJoin)).getOrElse(semiJoin) :: Nil
      // no predicate can be evaluated by matching hash keys
      case logical.Join(left, right, LeftSemi, condition) =>
	    // 剩下的Left Semi Join就直接变成LeftSemiJoinBNL实例
        joins.LeftSemiJoinBNL(planLater(left), planLater(right), condition) :: Nil
      case _ => Nil
    }
  }
  
  // 到这里，Left Semi Join已经全部由上面那个Strategy变成Physical Plan了
  object HashJoin extends Strategy with PredicateHelper {

    private[this] def makeBroadcastHashJoin(
        leftKeys: Seq[Expression],
        rightKeys: Seq[Expression],
        left: LogicalPlan,
        right: LogicalPlan,
        condition: Option[Expression],
        side: joins.BuildSide) = {
		// 产生一个BroadcastHashJoin实例，并用Filter把剩余的condition盖了上去
      val broadcastHashJoin = execution.joins.BroadcastHashJoin(
        leftKeys, rightKeys, side, planLater(left), planLater(right))
      condition.map(Filter(_, broadcastHashJoin)).getOrElse(broadcastHashJoin) :: Nil
    }

    def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {
      case ExtractEquiJoinKeys(Inner, leftKeys, rightKeys, condition, left, right)
        if sqlContext.conf.autoBroadcastJoinThreshold > 0 &&
           right.statistics.sizeInBytes &lt;= sqlContext.conf.autoBroadcastJoinThreshold =>
		// Inner Join，ON里有相等条件，右子树不算大 -> BroadcastHashJoin
        makeBroadcastHashJoin(leftKeys, rightKeys, left, right, condition, joins.BuildRight)

      case ExtractEquiJoinKeys(Inner, leftKeys, rightKeys, condition, left, right)
        if sqlContext.conf.autoBroadcastJoinThreshold > 0 &&
           left.statistics.sizeInBytes &lt;= sqlContext.conf.autoBroadcastJoinThreshold =>
		   // Inner Join，ON里有相等条件，左子树不算大 -> BroadcastHashJoin
          makeBroadcastHashJoin(leftKeys, rightKeys, left, right, condition, joins.BuildLeft)

      case ExtractEquiJoinKeys(Inner, leftKeys, rightKeys, condition, left, right)
        if sqlContext.conf.sortMergeJoinEnabled =>
		// Inner Join，ON里有相等条件，sortMergeJoin设置被开启 -> SortMergeJoin
        val mergeJoin =
          joins.SortMergeJoin(leftKeys, rightKeys, planLater(left), planLater(right))
        condition.map(Filter(_, mergeJoin)).getOrElse(mergeJoin) :: Nil

      case ExtractEquiJoinKeys(Inner, leftKeys, rightKeys, condition, left, right) =>
        val buildSide =
          if (right.statistics.sizeInBytes &lt;= left.statistics.sizeInBytes) {
            joins.BuildRight
          } else {
            joins.BuildLeft
          }
		// Inner Join，ON里有相等条件 -> ShuffledHashJoin，以较小的一边作为buildSide
        val hashJoin = joins.ShuffledHashJoin(
          leftKeys, rightKeys, buildSide, planLater(left), planLater(right))
        condition.map(Filter(_, hashJoin)).getOrElse(hashJoin) :: Nil

      case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =>
	  // ON里有相等条件 -> HashOuterJoin
        joins.HashOuterJoin(
          leftKeys, rightKeys, joinType, condition, planLater(left), planLater(right)) :: Nil

      case _ => Nil
    }
  }
  
  // ...
  
  object CartesianProduct extends Strategy {
    def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {
      case logical.Join(left, right, _, None) =>
	    // 没有ON语句 -> CartesianProduct
        execution.joins.CartesianProduct(planLater(left), planLater(right)) :: Nil
      case logical.Join(left, right, Inner, Some(condition)) =>
	    // Inner Join，有ON语句 -> CartesianProduct再盖一个Filter
        execution.Filter(condition,
          execution.joins.CartesianProduct(planLater(left), planLater(right))) :: Nil
      case _ => Nil
    }
  }
  
  object BroadcastNestedLoopJoin extends Strategy {
    def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {
      case logical.Join(left, right, joinType, condition) =>
        val buildSide =
          if (right.statistics.sizeInBytes &lt;= left.statistics.sizeInBytes) {
            joins.BuildRight
          } else {
            joins.BuildLeft
          }
		// 剩下的JOIN -> 以较小一侧为buildSide的BroadcastNestedLoopJoin
        joins.BroadcastNestedLoopJoin(
          planLater(left), planLater(right), buildSide, joinType, condition) :: Nil
      case _ => Nil
    }
  }
  
  // ...
  
}  
</pre>
<p>通过阅读上述代码，我们找到了如下几个与JOIN有关的SparkPlan：</p>
<ul>
  <li><code>BroadcastLeftSemiJoinHash</code>：Left Semi Join，ON中存在相等条件，右子树小于阈值（默认10MB）</li>
  <li><code>LeftSemiJoinHash</code>：Left Semi Join，ON中存在相等条件</li>
  <li><code>LeftSemiJoinBNL</code>：Left Semi Join</li>
  <li><code>BroadcastHashJoin</code>：Inner Join，ON里有相等条件，左子树或右子树小于阈值（默认10MB）。以较小的一侧为BuildSide</li>
  <li><code>SortMergeJoin</code>：Inner Join，ON里有相等条件，sortMergeJoin设置被开启</li>
  <li><code>ShuffledHashJoin</code>：Inner Join，ON里有相等条件。以较小的一侧为buildSide。</li>
  <li><code>HashOuterJoin</code>：ON里有相等条件</li>
  <li><code>CartesianProduct</code>：Inner Join，有ON语句</li>
  <li><code>CartesianProduct</code>：没有ON语句</li>
  <li><code>BroadcastNestedLoopJoin</code>：剩下的都是它</li>
</ul>
<p>足足10种用于Join的Physical Plan。看来SparkSQL也知道这是最关键的操作。接下来我们逐个解析这些Plan。</p>
<h2 class="jump">Physical Plan</h2>
<h3>BroadcastLeftSemiJoinHash</h3>
<p>准入条件：Left Semi Join，ON中存在相等条件，右子树小于阈值（默认10MB）</p>
<pre class="brush: scala">
case class BroadcastLeftSemiJoinHash(
    leftKeys: Seq[Expression],
    rightKeys: Seq[Expression],
    left: SparkPlan,
    right: SparkPlan) extends BinaryNode with HashJoin {
    // 继承自HashJoin
    // ...
}
</pre>
<p>好，在看之前我们先看看<code>HashJoin</code>：</p>
<pre class="brush: scala">
trait HashJoin {
  self: SparkPlan =>

  // 这些成员大部分由子类的构造函数传入
  val leftKeys: Seq[Expression]
  val rightKeys: Seq[Expression]
  val buildSide: BuildSide  // 只有两个子类case object：BuildLeft和BuildRight
  val left: SparkPlan
  val right: SparkPlan

  // buildPlan为buildSide指定的那边的SparkPlan，streamedPlan则为剩下那个
  protected lazy val (buildPlan, streamedPlan) = buildSide match {
    case BuildLeft => (left, right)
    case BuildRight => (right, left)
  }

  // buildKeys为buildSide指定的那边的keys，streamedKeys则为剩下那边的keys
  protected lazy val (buildKeys, streamedKeys) = buildSide match {
    case BuildLeft => (leftKeys, rightKeys)
    case BuildRight => (rightKeys, leftKeys)
  }

  override def output: Seq[Attribute] = left.output ++ right.output

  // abstract class Projection extends (Row => Row)
  // 根据key和output生成了一个key generator
  // 子类会使用这个generator为每个Row生成一个key(也是一个Row)并放入到HashSet或HashMap
  // 想必这个key应该实现了比较高效的hashCode方法
  @transient protected lazy val buildSideKeyGenerator: Projection =
    newProjection(buildKeys, buildPlan.output)
  
  // 同理
  @transient protected lazy val streamSideKeyGenerator: () => MutableProjection =
    newMutableProjection(streamedKeys, streamedPlan.output)

  // 直接看比较复杂，等用到时我们再进行解析
  protected def hashJoin(streamIter: Iterator[Row], hashedRelation: HashedRelation): Iterator[Row] = {
    // ...
  }
}
</pre>
<p>好，我们再回到<code>BroadcastLeftSemiJoinHash</code>：</p>
<pre class="brush: scala">
case class BroadcastLeftSemiJoinHash(
    leftKeys: Seq[Expression],
    rightKeys: Seq[Expression],
    left: SparkPlan,
    right: SparkPlan) extends BinaryNode with HashJoin {

  // 以右子树为buildSide
  override val buildSide: BuildSide = BuildRight
  
  // 输出属性集与左子树相同
  override def output: Seq[Attribute] = left.output

  // SparkPlan入口方法
  protected override def doExecute(): RDD[Row] = {
    // 获取右子树结果集
    val buildIter = buildPlan.execute().map(_.copy()).collect().toIterator
    
	val hashSet = new java.util.HashSet[Row]()
    var currentRow: Row = null
    // 利用右子树结果集构建一个key的HashSet
    while (buildIter.hasNext) {
      currentRow = buildIter.next()
	  // 利用buildSideKeyGenerator为右子树结果集的每个Row都生成一个key
      val rowKey = buildSideKeyGenerator(currentRow)
      if (!rowKey.anyNull) {
        val keyExists = hashSet.contains(rowKey)
        if (!keyExists) {
		  // key们放入到hashSet中
          hashSet.add(rowKey)
        }
      }
    }

	// 将hashSet广播出去
    val broadcastedRelation = sparkContext.broadcast(hashSet)

    streamedPlan.execute().mapPartitions { streamIter =>
	  // 利用streamSideKeyGenerator为左子树的Row生成key
      val joinKeys = streamSideKeyGenerator()
      streamIter.filter(current => {
        !joinKeys(current).anyNull && broadcastedRelation.value.contains(joinKeys.currentValue)
		// 在之前的hashSet中包含本key，则放入到结果集中
      })
    }
  }
}
</pre>
<p>
	首先，在实例化结果RDD的时候，右子树的结果就已经计算完毕并被收集回来，将右子树的Row变为key并放入HashSet再广播出去的动作将由Master独自完成。
	在结果RDD的<code>collect</code>或其他方法被调用的时候，左子树的每个Partition同样会将自己的Row变为key，并与之前广播的HashSet中的元素进行比对，
	返回key存在于HashSet中的记录。
</p>
<p>
	RDD的计算本该是lazy的。诚然，这里左子树的计算确实是lazy的，但右子树不是，右子树在RDD实例化的时候就已经计算完毕了，因此该方法不太适用于较大的右子树。
	不过，能产生这种SparkPlan本来就要求LeftSemiJoin操作右子树的Statistics值小于一定的阈值，因此这样做还是合理的。
</p>
<h3>LeftSemiJoinHash</h3>
<p>准入条件：Left Semi Join，ON中存在相等条件</p>
<pre class="brush: scala">
case class LeftSemiJoinHash(
    leftKeys: Seq[Expression],
    rightKeys: Seq[Expression],
    left: SparkPlan,
    right: SparkPlan) extends BinaryNode with HashJoin {

  // 同样以右子树作为BuildSide	
  override val buildSide: BuildSide = BuildRight

  // 表明对于leftKeys以及rightKeys的每个属性，具有相同值的Row可能分散在不同的Partition中
  override def requiredChildDistribution: Seq[ClusteredDistribution] =
    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil

  // 同样直接以左子树的输入作为输出	
  override def output: Seq[Attribute] = left.output

  protected override def doExecute(): RDD[Row] = {
    // 先计算出右子树的结果RDD
	// 再把左右子树的Partition们zip起来（意味着左右子树的结果Partition数相同）
    buildPlan.execute().zipPartitions(streamedPlan.execute()) { (buildIter, streamIter) =>
	  // 在zip起来的Partition内采取了和之前一样的算法
      val hashSet = new java.util.HashSet[Row]()
      var currentRow: Row = null

      // Create a Hash set of buildKeys
	  // 先构建右子树的key set
      while (buildIter.hasNext) {
        currentRow = buildIter.next()
        val rowKey = buildSideKeyGenerator(currentRow)
        if (!rowKey.anyNull) {
          val keyExists = hashSet.contains(rowKey)
          if (!keyExists) {
            hashSet.add(rowKey)
          }
        }
      }

	  // 再从左子树中筛选返回
      val joinKeys = streamSideKeyGenerator()
      streamIter.filter(current => {
        !joinKeys(current).anyNull && hashSet.contains(joinKeys.currentValue)
      })
    }
  }
}
</pre>
<p>
	可见，其核心算法本身和<code>BroadcastLeftSemiJoinHash</code>并无不同，但却使用了<code>zipPartitions</code>方法来计算两个RDD的Join结果。
	如果要确保结果完全正确，就需要两个RDD的Partition数相同，同时在key上有着相同值的Row必然处于index相同的Partition内。
	我暂时无法理解SparkSQL要如何保证这两个条件同时满足，只能先放一放了。
</p>
<h3>LeftSemiJoinBNL</h3>
<p>准入条件：Left Semi Join</p>
<pre class="brush: scala">
case class LeftSemiJoinBNL(streamed: SparkPlan, broadcast: SparkPlan, condition: Option[Expression])
// 注：实例化时传入的streamed为左子树，broadcast为右子树
  extends BinaryNode {
  // 由于ON语句中不再有相等条件，因此该算法也不使用HashSet来查找相同元素了

  override def left: SparkPlan = streamed
  override def right: SparkPlan = broadcast
  
  // 输出的属性与Partition方法与左子树保持一致
  override def outputPartitioning: Partitioning = streamed.outputPartitioning
  override def output: Seq[Attribute] = left.output

  // 根据传入的ON condition生成了一个(Row) => Boolean
  @transient private lazy val boundCondition =
    newPredicate(condition.getOrElse(Literal(true)), left.output ++ right.output)

  protected override def doExecute(): RDD[Row] = {
    // 计算右子树并把结果广播出去
    val broadcastedRelation =
      sparkContext.broadcast(broadcast.execute().map(_.copy()).collect().toIndexedSeq)

    streamed.execute().mapPartitions { streamedIter =>
      val joinedRow = new JoinedRow

	  // 筛选吻合的行
      streamedIter.filter(streamedRow => {
        var i = 0
        var matched = false

		// 遍历右子树结果，并在找到第一个匹配结果的时候结束循环
        while (i &lt; broadcastedRelation.value.size && !matched) {
          val broadcastedRow = broadcastedRelation.value(i)
		  // 利用当前的两个Row生成一个JoinedRow并验证是否吻合条件
          if (boundCondition(joinedRow(streamedRow, broadcastedRow))) {
            matched = true
          }
          i += 1
        }
        matched
      })
    }
  }
}
</pre>
<p>没什么特别，相当好理解。</p>
<h3>BroadcastHashJoin</h3>
<p>准入条件：Inner Join，ON里有相等条件，左子树或右子树小于阈值（默认10MB），以较小的一侧为BuildSide</p>
<pre class="brush: scala">
case class BroadcastHashJoin(
    leftKeys: Seq[Expression],
    rightKeys: Seq[Expression],
    buildSide: BuildSide,
    left: SparkPlan,
    right: SparkPlan)
  extends BinaryNode with HashJoin {

  val timeout: Duration = {
    // 默认为 5*60，即5分钟
    val timeoutValue = sqlContext.conf.broadcastTimeout
    if (timeoutValue &lt; 0) {
      Duration.Inf
    } else {
      timeoutValue.seconds
    }
  }
  
  override def outputPartitioning: Partitioning = streamedPlan.outputPartitioning

  override def requiredChildDistribution: Seq[Distribution] =
    UnspecifiedDistribution :: UnspecifiedDistribution :: Nil

  // 启动一个异步计算
  @transient
  private val broadcastFuture = future {
    // Note that we use .execute().collect() because we don't want to convert data to Scala types
	// 收集较小子树的结果
    val input: Array[Row] = buildPlan.execute().map(_.copy()).collect()
	// 生成一个Key到Row(s)的HashMap并广播出去
    val hashed = HashedRelation(input.iterator, buildSideKeyGenerator, input.length)
    sparkContext.broadcast(hashed)
  }(BroadcastHashJoin.broadcastHashJoinExecutionContext)

  protected override def doExecute(): RDD[Row] = {
    // 等待异步计算完成
    val broadcastRelation = Await.result(broadcastFuture, timeout)

    streamedPlan.execute().mapPartitions { streamedIter =>
      hashJoin(streamedIter, broadcastRelation.value)
    }
  }
}
</pre>
<p>我们看到，在最后<code>BroadcastHashJoin</code>调用了父类<code>HashJoin</code>的<code>hashJoin</code>方法。我们来看看那个方法：</p>
<pre class="brush: scala">
trait HashJoin {
  self: SparkPlan =>
  
  // ...
  
  // 参考上面，这里传入的hashedRelation实际上是Key到Row(s)的HashMap
  protected def hashJoin(streamIter: Iterator[Row], hashedRelation: HashedRelation): Iterator[Row] = {
    new Iterator[Row] {
      private[this] var currentStreamedRow: Row = _
      private[this] var currentHashMatches: CompactBuffer[Row] = _
      private[this] var currentMatchPosition: Int = -1

      private[this] val joinRow = new JoinedRow2

      private[this] val joinKeys = streamSideKeyGenerator()
	  
	  /**
	   * 这里我们需要考虑我们平常使用Iterator的方式，基本都是这样：
	   * while (iterator.hasNext) {
	   *    sth = iterator.next 
	   *    ..
	   * }
	   * 意味着hasNext和next会被交替调用
	   */
	  override final def hasNext: Boolean =
        (currentMatchPosition != -1 && currentMatchPosition &lt; currentHashMatches.size) ||
          (streamIter.hasNext && fetchNext())
	  // 在最初时，我们会先调用hasNext，这里进入第二条条件判断式，fetchNext被调用，获取到一个currentHashMatches
      // 接下来，hasNext在第一条条件判断式就会返回true，第二条被短路。我们通过调用next，让迭代器遍历currentHashMatches
      // 当一个currentHashMatches被遍历完毕，第一条条件判断式会返回false，这里就会进入第二条条件判断式，由fetchNext获取下一个currentHashMatches
	  // 综上，当且仅当fetchNext或streamIter.hasNext返回false时（实际上fetchNext也只有在!streamIter.hasNext时才会返回false），这里会返回false
      

      override final def next(): Row = {
	    // 遍历在fetchNext中拿到的currentHashMatches，生成JoinedRow
        val ret = buildSide match {
          case BuildRight => joinRow(currentStreamedRow, currentHashMatches(currentMatchPosition))
          case BuildLeft => joinRow(currentHashMatches(currentMatchPosition), currentStreamedRow)
        }
        currentMatchPosition += 1
        ret
      }

      // 找到下一个streamSide中吻合的条目
      private final def fetchNext(): Boolean = {
        currentHashMatches = null
        currentMatchPosition = -1

		// 找到一个吻合的条目并退出循环
        while (currentHashMatches == null && streamIter.hasNext) {
          currentStreamedRow = streamIter.next()
          if (!joinKeys(currentStreamedRow).anyNull) {
            currentHashMatches = hashedRelation.get(joinKeys.currentValue)
          }
        }

        if (currentHashMatches == null) {
          false // streamIter已完成遍历，故该迭代器也已完成遍历，返回false
        } else {
		  // 找到吻合的条目，迭代器再次初始化
          currentMatchPosition = 0
          true
        }
      }
    }
  }
}   
</pre>
<p>嗯，这背后确实是个标准的Hash Join算法，但我必须得说，这写得实在是太巧妙了。</p>
<p>
	<code>BroadcastHashJoin</code>实际上和<code>BroadcastLeftSemiJoinHash</code>很像，但后者的buildSide结果的收集是在<code>doExecute</code>被调用时进行，
	而前者在实例化时就已经以一个异步计算的形式开始了。考虑到SparkSQL的各种lazy变量，实际上前者的计算的启动时机比后者要早很多。
	前者在<code>planner.plan</code>的时候就已经开始了，而后者则要等到<code>QueryExecution#toRDD</code>。
</p>
<h3>SortMergeJoin</h3>
<p>准入条件：Inner Join，ON里有相等条件，sortMergeJoin设置被开启</p>
<pre class="brush: scala">
case class SortMergeJoin(
    leftKeys: Seq[Expression],
    rightKeys: Seq[Expression],
    left: SparkPlan,
    right: SparkPlan) extends BinaryNode {

  override def output: Seq[Attribute] = left.output ++ right.output
  override def outputPartitioning: Partitioning = left.outputPartitioning

  override def requiredChildDistribution: Seq[Distribution] =
    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil

  // this is to manually construct an ordering that can be used to compare keys from both sides
  private val keyOrdering: RowOrdering = RowOrdering.forSchema(leftKeys.map(_.dataType))

  private def requiredOrders(keys: Seq[Expression]): Seq[SortOrder] =
    keys.map(SortOrder(_, Ascending))
  // 左右子树出现在ON相等表达式中的属性按升序排序
  override def outputOrdering: Seq[SortOrder] = requiredOrders(leftKeys)
  override def requiredChildOrdering: Seq[Seq[SortOrder]] =
    requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil

  // 类似HashJoin的key generator	
  @transient protected lazy val leftKeyGenerator = newProjection(leftKeys, left.output)
  @transient protected lazy val rightKeyGenerator = newProjection(rightKeys, right.output)

  protected override def doExecute(): RDD[Row] = {
    // 或许到左右子树的结果RDD
    val leftResults = left.execute().map(_.copy())
    val rightResults = right.execute().map(_.copy())

	// 左右子树的Partition们zip起来
    leftResults.zipPartitions(rightResults) { (leftIter, rightIter) =>
      new Iterator[Row] {
        // Mutable per row objects.
        private[this] val joinRow = new JoinedRow5
        private[this] var leftElement: Row = _
        private[this] var rightElement: Row = _
        private[this] var leftKey: Row = _
        private[this] var rightKey: Row = _
        private[this] var rightMatches: CompactBuffer[Row] = _
        private[this] var rightPosition: Int = -1
        private[this] var stop: Boolean = false
        private[this] var matchKey: Row = _

        // 迭代器初始化
        initialize()
		
		// 将leftElement和rightElement分别指向左右侧第一个元素，并生成对应的key
		private def initialize() = {
          fetchLeft()
          fetchRight()
        }
		
		// 从左子树获取下一个Row
		private def fetchLeft() = {
          if (leftIter.hasNext) {
            leftElement = leftIter.next()
            leftKey = leftKeyGenerator(leftElement)
          } else {
            leftElement = null
          }
        }

		// 从右子树获取下一个Row
        private def fetchRight() = {
          if (rightIter.hasNext) {
            rightElement = rightIter.next()
            rightKey = rightKeyGenerator(rightElement)
          } else {
            rightElement = null
          }
        }

		// 同样考虑刚刚提到的Iterator的使用方式
        override final def hasNext: Boolean = nextMatchingPair()
		
		// 右迭代器搜索下一个与左侧匹配的条目 
        private def nextMatchingPair(): Boolean = {
          if (!stop && rightElement != null) {
            // 两边的指针一起跑，以找到第一个配对
            while (!stop && leftElement != null && rightElement != null) {
              val comparing = keyOrdering.compare(leftKey, rightKey)
			  // 找到配对，则stop为true，退出当前循环
              stop = comparing == 0 && !leftKey.anyNull			  
              if (comparing > 0 || rightKey.anyNull) {
                fetchRight() // 左边比右边大，右边前进一步
              } else if (comparing &lt; 0 || leftKey.anyNull) {
                fetchLeft() // 右边比左边大，左边前进一步
              }
            }
            rightMatches = new CompactBuffer[Row]()
            if (stop) {
              stop = false
			  // 将右侧的所有key相同的Row放入rightMatches，直到遇到第一个不同的key
              while (!stop && rightElement != null) {
                rightMatches += rightElement
                fetchRight()
                stop = keyOrdering.compare(leftKey, rightKey) != 0
              }
              if (rightMatches.size > 0) {
                rightPosition = 0
                matchKey = leftKey
              }
            }
          }
          rightMatches != null && rightMatches.size > 0
        }

        override final def next(): Row = {
          if (hasNext) {
            val joinedRow = joinRow(leftElement, rightMatches(rightPosition))
            rightPosition += 1
            if (rightPosition >= rightMatches.size) {
              rightPosition = 0
              fetchLeft() // 右侧匹配条目收集完毕，左侧前进一步
              if (leftElement == null || keyOrdering.compare(leftKey, matchKey) != 0) {
                stop = false // stop置为false，hasNext继续寻找下一对配对
                rightMatches = null
              }
            }
            joinedRow
          } else {
            // no more result
            throw new NoSuchElementException
          }
        }
      }
    }
  }
}
</pre>
<p>
	虽然算法不相同，但迭代器的设计思想上，<code>SortMergeJoin</code>和<code>BroadcastHashJoin</code>还是很像的，
	只是前者的迭代器在<code>next</code>方法里调用了<code>hasNext</code>，这样的设计更为安全，
	而后者如果在<code>next</code>之前没有调用过<code>hasNext</code>则会直接出错。
</p>
<h3>ShuffledHashJoin</h3>
<p>准入条件：Inner Join，ON里有相等条件。以较小的一侧为buildSide。</p>
<pre class="brush: scala">
case class ShuffledHashJoin(
    leftKeys: Seq[Expression],
    rightKeys: Seq[Expression],
    buildSide: BuildSide,
    left: SparkPlan,
    right: SparkPlan)
  extends BinaryNode with HashJoin {

  override def outputPartitioning: Partitioning = left.outputPartitioning

  override def requiredChildDistribution: Seq[ClusteredDistribution] =
    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil

  protected override def doExecute(): RDD[Row] = {
    buildPlan.execute().zipPartitions(streamedPlan.execute()) { (buildIter, streamIter) =>
      val hashed = HashedRelation(buildIter, buildSideKeyGenerator)
      hashJoin(streamIter, hashed)
    }
  }
}
</pre>
<p>好像没什么需要说的，十分直观。</p>
<h3>HashOuterJoin</h3>
<p>准入条件：ON里有相等条件</p>
<pre class="brush: scala">
@DeveloperApi
case class HashOuterJoin(
    leftKeys: Seq[Expression],
    rightKeys: Seq[Expression],
    joinType: JoinType,
    condition: Option[Expression],
    left: SparkPlan,
    right: SparkPlan) extends BinaryNode {

  // 从这里看得出来，HashOuterJoin同时接受三种Outer Join，只要它们的ON里有相等条件
  override def outputPartitioning: Partitioning = joinType match {
    case LeftOuter => left.outputPartitioning
    case RightOuter => right.outputPartitioning
    case FullOuter => UnknownPartitioning(left.outputPartitioning.numPartitions)
    case x => throw new Exception(s"HashOuterJoin should not take $x as the JoinType")
  }

  override def requiredChildDistribution: Seq[ClusteredDistribution] =
    ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil

  override def output: Seq[Attribute] = {
    joinType match {
      case LeftOuter =>
        left.output ++ right.output.map(_.withNullability(true))
      case RightOuter =>
        left.output.map(_.withNullability(true)) ++ right.output
      case FullOuter =>
        left.output.map(_.withNullability(true)) ++ right.output.map(_.withNullability(true))
      case x =>
        throw new Exception(s"HashOuterJoin should not take $x as the JoinType")
    }
  }

  @transient private[this] lazy val DUMMY_LIST = Seq[Row](null)
  @transient private[this] lazy val EMPTY_LIST = Seq.empty[Row]

  @transient private[this] lazy val leftNullRow = new GenericRow(left.output.length)
  @transient private[this] lazy val rightNullRow = new GenericRow(right.output.length)
  @transient private[this] lazy val boundCondition =
    condition.map(newPredicate(_, left.output ++ right.output)).getOrElse((row: Row) => true)
  
  // 出于性能考虑，SparkSQL自行实现了三种迭代器
  private[this] def leftOuterIterator(
  // 注意：这里传入的joinedRow的left已经设定，key就是left的key。见doExecute
      key: Row, joinedRow: JoinedRow, rightIter: Iterable[Row]): Iterator[Row] = {
    val ret: Iterable[Row] = {
      if (!key.anyNull) {
        val temp = rightIter.collect {
          case r if boundCondition(joinedRow.withRight(r)) => joinedRow.copy()
        }  // 收集右侧所有匹配的条目
        if (temp.size == 0) {
          joinedRow.withRight(rightNullRow).copy :: Nil // 没有收集到，直接返回个NULL
        } else {
          temp // 收集到了，就全部输出
        }
      } else {
        joinedRow.withRight(rightNullRow).copy :: Nil
      }
    }
    ret.iterator
  }

  // 同上，轴对称一下而已
  private[this] def rightOuterIterator(
      key: Row, leftIter: Iterable[Row], joinedRow: JoinedRow): Iterator[Row] = {

    val ret: Iterable[Row] = {
      if (!key.anyNull) {
        val temp = leftIter.collect {
          case l if boundCondition(joinedRow.withLeft(l)) => joinedRow.copy
        }
        if (temp.size == 0) {
          joinedRow.withLeft(leftNullRow).copy :: Nil
        } else {
          temp
        }
      } else {
        joinedRow.withLeft(leftNullRow).copy :: Nil
      }
    }
    ret.iterator
  }

  // 注意：这里传入的key先是左侧的key，之后才是右侧的key
  // leftIter和rightIter则是与该key对应的左右侧的Row。见doExecute
  private[this] def fullOuterIterator(
      key: Row, leftIter: Iterable[Row], rightIter: Iterable[Row],
      joinedRow: JoinedRow): Iterator[Row] = {

    if (!key.anyNull) {
      // 尝试让传入的左右侧条目在key上Join一下
      val rightMatchedSet = scala.collection.mutable.Set[Int]()
      leftIter.iterator.flatMap[Row] { l =>
        joinedRow.withLeft(l)
        var matched = false
        rightIter.zipWithIndex.collect {
          // 1. For those matched (satisfy the join condition) records with both sides filled,
          //    append them directly

		  // 尝试找到吻合Inner Join的左右条目
          case (r, idx) if boundCondition(joinedRow.withRight(r)) =>
            matched = true // matched置为true，意为当前左条目找到对应的右条目了
            // if the row satisfy the join condition, add its index into the matched set
			// 匹配到的右条目index放入set里，避免重复输出
            rightMatchedSet.add(idx)
            joinedRow.copy()

        } ++ DUMMY_LIST.filter(_ => !matched).map( _ => {
          // 2. For those unmatched records in left, append additional records with empty right.

          // DUMMY_LIST.filter(_ => !matched) is a tricky way to add additional row,
          // as we don't know whether we need to append it until finish iterating all
          // of the records in right side.
          // If we didn't get any proper row, then append a single row with empty right.
		  // 当前左条目没有找到对应的右条目，放入一个NULL
          joinedRow.withRight(rightNullRow).copy()
        })
      } ++ rightIter.zipWithIndex.collect {
        // 3. For those unmatched records in right, append additional records with empty left.

        // Re-visiting the records in right, and append additional row with empty left, if its not
        // in the matched set.
		// 对于剩下的（不在之前那个set内）的右条目，也放入一个NULL
        case (r, idx) if !rightMatchedSet.contains(idx) =>
          joinedRow(leftNullRow, r).copy()
      }
    } else {
	  // key本身就是个NULL，那传入的左右侧条目肯定都不能Join起来了，直接输出
      leftIter.iterator.map[Row] { l =>
        joinedRow(l, rightNullRow).copy()
      } ++ rightIter.iterator.map[Row] { r =>
        joinedRow(leftNullRow, r).copy()
      }
    }
  }

  // 根据给定的数据以及key generator，生成&lt;key, Row(s)>映射。同之前的hashedRelation
  private[this] def buildHashTable(
      iter: Iterator[Row], keyGenerator: Projection): JavaHashMap[Row, CompactBuffer[Row]] = {
    val hashTable = new JavaHashMap[Row, CompactBuffer[Row]]()
    while (iter.hasNext) {
      val currentRow = iter.next()
      val rowKey = keyGenerator(currentRow)

      var existingMatchList = hashTable.get(rowKey)
      if (existingMatchList == null) {
        existingMatchList = new CompactBuffer[Row]()
        hashTable.put(rowKey, existingMatchList)
      }

      existingMatchList += currentRow.copy()
    }

    hashTable
  }

  protected override def doExecute(): RDD[Row] = {
    val joinedRow = new JoinedRow()
    left.execute().zipPartitions(right.execute()) { (leftIter, rightIter) =>
	  // 根据Join类型不同分成不同的处理方式
      joinType match {
        case LeftOuter => // 左外连接和右外连接的代码可以一起看，感觉就是个逻辑上的轴对称而已233
          val rightHashTable = buildHashTable(rightIter, newProjection(rightKeys, right.output))
		  // 生成右侧的key row映射
          val keyGenerator = newProjection(leftKeys, left.output)
          leftIter.flatMap( currentRow => {
            val rowKey = keyGenerator(currentRow)
            joinedRow.withLeft(currentRow) // 它的right由leftOuterIterator设定
            leftOuterIterator(rowKey, joinedRow, rightHashTable.getOrElse(rowKey, EMPTY_LIST))
          })

        case RightOuter =>
          val leftHashTable = buildHashTable(leftIter, newProjection(leftKeys, left.output))
          val keyGenerator = newProjection(rightKeys, right.output)
          rightIter.flatMap ( currentRow => {
            val rowKey = keyGenerator(currentRow)
            joinedRow.withRight(currentRow)
            rightOuterIterator(rowKey, leftHashTable.getOrElse(rowKey, EMPTY_LIST), joinedRow)
          })

        case FullOuter =>
          val leftHashTable = buildHashTable(leftIter, newProjection(leftKeys, left.output))
          val rightHashTable = buildHashTable(rightIter, newProjection(rightKeys, right.output))
          (leftHashTable.keySet ++ rightHashTable.keySet).iterator.flatMap { key =>
            fullOuterIterator(key,
              leftHashTable.getOrElse(key, EMPTY_LIST),
              rightHashTable.getOrElse(key, EMPTY_LIST), joinedRow)
          }

        case x => throw new Exception(s"HashOuterJoin should not take $x as the JoinType")
      }
    }
  }
}
</pre>
<p>
	看起来有点费劲，写得很是不面向对象，但总体来说并没有什么特别深奥的地方，慢慢看还是可以看得懂的。
</p>
<h3>CartesianProduct</h3>
<p>准入条件：Inner Join，有ON语句；没有ON语句</p>
<pre class="brush: scala">
case class CartesianProduct(left: SparkPlan, right: SparkPlan) extends BinaryNode {
  override def output: Seq[Attribute] = left.output ++ right.output

  protected override def doExecute(): RDD[Row] = {
    val leftResults = left.execute().map(_.copy())
    val rightResults = right.execute().map(_.copy())

    leftResults.cartesian(rightResults).mapPartitions { iter =>
      val joinedRow = new JoinedRow
      iter.map(r => joinedRow(r._1, r._2))
    }
  }
}
</pre>
<p>其实就是RDD的<code>cartesian</code>了。</p>
<h3>BroadcastNestedLoopJoin</h3>
<p>准入条件：剩下的所有Join。以较小一侧为buildSide。</p>
<pre class="brush: scala">
case class BroadcastNestedLoopJoin(
    left: SparkPlan,
    right: SparkPlan,
    buildSide: BuildSide,
    joinType: JoinType,
    condition: Option[Expression]) extends BinaryNode {
  // 最后一个，最General的Join Physical Plan出现了

  // ...

  protected override def doExecute(): RDD[Row] = {
    // 收集buildSide侧的计算结果并广播
    val broadcastedRelation =
      sparkContext.broadcast(broadcast.execute().map(_.copy()).collect().toIndexedSeq)

    /** All rows that either match both-way, or rows from streamed joined with nulls. */
    val matchesOrStreamedRowsWithNulls = streamed.execute().mapPartitions { streamedIter =>
      val matchedRows = new CompactBuffer[Row]
      // TODO: Use Spark's BitSet.
      val includedBroadcastTuples =
        new scala.collection.mutable.BitSet(broadcastedRelation.value.size)
      val joinedRow = new JoinedRow
      val leftNulls = new GenericMutableRow(left.output.size)
      val rightNulls = new GenericMutableRow(right.output.size)

      streamedIter.foreach { streamedRow =>
        var i = 0
        var streamRowMatched = false

		// 找出所有匹配的Inner Join条目
        while (i &lt; broadcastedRelation.value.size) {
          // TODO: One bitset per partition instead of per row.
          val broadcastedRow = broadcastedRelation.value(i)
          buildSide match {
            case BuildRight if boundCondition(joinedRow(streamedRow, broadcastedRow)) =>
              matchedRows += joinedRow(streamedRow, broadcastedRow).copy()
              streamRowMatched = true
              includedBroadcastTuples += i // 记录broadcast侧已被匹配的条目
            case BuildLeft if boundCondition(joinedRow(broadcastedRow, streamedRow)) =>
              matchedRows += joinedRow(broadcastedRow, streamedRow).copy()
              streamRowMatched = true
              includedBroadcastTuples += i
            case _ =>
          }
          i += 1
        }

		// 根据Join类型不同决定是否要把无法匹配的条目放进结果集中
        (streamRowMatched, joinType, buildSide) match {
          case (false, LeftOuter | FullOuter, BuildRight) =>
            matchedRows += joinedRow(streamedRow, rightNulls).copy()
          case (false, RightOuter | FullOuter, BuildLeft) =>
            matchedRows += joinedRow(leftNulls, streamedRow).copy()
		  // 这里还有LeftOuter + BuildLeft和RightOuter + BuildRight的情况没有处理 
          case _ =>
        }
      }
      Iterator((matchedRows, includedBroadcastTuples))
    }

    val includedBroadcastTuples = matchesOrStreamedRowsWithNulls.map(_._2)
    val allIncludedBroadcastTuples = // 所有已被匹配的broadcast侧条目
      if (includedBroadcastTuples.count == 0) {
        new scala.collection.mutable.BitSet(broadcastedRelation.value.size)
      } else {
        includedBroadcastTuples.reduce(_ ++ _)
      }

    val leftNulls = new GenericMutableRow(left.output.size)
    val rightNulls = new GenericMutableRow(right.output.size)
    /** Rows from broadcasted joined with nulls. */
    val broadcastRowsWithNulls: Seq[Row] = { // 由broadcast侧未匹配条目与NULL组成的Row
      val buf: CompactBuffer[Row] = new CompactBuffer()
      var i = 0
      val rel = broadcastedRelation.value
      while (i &lt; rel.length) { // 遍历整个broadcast侧
        if (!allIncludedBroadcastTuples.contains(i)) {
          (joinType, buildSide) match { // 未匹配的条目根据不同的Join类型生成带NULL的Row
            case (RightOuter | FullOuter, BuildRight) => buf += new JoinedRow(leftNulls, rel(i))
            case (LeftOuter | FullOuter, BuildLeft) => buf += new JoinedRow(rel(i), rightNulls)
            case _ =>
          }
        }
        i += 1
      }
      buf.toSeq
    }

    // TODO: Breaks lineage.
    sparkContext.union(
      matchesOrStreamedRowsWithNulls.flatMap(_._1), sparkContext.makeRDD(broadcastRowsWithNulls))
	  // 将两个结果集union起来并返回
  }
}
</pre>
<p>也算是比较直观啦，并没有什么特别神奇的东西。</p>
<p>至此，我们就探索完SparkSQL为JOIN操作设计的9种Physical Plan了，相信在这个操作上对SparkSQL知根知底为以后的工作也能带来莫大的好处。</p>
