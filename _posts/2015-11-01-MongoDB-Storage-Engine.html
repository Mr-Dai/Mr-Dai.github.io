---
layout: posts
title: MongoDB存储引擎
author: Robert Peng
categories: mongodb
---
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushJScript.js"></script>

<h2 class="jump">序</h2>
<p>
	2015年3月份，MongoDB发布了3.0.1版，从原本的2.2、2.4、2.6升级到了最新的3.0。大量的新功能在3.0版本中引入，其中包括了MongoDB Java驱动的大幅更新。
	但对于MongoDB数据库本身来说，可更换的数据存储引擎算得上是3.0最重大的更新之一。
</p>
<p>
	在3.0之前，MongoDB是不能像MySQL那样随意选择存储引擎的。而到了3.0，MongoDB的所使用的存储引擎可由用户自行指定。
	目前，用户可选择的存储引擎包括<code>MMAPv1</code>和<code>WiredTiger</code>。
</p>
<h2 class="jump">什么是存储引擎？</h2>
<p>
	存储引擎是数据库与底层硬件沟通的桥梁，数据库通过调用存储引擎提供的接口来完成增删查改等操作。可以说，存储引擎也是一种硬件驱动。
</p>
<p>
	对于“硬件”，这里不仅指的是计算机不同型号的CPU、内存和磁盘的系统调用，同时还包括了MongoDB在硬件上存储数据的方式。
	比如，对于<code>MMAPv1</code>和<code>WiredTiger</code>，它们所使用的索引格式就有所不同。
</p>
<p>
	不难想到，数据结构和算法从来都不是面面俱到的，有些时候为了在某些方面表现出出色性能，其他方面必然就会有所欠缺。
	ACM中常说的“空间换时间”也大概是这么一个道理。因此，只有了解不同存储引擎的特性才能够更好地优化数据库系统的性能。
</p>
<h2 class="jump">MMAPv1</h2>
<p>
	<code>MMAPv1</code>实际上就是MongoDB在3.0以前原有的存储引擎，在3.0版本它也继续作为MongoDB的默认存储引擎（注：MongoDB 3.2版本将会把默认存储引擎改为<code>WiredTiger</code>）。
	之所以叫<code>MMAP</code>，实际上是因为这个存储引擎会把数据直接映射到虚拟内存上，即"memory mapping"。
	我们知道，MongoDB的客户端与服务器传输数据都是通过BSON格式完成的，而<code>MMAPv1</code>则会不做修改地将BSON数据直接保存在磁盘中。
	这一点通过观察<code>/data/db</code>文件夹下的文件即可获知。而<code>MMAPv1</code>通过将BSON数据直接映射到虚拟内存上，
	实际上也利用操作系统帮助自己完成了不少的工作。
</p>
<p>
	提起<code>MMAPv1</code>，我们这里要先讲讲它的记录分配机制。
</p>
<h2 class="jump">MMAPv1记录分配机制</h2>
<p>
	在MongoDB中，每条数据以<code>Document</code>的形式进行存储，并通过<code>Collection</code>来管理<code>Document</code>。
	通过观察<code>/data/db</code>文件夹即可得知，同一个<code>Collection</code>中的<code>Document</code>会根据插入（<kbd>insert</kbd>）的先后顺序，
	连续地写入到磁盘的同一个区域（region）上。考虑到MongoDB schemaless的特性，即使是同一个<code>Collection</code>，也会存在某些<code>Document</code>特别大，
	某些特别小的情况。除此之外，我们还要考虑到在数据库使用过程中，某些<code>Document</code>会因为<kbd>update</kbd>操作而变大。
</p>
<p>
	<code>MMAP</code>在第一次插入时会为每个<code>Document</code>开辟一小块专属的区域，你可以管它叫一个"record"（记录），或一个"slot"（record这个名字容易和别的东西混淆，所以后面我会管它叫slot），
	其他新插入的<code>Document</code>则必须从这一小块区域的结尾处开始写入。
</p>
<p>
	首先，一个slot为了能完整放入一个Document，首先它的大小必须大于等于这个Document的初始大小，但它的大小一旦确定，且尾部被写入了新的Document以后，它的大小就固定了。
	上面我们提到，MongoDB的Document有可能因为<kbd>update</kbd>操作而变大。如果在update以后，Document的大小超过了当前的slot怎么办？MongoDB采取的做法，
	是在Collection的尾部申请一个更大的slot，并把新的Document整个移动过去，同时还要update与该Document相关的索引，使其指向Document所在的新位置。不难想象，这样的操作是比单纯的写入费时得多的，
	而且Document原本的空间被释放以后，很可能就会形成一个空间碎片。
</p>
<p>
	为了减少这样的操作的发生，MongoDB采取的做法是在创建slot时，不仅使其能够放入一个Document，同时也会预留一定的空间，称之为padding（内边距）。这样一来，Document在自己的专属空间中有了一定的发展空间，
	合适地选择padding的大小便能有效地降低这种操作发生的几率。在3.0版之前，MongoDB尝试根据Document大小增加的方式来预估合适的Padding大小，而3.0则改而使用新的两种选择padding的策略，
	分别是二次幂分配（Power of 2 Sized Allocation）和无Padding分配（No Padding Allocation）。
</p>
<h2 class="jump">二次幂分配策略</h2>
<p>
	二次幂的空间分配策略实际上不止在MongoDB，在其他系统或语言中都十分常见。大家接触得最多的例子，包括了C++中的<code>vector</code>和Java中的<code>ArrayList</code>。
	MongoDB的二次幂空间分配策略，使得为每个Document分配的slot的大小从32B开始，不断地乘以2，直到能够放入该Document：即从32B开始，依次增加至64B、128B、256B... 1MB、2MB，直到能够放入该Document。
	当slot所需空间超过2MB以后，slot空间的增加策略不再是依次乘以二，而只是单纯的加2MB：即从2MB开始，后面依次是4MB、6MB... 直到能够放入该Document或达到16MB的上限。
	相对固定的slot大小保证了每个slot为Document预留了一定的增长空间，同时使得slot扩容时，Document未必需要移动位置。
</p>
<p>
	首先，slot扩容是因为当前slot无法再放入该Document，但slot的扩容未必就意味着Document需要移动：也许该slot的尾部正好跟着足够多的空闲空间，那么只要直接加大当前slot即可，Document的位置无需移动，
	同理其相关的索引也无需修改，这样便能一定程度上较小slot扩容的花销，同时有效地利用空间碎片。二次幂分配机制使用相对固定的slot大小，使得这种情况发生的几率大大增加，因为如果假设每个slot的初始大小都是n字节，
	如果有一个slot扩容时移动到了Collection的尾部，那么它之前的前一个slot便直接获得了一个n字节的空余空间，当那个slot也需要扩容时就可以正好利用上这个空间了。
</p>
<h2 class="jump">无Padding分配策略</h2>
<p>
	无Padding，顾名思义，MongoDB在采用该分配策略时，所有的slot都不会为Document预留任何的增长空间，slot的大小与Document所需的大小完全一致。由于完全没有增长空间，每次Document的大小增加时，
	几乎是必然会导致Document移位，因此这样的分配策略并不适用于<kbd>update</kbd>操作频繁发生的Collection。但很多情况下，Document的大小可能确实不会变化。这意味着该Collection可能发生的操作
	只包含查询、插入、删除和不会导致大小增加的更新，如使用<code>$inc</code>令某个计数器自增。在这种情况下，Document的移位可以说是不可能会发生，无Padding的分配策略最大地利用了磁盘的空间，
	使得Document与Document之间更加紧密，同理也使得查询操作的性能得到了提升。
</p>
<p>
	MongoDB默认情况下使用的是二次幂分配策略。要为某个Collection使用无Padding的分配策略，我们需要使用如下指令：
</p>
<p><code>db.runCommand({collMod: &lt;collection-name>, noPadding: true})</code></p>
<p>或使用如下指令显式创建一个新的Collection：</p>
<p><code>db.createCollection(&lt;collection-name>, {noPadding: true})</code></p>
<h2 class="jump">MMAPv1锁机制</h2>
<p>
	像MongoDB这样可以同时接受多个客户端发来请求的数据库系统，并发自然是个需要处理的问题，而数据同步的方法，自然是对数据文件进行加锁了。MongoDB使用的是Multiple-Reader-Single-Writer锁：
	这意味着你可以有任意多的Reader同时读取数据，但这个时候其他Writer都会被阻塞；而当一个Writer获得锁时，其他所有的Reader和Writer都会被阻塞。加锁状态的转移明确了以后，还有一点需要明确的就是单次加锁的范围。
</p>
<p>
	了解过<code>/data/db</code>目录下的文件的人都知道，每个Collection会被独立的存放在各自的区域里。除了Collection以外，还会有一个独立的区域保存着数据库的元数据，如索引信息等。
	对Collection的读写，冲突的发生大多数时候都存在于单个Document中，毫无疑问我们这时候只需要对涉及的相关Document加锁即可。这种加锁方式叫做以Document为单位的加锁（Document-wise locking）。
	而事实是，<code>MMAPv1</code>不支持以Document为单位的加锁。
</p>
<p>
	但这只是一方面。有时候，我们会在Collection以外的地方发生冲突。比如，两个操作涉及完全不同的两个Document，但这两个操作却涉及到了同一个索引，于是在这个索引上便发生了冲突。除了索引外，
	如日志（Journal）等数据库元信息都是有可能发生冲突的。这个时候我们就需要比单个Document更大范围的锁了。
</p>
<p>
	在3.0版本之前，<code>MMAPv1</code>对锁请求的做法是，以Database为单位加锁（database-wise locking），对同一个Database的其他Collection所做的操作也会被阻塞。
	而到了3.0版本，<code>MMAPv1</code>则开始使用以Collection为单位的加锁（collection-wise locking）。如此一来，MongoDB 3.0便拥有了更好的并发性能。
</p>
<h2 class="jump">更新更快：WiredTiger</h2>
<p>
	综合上述对<code>MMAPv1</code>的介绍，我们不难发现<code>MMAPv1</code>存在着两个缺点：1. 即使是3.0版本更新后的<code>MMAPv1</code>最小也只能支持到以Collection为单位的加锁。
	由于缺乏以Document为单位的加锁机制，这注定<code>MMAPv1</code>的并发性能比较有限；2. <code>MMAPv1</code>对Document的slot的分配机制使得Document的移动时常发生。
	尽管升级后的分配策略一定程度上减少了这种操作的发生，但这种操作依然会发生，而且发生时依然会在磁盘上留下空间碎片。这使得<code>MMAPv1</code>的磁盘利用率有限。
</p>
<p>
	在2014年的12月，MongoDB正式收购了WiredTiger公司。之后，WiredTiger便为MongoDB 3.0开发了一个专用版本的存储引擎。WiredTiger引擎的架构和算法是和<code>MMAPv1</code>完全不同的，
	结果就是使用WiredTiger引擎的MongoDB比起MMAPv1有了极其显著的性能提升。
</p>
<p>我们接下来逐条看一下WiredTiger的优点。</p>
<h2 class="jump">WiredTiger的Document级锁机制</h2>
<p>
	我们已经见识到，<code>MMAPv1</code>对于所有操作都会使用至少为Collection级以上的共享互斥锁机制，这样的机制会使得整个数据库系统的并发性能下降。<code>WiredTiger</code>在这一点上则截然不同。
	在平常的使用中，大多数对数据库的更新操作都只会对某个Collection中的少量Document进行更新。对多个Collection进行同时更新的情况已是十分稀有，对多个Database进行同时更新则是更为罕见了。
	由此可见，加锁粒度最小只支持到Collection是远远不够的。相对于<code>MMAPv1</code>，<code>WiredTiger</code>使用的实际为Document级的乐观锁机制。
</p>
<p>
	<code>WiredTiger</code>的乐观锁机制与其他乐观锁机制实现大同小异。<code>WiredTiger</code>会在更新Document前记录住即将被更新的所有Document的当前版本号，并在进行更新前再次验证其当前版本号。
	若当前版本号没有发生改变，则说明该Document在该原子事件中没有被其他请求所更新，可以顺利进行写入，并修改版本号；但如果版本号发生改变，则说明该Document在更新发生之前已被其他请求所更新，
	由此便触发了一次“写冲突”。不过，在遇到写冲突以后，<code>WiredTiger</code>也会自动重试更新操作。
</p>
<p>
	但这并不代表<code>WiredTiger</code>对所有操作都会使用如此松散的乐观锁机制。对于某些的全局的操作，<code>WiredTiger</code>仍然会使用Collection级、Database级甚至是Instance级的互斥锁，
	但这样的全局操作实际上甚少发生，通常只会在DBA需要对数据库进行维护时才会被触发。在产品运行的过程中，支撑应用程序的绝大多数数据库访问和修改都不属于全局操作。
</p>
<h2 class="jump">WiredTiger的压缩机制</h2>
<p>
	相比于<code>MMAPv1</code>只是单纯地将BSON数据直接存储在磁盘上，<code>WiredTiger</code>则会在在数据从内存存储到磁盘前进行一次数据压缩。毫无疑问，这样的处理可以更好地利用磁盘的空间，
	但也为服务器带来了额外的CPU负荷。<code>WiredTiger</code>目前使用<code>snappy</code>压缩和前缀压缩两种压缩算法，其中<code>snappy</code>是默认的用于所有Collection的压缩算法，
	而前缀压缩则默认用于对索引的压缩。
</p>
<h2 class="jump">WiredTiger的未来</h2>
<p>
	8个月后的今天，MongoDB宣布即将发布3.2版本，而在3.2版本中，<code>WiredTiger</code>替代<code>MMAPv1</code>成为了MongoDB默认的存储引擎。至此，<code>MMAPv1</code>彻底走下神坛。
	尽管从3.0到3.2，<code>WiredTiger</code>的不稳定导致它暂且不能直接用于生产环境，但相信在MongoDB 3.2发布后，性能几倍于<code>MMAPv1</code>的<code>WiredTiger</code>将成为大家的首选。
</p>
