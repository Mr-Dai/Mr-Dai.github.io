---
layout: post_original
title: Spark Catalyst 源码解析：Intro
author: Robert Peng
category: Spark
---
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushScala.js"></script>

<h2 class="jump">前言</h2>
<p>
	我的<a href="{{ site.url }}{% post_url 2015-07-27-SparkSQL-HiveThriftServer-Source-1 %}">上一个系列</a>的SparkSQL源码解析已经完结了一段时间了。
	当时我出于实习工作的需要阅读了SparkSQL HiveThriftServer以及Spark Scala Interpreter的源代码，并顺势写下了那个系列的源码解析文章。
	但读SparkSQL源代码怎么能只读那些外围插件的源代码呢？于是我又开一个新坑了。
</p>
<p>
	在上一个系列中也提到过，SparkSQL实际上由4个项目组成，分别为Spark Core、Spark Catalyst、Spark Hive和Spark Hive ThriftServer。
	这个系列的文章所要介绍的是Spark Catalyst项目。它在SparkSQL中担任的角色是优化器。这个系列的文章我将会按照标准的SparkSQL执行流程来解析源代码，
	因此文章中将不可避免地出现Spark Core的部分代码。
</p>
<p>
	正如我所讲，本系列文章将按照SparkSQL的执行顺序来讲解代码，但很多人可能并不了解自己在调用了<code>SQLContext#sql</code>以后到底会发生什么。
	因此在阅读本文之前，我强烈建议各位先看一下<a href="/file/SparkSQL.pdf">这篇</a>论文。这篇论文是SparkSQL的官方论文，其中提到了SparkSQL Catalyst的执行流程。
	通过完整阅读这篇论文并掌握其中出现的一些专属名词，将对你接下来的代码阅读工作大有裨益。
</p>
<p>
	本文使用的是当下最新的<code>1.4.1</code>版本的Spark。在该版本中，SparkSQL的版本号为<code>2.10</code>。
</p>

<h2 class="jump">SQLContext</h2>
<p>
	毋庸置疑，一切的一切都从<code>SQLContext#sql</code>开始。不过，我们先来看看<code>SQLContext</code>这个类都包含了些什么变量。
</p>
<pre class="brush: scala">
class SQLContext(@transient val sparkContext: SparkContext)
  extends org.apache.spark.Logging with Serializable {
self =>

  // ...

  @transient
  protected[sql] lazy val catalog: Catalog = new SimpleCatalog(conf)

  @transient
  protected[sql] lazy val analyzer: Analyzer =
    new Analyzer(catalog, functionRegistry, conf) {
      override val extendedResolutionRules =
        ExtractPythonUdfs ::
        sources.PreInsertCastAndRename ::
        Nil

      override val extendedCheckRules = Seq(
        sources.PreWriteCheck(catalog)
      )
    }
	
  @transient
  protected[sql] lazy val optimizer: Optimizer = DefaultOptimizer

  @transient
  protected[sql] val ddlParser = new DDLParser(sqlParser.parse(_))

  @transient
  protected[sql] val sqlParser = new SparkSQLParser(getSQLDialect().parse(_))
  
  @transient
  protected[sql] val planner = new SparkPlanner

  // ...
}
</pre>
<p>
	上述代码当然不是<code>SQLContext</code>的全部变量，但我们暂时只需要看到这些。首先<code>catalog</code>变量只要是看过论文的读者自然是不会陌生了，
	它用来存放所有<code>SQLContext</code>已经知晓的表，在对Attribute、Relation等进行resolve的时候就需要利用Catalog提供的信息。
	剩余的五个变量中我们看到了4个角色，分别为Parser、Analyzer、Optimizer、Planner。同样，在论文中已经提及到了这些角色的作用，
	其中parser负责把用户输入的SQL语句进行解释，转变为Unresolved Logical Plan。Unresolved Logical Plan中会包含SQL语句中出现的变量名和表名，
	这些词素暂时来讲都会被标记为unresolved，即“不知道是否存在这个表”或“不知道表中是否有这个字段”。这个时候轮到Analyzer登场，它利用Catalog提供的信息，
	对所有这些unresolved的词素进行resolve，并在resolve失败时抛出错误。结束后便得到了Analyzed Logical Plan。接下来轮到Optimizer，
	它使用rule-based的优化规则对传入的Analyzed Logical Plan进行优化，得到一个Optimized Logical Plan。最终Optimized Logical Plan传入到Planner，
	生成物理执行计划，得到Physical Plan。
</p>
<p>
	这么多的废话，其实就变成这样一张图：
</p>
<p class="center"><img style="width: 100%" alt="" src="/img/Spark-Catalyst@1.png"></p>
<p>这就是SparkSQL的基本执行流程，一切由<code>SQLContext#sql</code>开始。那我们就先来看看起点吧：</p>
<pre class="brush: scala">
def sql(sqlText: String): DataFrame = {
  DataFrame(this, parseSql(sqlText))
}	// 调用parseSql方法将传入的sql语句转变为了unresolved logical plan，并用来实例化了一个DataFrame

// 调用了ddlParser的parse函数来解析传入的sql语句
protected[sql] def parseSql(sql: String): LogicalPlan = ddlParser.parse(sql, false)
</pre>
<p>好，到此为止，Parser的代码我们留到下次。我们先去看看<code>DataFrame</code>的那个构造函数都做了什么：</p>
<pre class="brush: scala">
class DataFrame private[sql](
    @transient val sqlContext: SQLContext,
    @DeveloperApi @transient val queryExecution: SQLContext#QueryExecution)
  extends RDDApi[Row] with Serializable {

  /**
   * A constructor that automatically analyzes the logical plan.
   *
   * This reports error eagerly as the [[DataFrame]] is constructed, unless
   * [[SQLConf.dataFrameEagerAnalysis]] is turned off.
   */
  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan) = {
    this(sqlContext, {
      val qe = sqlContext.executePlan(logicalPlan)
      if (sqlContext.conf.dataFrameEagerAnalysis) {
        qe.assertAnalyzed()  // This should force analysis and throw errors if there are any
      }
      qe
    })
  }
  
  // ...
  
}
</pre>
<p>
	如此看来，尽管传入的LogicalPlan仍然是个unresolved logical plan，但<code>DataFrame</code>的这个构造函数立马就触发了analyze操作，
	并返回了一个<code>SQLContext#QueryExecution</code>类。我们就来看看<code>SQLContext</code>的这个内部类吧：
</p>
<pre class="brush: scala">
/**
 * :: DeveloperApi ::
 * The primary workflow for executing relational queries using Spark.  Designed to allow easy
 * access to the intermediate phases of query execution for developers.
 */
 @DeveloperApi
 protected[sql] class QueryExecution(val logical: LogicalPlan) {
   def assertAnalyzed(): Unit = analyzer.checkAnalysis(analyzed)

   // Unresolved Logical Plan -> Analyzed Logical Plan -> Optimized Logical Plan
   // -> Physical Plan -> Executed Physical Plan -> RDD

   // 分析unresolved的LogicalPlan，得到Analyzed Logical Plan
   // Unresolved Logical Plan -> Analyzed Logical Plan
   lazy val analyzed: LogicalPlan = analyzer.execute(logical)

   // 将LogicalPlan中的结点尽可能地替换为cache中的结果，得到Analyzed Logical Plan with Cached Data
   lazy val withCachedData: LogicalPlan = {
     assertAnalyzed()
     cacheManager.useCachedData(analyzed)
   }

   // 对Analyzed Logical Plan with Cached Data进行优化，得到Optimized Logical Plan
   // Analyzed Logical Plan -> Optimized Logical Plan
   lazy val optimizedPlan: LogicalPlan = optimizer.execute(withCachedData)

   // 生成PhysicalPlan
   // Optimized Logical Plan -> Physical Plan
   lazy val sparkPlan: SparkPlan = {
     SparkPlan.currentContext.set(self)
     planner.plan(optimizedPlan).next()
   }
   // executedPlan should not be used to initialize any SparkPlan. It should be
   // only used for execution.
   // 准备好的PhysicalPlan
   lazy val executedPlan: SparkPlan = prepareForExecution.execute(sparkPlan)

   /** Internal version of the RDD. Avoids copies and has no schema */
   // 执行并返回结果
   lazy val toRdd: RDD[Row] = executedPlan.execute()

   protected def stringOrError[A](f: => A): String =
     try f.toString catch { case e: Throwable => e.toString }

   def simpleString: String =
     s"""== Physical Plan ==
        |${stringOrError(executedPlan)}
     """.stripMargin.trim

   override def toString: String = {
     def output =
       analyzed.output.map(o => s"${o.name}: ${o.dataType.simpleString}").mkString(", ")

     // TODO previously will output RDD details by run (${stringOrError(toRdd.toDebugString)})
     // however, the `toRdd` will cause the real execution, which is not what we want.
     // We need to think about how to avoid the side effect.
     s"""== Parsed Logical Plan ==
        |${stringOrError(logical)}
        |== Analyzed Logical Plan ==
        |${stringOrError(output)}
        |${stringOrError(analyzed)}
        |== Optimized Logical Plan ==
        |${stringOrError(optimizedPlan)}
        |== Physical Plan ==
        |${stringOrError(executedPlan)}
        |Code Generation: ${stringOrError(executedPlan.codegenEnabled)}
        |== RDD ==
     """.stripMargin.trim
  }
}  
</pre>
<p>
	从类的注释上就能看到，<code>SQLContext#QueryExecution</code>这个类包含了一次SQL查询的整个生命周期，从unresolved到analyzed到optimized到physical到RDD，
	全都包含在了一个类中，开发者也可以很方便地通过这一个类对整个计算过程进行监控。<code>DataFrame</code>的构造函数通过调用该类的<code>assertAnalyzed</code>
	方法，触发了<code>sqlContext.analyzer</code>对<code>logicalPlan</code>变量的<code>analyze</code>操作。
	光是调用<code>SQLContext#sql</code>方法，Logical Plan的Analysis步骤就已经完成了。
</p>
<h2 class="jump">总结</h2>
<p>
	知道<code>SQLContext#QueryExecution</code>这样一个类的存在对我们以后的代码阅读工作将带来大量的好处。下一次我将从SparkSQL的第一个步骤：Parse进行讲解，敬请期待。
</p>
<hr />
<a href="{{ site.url }}{% post_url 2015-08-17-SparkSQL-Catalyst-Source-2 %}">Spark Catalyst 源码解析：Parser</a>