---
layout: posts
title: Spark Catalyst 源码解析：Parser
author: Robert Peng
category: Spark
---
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushScala.js"></script>

<h2 class="jump">前情提要</h2>
<p>
	在<a href="/2015/08/17/SparkSQL-Catalyst-Source-1.html">上一篇文章</a>中，我们了解了SparkSQL查询的基本执行过程，
	并了解到<code>SQLContext</code>的内部类<code>QueryExecution</code>包含了整个执行过程的每一个执行步骤。
</p>
<p>在这篇文章中，我将开始讲解SQL语句如何通过Parser转变为Unresolved Logical Plan。</p>

<h2 class="jump">DDLParser</h2>
<p>我们回到<code>SQLContext#parseSql</code>方法：</p>
<pre class="brush: scala">
@transient
protected[sql] val ddlParser = new DDLParser(sqlParser.parse(_))

@transient
protected[sql] val sqlParser = new SparkSQLParser(getSQLDialect().parse(_))

protected[sql] def parseSql(sql: String): LogicalPlan = ddlParser.parse(sql, false)
</pre>
<p>
	可以看到，<code>parseSql</code>方法调用了<code>ddlParser</code>的<code>parse</code>方法。<code>ddlParser</code>在初始化时传入了<code>sqlParser.parse</code>方法作为参数，
	而<code>sqlParser</code>在初始化时也传入了一个SQL方言的<code>parse</code>方法作为参数。这三个<code>parse</code>之间很有可能是一个`fallback`的关系。那我们先来看看<code>DDLParser</code>：
</p>
<pre class="brush: scala">
/**
 * A parser for foreign DDL commands.
 */
private[sql] class DDLParser(parseQuery: String => LogicalPlan)
  extends AbstractSparkSQLParser with DataTypeParser with Logging {

  def parse(input: String, exceptionOnError: Boolean): LogicalPlan = {
    try {
      // 先尝试用AbstractSparkSQLParser#parse进行解析
      parse(input)
    } catch {
      case ddlException: DDLException => throw ddlException
      
	  // 解析失败则使用传入的解析函数parseQuery进行解析
      case _ if !exceptionOnError => parseQuery(input)
      case x: Throwable => throw x
    }
  }
  
  // ...
}
</pre>
<p>先不急着往下看，因为这里调用了<code>AbstractSparkSQLParser</code>的<code>parse</code>方法。我们先看看<code>AbstractSparkSQLParser</code>：</p>
<pre class="brush: scala">
private[sql] abstract class AbstractSparkSQLParser
  extends StandardTokenParsers with PackratParsers {

  def parse(input: String): LogicalPlan = {
    // 将Keyword们作为保留字放入到lexical变量中
    initLexical
	// 开始解释传入的字符串
    phrase(start)(new lexical.Scanner(input)) match {
      case Success(plan, _) => plan
      case failureOrError => sys.error(failureOrError.toString)
    }
  }
  /* One time initialization of lexical.This avoid reinitialization of  lexical in parse method */
  protected lazy val initLexical: Unit = lexical.initialize(reservedWords)

  protected case class Keyword(str: String) {
    def normalize: String = lexical.normalizeKeyword(str)
    def parser: Parser[String] = normalize
  }

  protected implicit def asParser(k: Keyword): Parser[String] = k.parser

  // 通过反射机制将类的所有返回Keyword类型的函数结果注册为保留字（reserved word）
  protected lazy val reservedWords: Seq[String] =
    this.getClass
      .getMethods
      .filter(_.getReturnType == classOf[Keyword])
      .map(_.invoke(this).asInstanceOf[Keyword].normalize)

  // Set the keywords as empty by default, will change that later.
  // SQL词素
  override val lexical = new SqlLexical

  protected def start: Parser[LogicalPlan]

  // Returns the whole input string
  protected lazy val wholeInput: Parser[String] = new Parser[String] {
    def apply(in: Input): ParseResult[String] =
      Success(in.source.toString, in.drop(in.source.length()))
  }

  // Returns the rest of the input string that are not parsed yet
  protected lazy val restInput: Parser[String] = new Parser[String] {
    def apply(in: Input): ParseResult[String] =
      Success(
        in.source.subSequence(in.offset, in.source.length()).toString,
        in.drop(in.source.length()))
  }
}
</pre>
<p>我们看到，真正启动<code>parse</code>过程的实际上是如下代码块：</p>
<pre class="brush: scala">
phrase(start)(new lexical.Scanner(input)) match {
  case Success(plan, _) => plan
  case failureOrError => sys.error(failureOrError.toString)
}
</pre>
<p>
	这里调用的<code>phrase</code>方法实际上来自于<code>AbstractSparkSQLParser</code>的父类<code>PackratParsers</code>。
	<code>PackratParsers</code>和<code>StandardTokenParsers</code>实际上都是Scala自带的类。它们的功能较为复杂，
	而且SparkSQL本身的作用原理关系并不是很大，我在这里就简单讲述一下。
</p>
<pre class="brush: scala">
// PackratParsers.scala

/**
 *  A parser generator delimiting whole phrases (i.e. programs).
 *
 *  Overridden to make sure any input passed to the argument parser
 *  is wrapped in a `PackratReader`.
 */
override def phrase[T](p: Parser[T]) = {
  val q = super.phrase(p)
  new PackratParser[T] {
    def apply(in: Input) = in match {
      case in: PackratReader[_] => q(in)
      case in => q(new PackratReader(in))
    }
  }
}
</pre>
<p>
	可以看到，<code>PackratParsers#phrase</code>方法接受一个<code>Parser</code>作为参数，并以其为参数调用了其父类<code>Parsers</code>
	的<code>phrase</code>方法，该方法同样返回一个<code>Parser</code>。而后，<code>PackratParsers#phrase</code>返回了一个<code>PackratParser</code>，
	由<code>AbstractSparkSQLParser</code>调用这个对象的<code>apply</code>方法传入SQL语句。
</p>
<p>我们回到<code>DDLParser</code>：</p>
<pre class="brush: scala">
private[sql] class DDLParser(parseQuery: String => LogicalPlan)
  extends AbstractSparkSQLParser with DataTypeParser with Logging {

  def parse(input: String, exceptionOnError: Boolean): LogicalPlan = {
    // ...
  }

  // 这些keyword会在initLexical时被加载
  protected val CREATE = Keyword("CREATE")
  protected val TEMPORARY = Keyword("TEMPORARY")
  protected val TABLE = Keyword("TABLE")
  protected val IF = Keyword("IF")
  protected val NOT = Keyword("NOT")
  protected val EXISTS = Keyword("EXISTS")
  protected val USING = Keyword("USING")
  protected val OPTIONS = Keyword("OPTIONS")
  protected val DESCRIBE = Keyword("DESCRIBE")
  protected val EXTENDED = Keyword("EXTENDED")
  protected val AS = Keyword("AS")
  protected val COMMENT = Keyword("COMMENT")
  protected val REFRESH = Keyword("REFRESH")

  protected lazy val ddl: Parser[LogicalPlan] = createTable | describeTable | refreshTable

  protected def start: Parser[LogicalPlan] = ddl
  
  // ...
  
}
</pre>
<p>
	在接下来的代码中，<code>AbstractSparkSQLParser</code>实现了三个parser：<code>createTable</code>、<code>describeTable</code>和<code>refreshTable</code>，
	并将其重载为<code>AbstractSparkSQLParser#start</code>变量，由此<code>DDLParser</code>改变了<code>AbstractSparkSQLParser#start</code>的功能。
</p>
<p>
	上述的这些Keyword全都是Spark所支持的DLL keyword，没有包含SQL的保留字。不难想象<code>DDLParser</code>仅用于解析DDL语句，
	当遇到SQL语句时，解析器将fallback到实例化<code>DDLTask</code>时传入的<code>parseQuery</code>函数，而这个函数正是<code>SparkSQLParser#parse</code>函数。
</p>
<p>通过查看<code>SparkSQLParser</code>的源代码，可以有如下发现：</p>
<pre class="brush: scala">
/**
 * The top level Spark SQL parser. This parser recognizes syntaxes that are available for all SQL
 * dialects supported by Spark SQL, and delegates all the other syntaxes to the `fallback` parser.
 *
 * @param fallback A function that parses an input string to a logical plan
 */
private[sql] class SparkSQLParser(fallback: String => LogicalPlan) extends AbstractSparkSQLParser {

  // ...
	
  protected val AS = Keyword("AS")
  protected val CACHE = Keyword("CACHE")
  protected val CLEAR = Keyword("CLEAR")
  protected val IN = Keyword("IN")
  protected val LAZY = Keyword("LAZY")
  protected val SET = Keyword("SET")
  protected val SHOW = Keyword("SHOW")
  protected val TABLE = Keyword("TABLE")
  protected val TABLES = Keyword("TABLES")
  protected val UNCACHE = Keyword("UNCACHE")
  
  // ...
  
}
</pre>
<p>
	从注释上看，<code>SparkSQLParser</code>用于解析所有SparkSQL所支持的SQL方言所共有的关键字。当该解析器失败时，
	将会继续fallback到当时在<code>SQLContext</code>传入的<code>getSQLDialect().parse(_)</code>，使用某个特定的SQL方言进行解析。
</p>

<h2 class="jump">总结</h2>
<p>
	个人认为大多数人应该不会太在意Parser的原理，毕竟没什么人会需要去修改SparkSQL语句的解析逻辑，因此这一篇文章只能算是抛砖引玉，
	真正的解析逻辑还有待你们自己去发掘。在下一篇文章中我会先为大家讲解一下LogicalPlan的数据结构，敬请期待。
</p>
<hr />
<a href="/2015/08/18/SparkSQL-Catalyst-Source-3.html">Spark Catalyst 源码解析：LogicalPlan</a><br />
<a href="/2015/08/19/SparkSQL-Catalyst-Adv-1.html">Spark Catalyst 进阶：Parser词素</a>
