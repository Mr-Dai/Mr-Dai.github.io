---
layout: post_original
title: Spark Core 源码解析：RDD
author: Robert Peng
category: Spark
---
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushBash.js"></script>
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushJava.js"></script>
<script type="text/javascript" src="/js/syntaxhighlighters/shBrushScala.js"></script>

<h2 class="jump">前言</h2>
<p>
	我的<a href="{{ site.url }}{% post_url 2015-08-17-SparkSQL-Catalyst-Source-1 %}">上一个系列</a>的Spark源码解析已经完结了一段时间了。
	当时我出于实习工作的需要阅读了SparkSQL以及Spark REPL的源代码，并顺势写下了那个系列的源码解析文章。
	但读Spark源代码怎么能只读那些外围插件的源代码呢？于是我又开一个新坑了。
</p>
<p>
	要理解Spark的中心思想，首先当然得从Spark Core开始。Spark Core中包含了所有Spark的核心类的定义，其中就有我们用得最多的<code>SparkContext</code>和
	<code>RDD</code>。在开始阅读本文之前，我希望各位可以先完整阅读<a href="/file/RDDs.pdf">这篇</a>论文以及<a href="/file/Spark.pdf">这篇</a>论文。这两篇论文的撰写者相同，均属UC Berkeley大学，
	虽然我不确定他们是不是，但我想他们应该就是Spark的创始人了。前一篇论文在第二节详细介绍了RDD的概念，并在第五节详细介绍了Spark的一些实现原理。
	后一篇论文在内容上并不如前一篇论文充分，而且有大量的重复内容，但其中也包含了一些新内容，值得大家学习一下。
	源代码中会出现很多奇怪的名词，恐怕你必须通过完整阅读这两篇论文才能够理解。我不会在文中重复解释这些术语的确切意思，
	因此我希望你能静下心来读完这两篇论文再继续往下看。我相信这样的阅读是完全值得的。
</p>
<p>
	首先这个系列的出发点其实就有两个，一个是<code>SparkContext</code>，另一个就是<code>RDD</code>。我有思考过哪个更好，最终我选择了<code>RDD</code>，
	因为它的实现更简单，与Spark其他类的依赖也少得多。在我们完整阅读了<code>RDD</code>的源代码后，想必阅读<code>SparkContext</code>的源代码也会变得轻松很多。
	但这并不代表在这篇文章中不会出现<code>SparkContext</code>的代码。这篇文章将涵盖与<code>RDD</code>功能实现有关的代码，至于这些代码来自于哪个类并不重要。
</p>
<p>
	那我们开始吧。
</p>
<hr />
<h2 class="jump">RDD</h2>
<p>在开始跳进去看RDD的方法之前，我们应该先了解一下RDD的一些基本信息。</p>
<p>首先，我们先来看看RDD的构造方法：</p>
<pre class="brush: scala">
abstract class RDD[T: ClassTag](
    @transient private var _sc: SparkContext,
    @transient private var deps: Seq[Dependency[_]]
  ) extends Serializable with Logging {

  if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) {
    // This is a warning instead of an exception in order to avoid breaking user programs that
    // might have defined nested RDDs without running jobs with them.
    logWarning("Spark does not support nested RDDs (see SPARK-5063)")
  }
  
  /** Construct an RDD with just a one-to-one dependency on one parent */
  def this(@transient oneParent: RDD[_]) =
    this(oneParent.context , List(new OneToOneDependency(oneParent)))
}
</pre>
<p>
	这里我们看到，<code>RDD</code>在创建时便会放入一个<code>SparkContext</code>和它的<code>Dependency</code>们。
	关于<code>Dependency</code>类，在上面的论文中有介绍，它包含了当前<code>RDD</code>的父<code>RDD</code>的引用，
	以及足够从父<code>RDD</code>恢复丢失的partition的信息。
</p>
<p>接下来我们看看<code>RDD</code>需要子类实现的虚函数：</p>
<pre class="brush: scala">
// 由子类实现来计算一个给定的Partition
def compute(split: Partition, context: TaskContext): Iterator[T]

// 由子类实现，返回这个RDD的Partition集合
protected def getPartitions: Array[Partition]

// 由子类实现，返回这个RDD的Dependency集合
protected def getDependencies: Seq[Dependency[_]] = deps

// 可由子类重载，以提供更加偏好的Partition放置策略
protected def getPreferredLocations(split: Partition): Seq[String] = Nil

// 可由子类重载来改变partition的方式
@transient val partitioner: Option[Partitioner] = None
</pre>
<p>
	这些函数基本都是用于执行Spark计算的方法，也包括了论文中提到的三大RDD接口中的两个，
	即<code>getPartitions</code>以及<code>getPreferredLocations</code>。其中有两个函数是子类必须实现的，即
	<code>compute</code>和<code>getPartitions</code>。我们记住它们的功能定义，以免它们在子类中再次出现时一时想不起来它们的功能。
</p>
<p>
	继续往下，我们看到除了包含<code>SparkContext</code>变量和<code>Dependency</code>们，一个RDD还包含了自己的<code>id</code>
	以及<code>name</code>：
</p>
<pre class="brush: scala">
// 创建该RDD的SparkContext
def sparkContext: SparkContext = sc

// SparkContext内部的唯一ID
val id: Int = sc.newRddId()

// RDD的名字
@transient var name: String = null

// 给RDD一个新的名字
def setName(_name: String): this.type = {
  name = _name
  this
}
</pre>
<p>再继续往下，便是RDD的公用API了。</p>
<hr />
<h2 class="jump">RDD Action</h2>
<p>
	RDD提供了大量的API供我们使用。通过浏览RDD的<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD">ScalaDoc</a>，不难发现RDD拥有数十种<code>public</code>的接口，
	更不要提那些我们即将面对的非<code>public</code>的接口了。因此直接跳进<code>RDD.scala</code>从上往下阅读源代码是不科学的。
	这里我使用另外一种阅读方式。
</p>
<p>
	正如Spark的论文中所描述的，RDD的API并不是每一个都会启动Spark的计算。被称之为<code>Transformation</code>的操作可以用一个RDD产生另一个RDD，
	但这样的操作实际上是lazy的：它们并不会被立即计算，而是当你真正触发了计算动作的时候，所有你提交过的Transformation们会在经过Spark优化以后再顺序执行。
	那么怎么样的操作会触发Spark的计算呢？
</p>
<p class="center"><img style="max-width: 100%" alt="" src="/img/SparkCore@1.jpg"></p>
<p>
	这些被称之为<code>Action</code>的RDD操作便会触发Spark的计算动作。根据上图，Action包括<code>count</code>、<code>collect</code>、<code>reduce</code>、
	<code>lookup</code>和<code>save</code>（已被更名为<code>saveAsTextFile</code>和<code>saveAsObjectFile</code>）。不难发现，除了<code>save</code>，
	其他四个操作都是将结果直接获取到driver程序中的操作，由这些操作来启动Spark的计算也是十分合理的。
</p>
<p>
	那么我们不妨先来看一下这几个函数的源代码：
</p>
<pre class="brush: scala" id="rdd-actions">

// RDD.scala

def collect(): Array[T] = withScope {
  val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
  Array.concat(results: _*)
}

// 返回RDD的元素个数
def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum

// 使用给定的二元运算符来reduce该RDD
def reduce(f: (T, T) => T): T = withScope {
  // Clean一下用户传入的closure，以准备将其序列化
  val cleanF = sc.clean(f)
  // 应用在每个partition上的reduce函数。相当于Hadoop MR中的combine
  val reducePartition: Iterator[T] => Option[T] = iter => {
    if (iter.hasNext) {
      Some(iter.reduceLeft(cleanF)) // 在单个Partition内部使用Iterator#reduceLeft来计算结果
    } else {
      None
    }
  }
  
  var jobResult: Option[T] = None
  // 合并每个partition的reduce结果
  val mergeResult = (index: Int, taskResult: Option[T]) => {
    if (taskResult.isDefined) {
      jobResult = jobResult match {
        case Some(value) => Some(f(value, taskResult.get))
        case None => taskResult
      }
    }
  }
  // 启动Spark Job
  sc.runJob(this, reducePartition, mergeResult)

  jobResult.getOrElse(throw new UnsupportedOperationException("empty collection"))
}

// PairRDDFunctions.scala

// 根据给定的RDD的key来查找它对应的Seq[value]
// 如果该RDD有给定的Partitioner，该方法会先利用getPartition方法定位Partition再进行搜索，
// 如此一来便能提高效率 
def lookup(key: K): Seq[V] = self.withScope {
  self.partitioner match {
    case Some(p) => // 存在特定的Partitioner
      val index = p.getPartition(key)  // 定位具体的Partition
      val process = (it: Iterator[(K, V)]) => {
        val buf = new ArrayBuffer[V]
        for (pair &lt;- it if pair._1 == key) {
          buf += pair._2
        }
        buf
      } : Seq[V]
	  // 仅在该Partition上查找
      val res = self.context.runJob(self, process, Array(index), false)
      res(0)  
    case None =>
	  // 若找不到特定的Partitioner，则使用RDD#filter来查找
      self.filter(_._1 == key).map(_._2).collect()
  }
}  
</pre>

<p>
	上述四个函数都有一个特点：它们都直接或间接地调用了<code>sparkContext.runJob</code>方法来获取结果。
	可见这个方法便是启动Spark计算任务的入口。我们记下这个入口，留到研读<code>SparkContext</code>源代码的时候再进行解析。
</p>
<hr />
<h2 class="jump">RDD Transformations</h2>
<p>
	讲完了Action，自然就轮到了Transformation了。可是有那~么多的Transformation啊。我们就一个一个地看看这些常用的Transformation吧。
</p>
<h3>map</h3>
<p>我们先从用得最多的开始。我们直接看源码：</p>
<pre class="brush: scala">
/**
 * Return a new RDD by applying a function to all elements of this RDD.
 */
def map[U: ClassTag](f: T => U): RDD[U] = withScope {
  val cleanF = sc.clean(f)
  new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.map(cleanF))
}
</pre>
<p>
	和论文中说的一样，<code>map</code>函数会利用当前RDD以及用户传入的匿名函数构建出一个<code>MapPartitionsRDD</code>。
	毋庸置疑这个东西肯定是继承自<code>RDD</code>类的。我们可以看看它的源代码：
</p>
<pre class="brush: scala">
private[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag](
    prev: RDD[T],
    f: (TaskContext, Int, Iterator[T]) => Iterator[U],  // (TaskContext, partition index, iterator)
    preservesPartitioning: Boolean = false)
  extends RDD[U](prev) {

  override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None

  override def getPartitions: Array[Partition] = firstParent[T].partitions

  override def compute(split: Partition, context: TaskContext): Iterator[U] =
    f(context, split.index, firstParent[T].iterator(split, context))
}
</pre>
<p>
	可以看到，<code>MapPartitionsRDD</code>实现了<code>getPartitions</code>和<code>compute</code>方法。
</p>
<p>
	<code>getPartitions</code>方法直接返回了它的<code>firstParent</code>的partition。实际上<code>MapPartitionsRDD</code>也只会有一个parent，
	也就是构造函数传入的<code>prev</code>。
</p>
<p>
	<code>compute</code>方法在这里直接应用了构造参数传入的方法<code>f</code>。我们看回<code>RDD#map</code>，
	传入的方法是<code>(context, pid, iter) => iter.map(cleanF)</code>。结合到<code>MapPartitionsRDD</code>的源代码里就不难看出其实现原理了。
	这里我们最好记住匿名函数的<code>context</code>是<code>TaskContext</code>、<code>pid</code>是<code>Partition</code>的id、
	<code>iter</code>即该<code>Partition</code>的<code>iterator</code>。记住这些以免后面再次出现的时候一时晕菜。
</p>
<p>
	注意到，<code>MapPartitionsRDD</code>还重载了<code>partitioner</code>变量，
	其值取决于构造函数传入的<code>preservesPartitioning</code>参数，该参数默认为<code>false</code>。
	在<code>RDD#map</code>方法里并未对该参数赋值。
</p>
<h3>withScope</h3>
<p>
	我们回到刚才的<code>RDD#map</code>方法，注意到它还调用了一个函数，就是<code>withScope</code>。
	这个函数出现的次数相当多，你在很多RDD API里都能发现它。我们来看看它的源代码：
</p>
<pre class="brush: scala">

// RDD.scala

private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body)

// RDDOperationScope.scala

/**
 * A general, named code block representing an operation that instantiates RDDs.
 *
 * All RDDs instantiated in the corresponding code block will store a pointer to this object.
 * Examples include, but will not be limited to, existing RDD operations, such as textFile,
 * reduceByKey, and treeAggregate.
 *
 * An operation scope may be nested in other scopes. For instance, a SQL query may enclose
 * scopes associated with the public RDD APIs it uses under the hood.
 *
 * There is no particular relationship between an operation scope and a stage or a job.
 * A scope may live inside one stage (e.g. map) or span across multiple jobs (e.g. take).
 */
@JsonInclude(Include.NON_NULL)
@JsonPropertyOrder(Array("id", "name", "parent"))
private[spark] class RDDOperationScope(
    val name: String,
    val parent: Option[RDDOperationScope] = None,
    val id: String = RDDOperationScope.nextScopeId().toString) {

  def toJson: String = {
    RDDOperationScope.jsonMapper.writeValueAsString(this)
  }

  /**
   * Return a list of scopes that this scope is a part of, including this scope itself.
   * The result is ordered from the outermost scope (eldest ancestor) to this scope.
   */
  @JsonIgnore
  def getAllScopes: Seq[RDDOperationScope] = {
    parent.map(_.getAllScopes).getOrElse(Seq.empty) ++ Seq(this)
  }

  override def equals(other: Any): Boolean = {
    other match {
      case s: RDDOperationScope =>
        id == s.id && name == s.name && parent == s.parent
      case _ => false
    }
  }

  override def toString: String = toJson
}

/**
 * A collection of utility methods to construct a hierarchical representation of RDD scopes.
 * An RDD scope tracks the series of operations that created a given RDD.
 */
private[spark] object RDDOperationScope extends Logging {
  private val jsonMapper = new ObjectMapper().registerModule(DefaultScalaModule)
  private val scopeCounter = new AtomicInteger(0)

  def fromJson(s: String): RDDOperationScope = {
    jsonMapper.readValue(s, classOf[RDDOperationScope])
  }

  /** Return a globally unique operation scope ID. */
  def nextScopeId(): Int = scopeCounter.getAndIncrement

  /**
   * Execute the given body such that all RDDs created in this body will have the same scope.
   * The name of the scope will be the first method name in the stack trace that is not the
   * same as this method's.
   *
   * Note: Return statements are NOT allowed in body.
   */
  private[spark] def withScope[T](
      sc: SparkContext,
      allowNesting: Boolean = false)(body: => T): T = {
    val ourMethodName = "withScope"
    val callerMethodName = Thread.currentThread.getStackTrace()
      .dropWhile(_.getMethodName != ourMethodName)	// 去掉了withScope之后的所有函数调用
      .find(_.getMethodName != ourMethodName)	// 找到调用withScope的函数，如RDD#withScope
      .map(_.getMethodName)
      .getOrElse {
        // Log a warning just in case, but this should almost certainly never happen
        logWarning("No valid method name for this RDD operation scope!")
        "N/A"
      }
    withScope[T](sc, callerMethodName, allowNesting, ignoreParent = false)(body)
  }

  /**
   * Execute the given body such that all RDDs created in this body will have the same scope.
   *
   * If nesting is allowed, any subsequent calls to this method in the given body will instantiate
   * child scopes that are nested within our scope. Otherwise, these calls will take no effect.
   *
   * Additionally, the caller of this method may optionally ignore the configurations and scopes
   * set by the higher level caller. In this case, this method will ignore the parent caller's
   * intention to disallow nesting, and the new scope instantiated will not have a parent. This
   * is useful for scoping physical operations in Spark SQL, for instance.
   *
   * Note: Return statements are NOT allowed in body.
   */
  private[spark] def withScope[T](
      sc: SparkContext,
      name: String,
      allowNesting: Boolean,
      ignoreParent: Boolean)(body: => T): T = {
    // Save the old scope to restore it later
    val scopeKey = SparkContext.RDD_SCOPE_KEY
    val noOverrideKey = SparkContext.RDD_SCOPE_NO_OVERRIDE_KEY
    val oldScopeJson = sc.getLocalProperty(scopeKey)
    val oldScope = Option(oldScopeJson).map(RDDOperationScope.fromJson)
    val oldNoOverride = sc.getLocalProperty(noOverrideKey)
    try {
      if (ignoreParent) {
        // Ignore all parent settings and scopes and start afresh with our own root scope
        sc.setLocalProperty(scopeKey, new RDDOperationScope(name).toJson)
      } else if (sc.getLocalProperty(noOverrideKey) == null) {
        // Otherwise, set the scope only if the higher level caller allows us to do so
        sc.setLocalProperty(scopeKey, new RDDOperationScope(name, oldScope).toJson)
      }
      // Optionally disallow the child body to override our scope
      if (!allowNesting) {
        sc.setLocalProperty(noOverrideKey, "true")
      }
	  // 在执行传入的函数前先将一个新的RDDOperationScope设定到sc中
      body
    } finally {
	  // 执行完毕后再还原
      // Remember to restore any state that was modified before exiting
      sc.setLocalProperty(scopeKey, oldScopeJson)
      sc.setLocalProperty(noOverrideKey, oldNoOverride)
    }
  }
}
</pre>
<p>
	暂时来讲，<code>withScope</code>方法所涉及到的环境变量包括<code>scopeKey</code>和<code>noOverrideKey</code>。
	以我们目前的高度，这两个变量的具体使用应该是不会接触到的，我们不妨留到深入探讨<code>SparkContext</code>的时候再仔细研究这两个变量。
</p>
<h3>filter</h3>
<pre class="brush: scala">
def filter(f: T => Boolean): RDD[T] = withScope {
  val cleanF = sc.clean(f)
  new MapPartitionsRDD[T, T](
    this,
    (context, pid, iter) => iter.filter(cleanF),
    preservesPartitioning = true)
}
</pre>
<p>可见，filter本质上也是一种map。</p>
<h3>flatMap</h3>
<pre class="brush: scala">
def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope {
  val cleanF = sc.clean(f)
  new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.flatMap(cleanF))
}
</pre>
<p>基本同上。</p>
<h3>sample</h3>
<pre class="brush: scala">
/**
 * Return a sampled subset of this RDD.
 *
 * @param withReplacement can elements be sampled multiple times (replaced when sampled out)
 * @param fraction expected size of the sample as a fraction of this RDD's size
 *  without replacement: probability that each element is chosen; fraction must be [0, 1]
 *  with replacement: expected number of times each element is chosen; fraction must be >= 0
 * @param seed seed for the random number generator
 */
def sample(
    withReplacement: Boolean,
    fraction: Double,
    seed: Long = Utils.random.nextLong): RDD[T] = withScope {
  require(fraction >= 0.0, "Negative fraction value: " + fraction)
  if (withReplacement) {
    new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed)
  } else {
    new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed)
  }
}
</pre>
<p>
	可见，<code>sample</code>方法生成了一个<code>PartitionwiseSampledRDD</code>，并根据参数的不同分别传入<code>PoissonSampler</code>或<code>BernoulliSampler</code>。
	从名字上看，这两个Sampler自然是对应着泊松分布和贝努利分布，只是两种不同的随机采样器。因此这里我们就不解析这两个采样器了。我们来看一下这个<code>PartitionwiseSampledRDD</code>：
</p>
<pre class="brush: scala">
private[spark]
class PartitionwiseSampledRDDPartition(val prev: Partition, val seed: Long)
  extends Partition with Serializable {
  override val index: Int = prev.index
}

/**
 * A RDD sampled from its parent RDD partition-wise. For each partition of the parent RDD,
 * a user-specified [[org.apache.spark.util.random.RandomSampler]] instance is used to obtain
 * a random sample of the records in the partition. The random seeds assigned to the samplers
 * are guaranteed to have different values.
 *
 * @param prev RDD to be sampled
 * @param sampler a random sampler
 * @param preservesPartitioning whether the sampler preserves the partitioner of the parent RDD
 * @param seed random seed
 * @tparam T input RDD item type
 * @tparam U sampled RDD item type
 */
private[spark] class PartitionwiseSampledRDD[T: ClassTag, U: ClassTag](
    prev: RDD[T],
    sampler: RandomSampler[T, U],
    @transient preservesPartitioning: Boolean,
    @transient seed: Long = Utils.random.nextLong)
  extends RDD[U](prev) {

  @transient override val partitioner = if (preservesPartitioning) prev.partitioner else None

  override def getPartitions: Array[Partition] = {
    val random = new Random(seed)
    firstParent[T].partitions.map(x => new PartitionwiseSampledRDDPartition(x, random.nextLong()))
  }

  override def getPreferredLocations(split: Partition): Seq[String] =
    firstParent[T].preferredLocations(split.asInstanceOf[PartitionwiseSampledRDDPartition].prev)

  override def compute(splitIn: Partition, context: TaskContext): Iterator[U] = {
    val split = splitIn.asInstanceOf[PartitionwiseSampledRDDPartition]
    val thisSampler = sampler.clone
    thisSampler.setSeed(split.seed)
    thisSampler.sample(firstParent[T].iterator(split.prev, context))
  }
}
</pre>
<p>
	实现逻辑也十分直观：<code>getPartitions</code>方法表明<code>PartitionwiseSampledRDD</code>直接利用它的parent RDD的partition作为自己的partition；
	<code>compute</code>方法则表明<code>PartitionwiseSampledRDD</code>将通过调用<code>RandomSampler</code>的<code>sample</code>方法来对Iterator进行取样。
</p>
<h3>cartesian</h3>
<pre class="brush: scala">
/**
 * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of
 * elements (a, b) where a is in `this` and b is in `other`.
 */
def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope {
  new CartesianRDD(sc, this, other)
}
</pre>
<p>使用两个<code>RDD</code>构建了一个<code>CartesianRDD</code>，似乎也十分合理。那我们来看一下这个<code>CartesianRDD</code>：</p>
<pre class="brush: scala">
private[spark] class CartesianPartition(
    idx: Int,
    @transient rdd1: RDD[_],
    @transient rdd2: RDD[_],
    s1Index: Int,
    s2Index: Int
  ) extends Partition {
  var s1 = rdd1.partitions(s1Index)
  var s2 = rdd2.partitions(s2Index)
  override val index: Int = idx

  // 重载了Serializable的writeObject方法，在任务序列化时更新s1、s2
  @throws(classOf[IOException])
  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {
    // Update the reference to parent split at the time of task serialization
    s1 = rdd1.partitions(s1Index)
    s2 = rdd2.partitions(s2Index)
    oos.defaultWriteObject()
  }
}

private[spark]
class CartesianRDD[T: ClassTag, U: ClassTag](
    sc: SparkContext,
    var rdd1 : RDD[T],
    var rdd2 : RDD[U])
  extends RDD[Pair[T, U]](sc, Nil)
  with Serializable {

  val numPartitionsInRdd2 = rdd2.partitions.length

  // 以rdd1与rdd2的partition来生成自己的partition
  override def getPartitions: Array[Partition] = {
    // create the cross product split
    val array = new Array[Partition](rdd1.partitions.length * rdd2.partitions.length)
    for (s1 &lt;- rdd1.partitions; s2 &lt;- rdd2.partitions) {
      val idx = s1.index * numPartitionsInRdd2 + s2.index
      array(idx) = new CartesianPartition(idx, rdd1, rdd2, s1.index, s2.index)
    }
    array
  }

  // preferredLocations依赖于rdd1和rdd2的preferredLocations
  override def getPreferredLocations(split: Partition): Seq[String] = {
    val currSplit = split.asInstanceOf[CartesianPartition]
    (rdd1.preferredLocations(currSplit.s1) ++ rdd2.preferredLocations(currSplit.s2)).distinct
  }

  // 直接使用rdd1和rdd2生成自身结果
  override def compute(split: Partition, context: TaskContext): Iterator[(T, U)] = {
    val currSplit = split.asInstanceOf[CartesianPartition]
    for (x &lt;- rdd1.iterator(currSplit.s1, context);
         y &lt;- rdd2.iterator(currSplit.s2, context)) yield (x, y)
  }

  // 指明自己依赖于rdd1和rdd2
  override def getDependencies: Seq[Dependency[_]] = List(
    new NarrowDependency(rdd1) {
      def getParents(id: Int): Seq[Int] = List(id / numPartitionsInRdd2)
    },
    new NarrowDependency(rdd2) {
      def getParents(id: Int): Seq[Int] = List(id % numPartitionsInRdd2)
    }
  )

  override def clearDependencies() {
    super.clearDependencies()
    rdd1 = null
    rdd2 = null
  }
}
</pre>
<p>也比较直观。</p>
<h3>distinct</h3>
<pre class="brush: scala">
/**
 * Return a new RDD containing the distinct elements in this RDD.
 */
def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
  map(x => (x, null)).reduceByKey((x, y) => x, numPartitions).map(_._1)
}
</pre>
<p>使用了<code>reduceByKey</code>的功能实现了<code>distinct</code>，可以理解。</p>
<h3>groupBy</h3>
<pre class="brush: scala">
def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope {
  groupBy[K](f, defaultPartitioner(this))
}

def groupBy[K](f: T => K, numPartitions: Int)
              (implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope {
  groupBy(f, new HashPartitioner(numPartitions))
}

def groupBy[K](f: T => K, p: Partitioner)
              (implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope {
  val cleanF = sc.clean(f)
  // 利用传入的f为每个记录生成key以后再groupByKey
  this.map(t => (cleanF(t), t)).groupByKey(p)
}
</pre>
<hr />
<h2 class="jump">总结</h2>
<p>
	至此，我们便基本能够理解了：RDD Transformation将以原本的RDD作为parent来构造一个新的RDD，不断地调用Transformation Operation就可以产生出一条RDD操作链，
	但整条流水线的启动被一直延后到RDD Action；RDD Action通过调用<code>SparkContext#runJob</code>启动整条流水线。
</p>