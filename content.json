{"pages":[{"title":"呆呆","text":"日常咸鱼，热爱技术。 主要编程语言：Python、Go、Java 主要研究方向：大数据、分布式系统、Linux、容器、云原生、Service Mesh","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"Bigtable 论文详述","text":"在这篇博文中，我会详细阐释《Bigtable: A Distributed Storage System for Structured Data》一文中所提及的 Bigtable 的开发背景以及工作原理。 作为 Google 的大数据三架马车之一，Bigtable 依托于 Google 的 GFS、Chubby 及 SSTable 而诞生，用于解决 Google 内部不同产品在对数据存储的容量和响应时延需求的差异化，力求在确保能够容纳大量数据的同时减少数据的查询耗时。Apache HBase 的设计很大程度上受到了 Bigtable 的影响，学习 Bigtable 的原理也有助于更好地理解 HBase。 数据模型Bigtable 会把数据存储在若干个 Table（表）中，Table 中的每个 Cell（数据单元）的形式如下： $$(row: \\textrm{string}, column: \\textrm{string}, time: \\textrm{int64}) \\rightarrow \\textrm{string}$$ Cell 内的数据由字节串（string）构成，使用行、列和时间戳三个维度进行定位。 Bigtable 在存储数据时会按照 Cell 的 Row Key 对 Table 进行字典排序，并提供行级事务的支持（类似于 MongoDB，不支持跨行事务）。作为分布式的存储引擎，Bigtable 会把一个 Table 按 Row 切分成若干个相邻的 Tablet，并将 Tablet 分配到不同的 Tablet Server 上存储。如此一来，客户端查询较为接近的 Row Key 时 Cell 落在同一个 Tablet 上的概念也会更大，查询的效率也会更高。 除外，Bigtable 会按照由若干个 Column 组成的 Column Family（列族）对 Table 的访问权限控制。Column Key 由 family:qualifier 的形式组成，用户在使用前必须首先声明 Table 中有哪些 Column Family，声明后即可在该 Column Family 中创建任意 Column。由于同一个 Column Family 中存储的数据通常属于同一类型，Bigtable 还会对属于同一 Column Family 的数据进行合并压缩。由于 Bigtable 允许用户以 Column Family 为单位为其他用户设定数据访问权限，数据统计作业有时也会从一个 Column Family 中读出数据后，将统计结果写入到另一个 Column Family 中。 Table 中的不同 Cell 可以保存同一份数据的多个版本，以时间戳进行区分。时间戳本质上为 64 位整数，可由 Bigtable 自动设定为数据写入的当前时间（微秒），也可由应用自行设定，但应用需要自行确保 Cell 间不会出现冲突。对于拥有相同 Row Key 和 Column Key 的 Cell，Bigtable 会按照时间戳降序进行排序，如此一来最新的数据便会被首先读取。在此基础上，用户还可以设定让 Bigtable 只保存最近若干个版本的数据或是时间戳在指定时间范围内的数据。 系统原理一个完整的 Bigtable 集群由两类节点组成：Master 和 Tablet Server。 Master 负责检测集群中的 Tablet Server 组成以及它们的加入和退出事件，会将 Tablet 分配至 Tablet Server，并负责均衡 Tablet Server 间的存储负载以及从 GFS 上回收无用的文件。除外，Master 还负责管理如 Table、Column Family 的创建和删除等 Schema 修改操作。 每个 Tablet Server 会负责管理若干个由 Master 指定的 Tablet，负责处理针对这些 Tablet 的读写请求，并负责在 Tablet 变得过大时对其进行切分。 Bigtable 集群会管理若干个 Table，每个 Table 由若干个 Tablet 组成，每个 Tablet 都会关联一个指定的 Row Key 范围，那么这个 Tablet 就包含了该 Table 在该范围内的所有数据。初始时，Table 会只有一个 Tablet，随着 Tablet 增大被 Tablet Server 自动切分，Table 就会包含越来越多的 Tablet。 Tablet 定位Bigtable 的 Tablet 之间会形成一个三层结构，具体如下： 在 Chubby 中的一个 File 保存着 Root Tablet 的位置 Root Tablet 保存着 METADATA Table 所有 Tablet 的位置 METADATA Table 中保存着其他所有 Table 的 Tablet 的位置 值得注意的是，Root Tablet 是特殊的：无论它的体积如何增长都不会被切分，保证唯一。METADATA 中的每一行都代表 Bigtable 中其他 Table 的一个 Tablet，其 Row Key 由该 Tablet 的 Table 名及 Row Key 上限编码而成。除了 Tablet 的位置信息外，METADATA 表也会保存一些其他有用的元信息，例如 Tablet 的事件日志等。 客户端想要定位某个 Tablet 时，便会递归地安装上述层次向下求得位置，并把中间获得的结果缓存在自己的内存中。如果某一时刻客户端发现缓存在内存中的地址已不再有效，它便会再次递归地沿着上述层次向上，最终再次向下求得所需 Tablet 的位置。 集群成员变化与 Tablet 分配Bigtable Master 利用了 Chubby 来探测 Tablet Server 加入和离开集群的事件。每个 Tablet Server 在 Chubby 上都会有一个对应的唯一文件，Tablet Server 在启动时便会拿到该文件在 Chubby 上的互斥锁，Master 则通过监听这些文件的父目录来检测 Tablet Server 的加入。如果 Tablet Server 失去了互斥锁，那么 Master 就会认为 Tablet Server 已退出集群。尽管如此，只要该文件仍然存在，Tablet Server 就会不断地尝试再次获取它的互斥锁；如果该文件已被删除（见下文），那么 Tablet Server 就会自行关闭。 在了解了集群中有哪些 Tablet Server 后，Master 便需要将 Tablet 分配给 Tablet Server。同一时间，一个 Tablet 只能被分配给一个 Tablet Server。Master 会通过向 Tablet Server 发送 Tablet 载入请求来分配 Tablet。除非该载入请求在 Master 失效前仍未被 Tablet Server 接收到，那么就可以认为此次 Tablet 分配操作已成功：Tablet Server 只会接受来自当前 Master 的节点的请求。当 Tablet Server 决定不再负责某个 Tablet 时，它也会发送请求通知 Master。 Master 在检测到 Tablet Server 失效（互斥锁丢失）后，便会将其负责的 Tablet 重新分配。为此，Master 会尝试在 Chubby 上获取该 Tablet Server 对应的文件的互斥锁，并在成功获取后删除该文件，确保 Tablet Server 能够正确下线。之后，Master 便可顺利将 Tablet 分配至其他 Tablet Server。 如果 Master 与 Chubby 之间的通信连接断开，那么 Master 便会认为自己已经失效并自动关闭。Master 失效后，新 Master 恢复的过程如下： 在 Chubby 上获取 Master 独有的锁，确保不会有另一个 Master 同时启动 利用 Chubby 获取仍有效的 Tablet Server 从各个 Tablet Server 处获取其所负责的 Tablet 列表，并向其表明自己作为新 Master 的身份，确保 Tablet Server 的后续通信能发往这个新 Master Master 确保 Root Tablet 及 METADATA 表的 Tablet 已完成分配 Master 扫描 METADATA 表获取集群中的所有 Tablet，并对未分配的 Tablet 重新进行分配 Tablet 读写与维护如上所述，Tablet 的数据实际上存储在 GFS 中，由 GFS 提供数据的冗余备份。Tablet 数据读操作与写操作的示意图如下： 可见，一个 Tablet 由若干个位于 GFS 上的 SSTable 文件、一个位于内存内的 MemTable 以及一份 Commit Log 组成。 在进行写操作时，Bigtable 首先会用先写日志（Write-Ahead Log）的方式，把此次变更记录到 Commit Log 中。而后，插入的数据会被放入到位于内存内的一个 MemTable 中，其中 MemTable 保持其内部的数据有序。而对于那些已经持久化的数据则会作为一个个 SSTable 文件保存在 GFS 中。 在进行读操作时，Tablet Server 也会进行相应的权限检查，而后会首先尝试从 MemTable 中获取所需的最新数据，如果无法查得再从 SSTable 中进行查找。 除外，Tablet Server 在收到操作请求时也会检查请求的用户是否有足够的权限，而允许执行的用户列表则存储在 Chubby 的一个文件中。 Tablet Server 在载入 Tablet 时，首先需要从 METADATA 表中获取 Tablet 对应的 SSTable 文件及 Commit Log 的日志，并利用 Commit Log 中的条目恢复出 Tablet 的 MemTable。 Memtable 与 SSTable 本身都采取了数据不可变的设计思路：更改操作产生的新条目以 Copy On Write 的方式放入到 MemTable 中；待 MemTable 内的条目数达到一定阈值后，Bigtable 便会将新到来的请求写入到另一个 MemTable，同时开始将旧的 MemTable 写入到新的 SSTable 文件中，该操作被称为 Bigtable 的 Minor Compaction。对于已在原有 SSTable 文件中的旧数据，Bigtable 也不会将其移除。 每一次 Minor Compaction 都会产生一个新的 SSTable 文件，而过多的 SSTable 文件会导致后续的读操作需要扫描更多的 SSTable 文件以获得最新的正确数据。为了限制 SSTable 文件数，Bigtable 会周期地进行 Merging Compaction，将若干个 SSTable 和 MemTable 中的数据原样地合并成一个 SSTable。 Bigtable 还会周期地执行一种被称为 Major Compaction 的特殊 Merging Compaction 操作：在这个过程中，Bigtable 除了会将若干个 SSTable 合并为一个 SSTable，同时将 SSTable 中那些应后续变更或删除操作而被标记为无效的条目移除。 额外优化上一章中我们聊了 Bigtable 写入数据和读取数据的基本过程，但仅有此还不足以让 Bigtable 支撑上游产品的实际使用需求。本章我们就来讲讲 Google 为了让 Bigtable 拥有实际可用的性能及可用性所做出的主要优化。 Locality GroupBigtable 允许客户端为 Column Family 指定一个 Locality Group，并以 Locality Group 为基础指定其实际的文件存储格式以及压缩方式。 首先，在进行上面我们提到的 Compaction 操作时，Bigtable 会为 Tablet 中的每个 Locality Group 生成独立的 SSTable 文件。由此，用户便可将那些很少同时访问的 Column Famliy 放入到不同的 Locality Group 中，以提高查询效率。除外 Bigtable 也提供了其他基于 Locality Group 的调优参数设置，如设置某个 Locality Group 为 in-memory 等。 在压缩方面，Bigtable 允许用户指定某个 Locality Group 是否要对数据进行压缩以及使用何种格式进行压缩。值得注意的是，Bigtable 对 SSTable 的压缩是基于 SSTable 文件的 Block 进行的，而不是对整个文件直接进行压缩。尽管这会让压缩的效率下降，但这也使得用户在读取数据时 Bigtable 只需要对 SSTable 的某些 Block 进行解压。 读缓存与 Bloom Filter了解过 LSM Tree 的读者可能已经意识到，Bigtable 使用的存储方式正是 LSM Tree：这种存储方式可以将对磁盘的随机写转换为顺序写，代价则是读取性能的下降。LSM Tree 被应用在 Bigtable 上是合情合理的，毕竟 Bigtable 的文件实际上存储在 GFS 中，而 GFS 主要针对顺序写进行优化，对随机写的支持可以说是极差。那么 Bigtable 在使用 LSM Tree 确保了写入性能后，当然就要通过其他的方式来确保自己的读性能了。首先便是读缓存。 总的来说，Bigtable 的读缓存由两个缓存层组成：Scan Cache 和 Block Cache。Block Cache 会缓存从 GFS 中读出的 SSTable 文件 Block，提高客户端读取某个数据附近的其他数据的效率；Scan Cache 则在 Block Cache 之上，缓存由 SSTable 返回给 Tablet Server 的键值对，以提高客户端重复读取相同数据的效率。 除外，为了提高检索的效率，Bigtable 也允许用户为某个 Locality Group 开启 Bloom Filter 机制，通过消耗一定量的内存保存为 SSTable 文件构建的 Bloom Filter，以在客户端检索记录时利用 Bloom Filter 快速地排除某些不包含该记录的 SSTable，减少需要读取的 SSTable 文件数。 Commit LogBigtable 使用了 Write-Ahead Log 的做法来确保数据高可用，那么便涉及了大量对 Commit Log 的写入，因此这也是个值得优化的地方。 首先，如果 Bigtable 为不同的 Tablet 使用不同的 Commit Log，那么系统就会有大量的 Commit Log 文件同时写入，提高了底层磁盘寻址的时间消耗。为此，Tablet Server 会把其接收到的所有 Tablet 写入操作写入到同一个 Commit Log 文件中。 这样的设计带来了另一个问题：如果该 Tablet Server 下线，其所负责的 Tablet 可能会被重新分配到其他若干个 Tablet Server 上，它们在恢复 Tablet MemTable 的过程中会重复读取上一个 Tablet Server 产生的 Commit Log。为了解决该问题，Tablet Server 在读取 Commit Log 前会向 Master 发送信号，Master 就会发起一次对原 Commit Log 的排序操作：原 Commit Log 会按 64 MB 切分为若干部分，每个部分并发地按照 (table, row name, log sequence number) 进行排序。完成排序后，Tablet Server 读取 Commit Log 时便可只读取自己需要的那一部分，减少重复读取。 结语总的来说，作为 Google 大数据的三驾马车之一，Bigtable 的论文还是很值得我们去学习的。对我个人而言，Bigtable 论文中的以下两点是最有启发性的： 利用 Chubby 分布式锁服务实现节点间的协调 利用 LSM Tree 将数据库的随机写入操作转化为顺序写入，进而利用 GFS 提供数据冗余 类似 Chubby 的服务在开源界已经有很多了，无论是倍受其影响的 ZooKeeper 还是后来出现的 etcd，现在大家对于如何使用这类服务都有了很好的认识。关于后一点，LSM Tree 也渐渐成为了分布式数据存储的宠儿，无论是 Bigtable、HBase，还是 LevelDB、RocksDB 以及 TiDB，我们都能够看到它的身影，未来我也许也会专门写一篇文章总结 B、B+ 树和 LSM Tree，对比一下它们在分布式存储领域中的表现。","link":"/bigtable/"},{"title":"分布式一致性模型介绍","text":"在阅读不同的分布式系统论文时发现，论文中经常会提到该系统实现了一个什么样的一致性模型。了解常见的几种一致性模型的定义想必会对后续的论文阅读有不少的帮助。 这篇文章的内容梳理自此前我在公司内部做的技术分享，介绍了分布式系统的一致性模型是什么，有哪些常见的一致性模型，以及常见的分布式一致性实现方式。 因为是整理自我以前做过的分享，本文会有大量的 PPT 截图，此外也会有一些当时我向听众的提问，也会以思考题的形式梳理在本文中，题目的答案可以在文末找到。 一致性与一致性模型在这一节中，我们先来看一下分布式系统的一致性和一致性模型的定义。 众所周知，一个分布式系统为了保证数据服务的可靠性，一种常用的实现方式就是 数据备份： 在数据备份模式中，分布式系统会在不同的节点上存储相同的数据，这样当部分节点失效时，其他节点上也能有完整的数据来响应外部的请求，从而保证服务的高可用。 数据备份模式并不局限于常见的分布式数据库系统。如 DNS、CDN，甚至是客户端的 HTTP 请求缓存，都可以被视为数据备份的一种形式，因为本质上它们都是在不同的机器节点上存储了相同的数据。 而分布式系统的所谓 一致性，即指每份数据备份的内容相同。 CAP 理论中的一致性，指 外部的每个读操作都能返回最新的写入结果，即系统能够对外呈现一致的数据视图即可，不要求内部每个节点时刻都持有相同的数据。 在介绍一致性模型的定义之前，我们先来看一种最简单也是最理想的一致性模型：严格一致性（Strict Consistency）。 严格一致性模型要求，在分布式系统一个节点上的数据写入 立刻 就对其他 所有 节点可见： 严格一致性是最严格也是最理想的一致性模型，它要求分布式系统时刻保持一致。说它是一种理想的一致性模型，是因为系统如果要实现严格一致性的话所需要承担的 节点间通信成本 是非常大的。我们假想一个场景，有数据备份 $R$，客户端 P 以 $N$ 次每秒的频率读取数据备份 $R$，其他客户端以 $M$ 次每秒的频率更新数据备份 $R$。若有 $N &lt;&lt; M$，意味着数据备份 $R$ 有大量的历史版本实际上不会被客户端 P 观察到，而严格一致性依然要求系统对这些历史变更进行完整的同步，实际上这就引入了不少的浪费，因为我们知道要讲数据变更在多个节点间同步是需要消耗一定的计算资源及网络资源的。 综上所述，严格一致性是最严格也是最理想的一致性模型，它缺少足够的现实意义，在于系统要实现严格一致性需要付出不必要的高昂成本。为了在一致性和性能之间达到良好的平衡，现代分布式系统通常会选择 牺牲一定的一致性 从而换取足够的性能。但这些系统依然希望能够支持到 CAP 理论中的 C，即时刻能为外部客户端提供一致的数据视图。要做到这一点，现代分布式系统就会对客户端发起请求的方式提出一定的约束。 讲到这里，我们就可以来介绍一下，什么是一致性模型了。各种各样的 一致性模型（Consistency Model）实际上描述了客户端和系统交互的一系列规则，客户端遵循这些规则才能始终从系统获得一致、可预测的数据响应。客户端必须遵循一致性模型所设定的规则，否则可能会从系统处得到预期外的结果，而这种情况下应当被视为客户端有 Bug。 严格一致性是对客户端最友好的一致性模型：它不要求客户端留意任何规则，允许客户端在任何时刻发起任意请求，系统依然能返回一致的结果。 在前面我们也提到，系统的一致性与性能之间是存在本质矛盾的，强一致性保证也会影响系统的可扩展性，在于： 数据备份间的同步会消耗系统对外的请求处理能力 越多的系统节点意味着越高的同步成本：数据备份同步本身就缺乏足够的可扩展性！ 现代的分布式系统通常都会基于主要针对支持的使用场景和数据使用模式，来 放松 自己对外的一致性保证，由客户端应用来 容忍 特定的不一致行为，进而换取足够的性能。也是由此，业界诞生出了不同类型的一致性模型和分布式系统。 常见的一致性模型在这一节中，我将给大家介绍几种常见的一致性模型。 严格一致性 - Strict Consistency在上一节中我们已经介绍过了严格一致性模型，它要求系统接收到的写入操作要 立刻 在 所有 节点上可见。 同样，如上一节所述，严格一致性是最严格也是最理想的一致性模型，它缺少足够的现实意义。 顺序一致性 - Sequential Consistency顺序一致性相比之下会比严格一致性稍微放松一些，它不要求写入操作立刻在所有节点上可见，仅要求 不同的写入操作以相同的顺序在各个节点上可见。 例如我们看下面这个操作时序图： 节点 P1 和 P2 分别在时间点 1 和 2 完成了 $x = a$ 和 $x = b$ 的写入。然后我们可以看到，节点 P3 和 P4 均是先能够读出 $x = b$，再能够读出 $x = a$。尽管看起来与最初写入的顺序不同，但我们看到两次写入操作都是以先 $b$ 后 $a$ 的顺序在 P3 P4 节点上可见，因此这样的时序是满足顺序一致性的。 因果一致性 - Causal Consistency因果一致性是比顺序一致性更加放松的一致性模型，它要求 存在因果关系的写入操作以相同的顺序在各个节点上可见。 对于如何判断两个操作间是否存在因果关系，不同的分布式系统可能有不同的设定。通常会把同一个节点先后完成的操作视为存在因果关系。 例如我们看下面这个操作时序图： 先看节点 P1，它先后完成了 $x = a$ 和 $x = c$ 的写入操作，两次操作视为存在因果关系，因此其他节点需要满足先观察到 $x = a$ 再观察到 $x = c$。再看节点 P2，它先是读出了 $x = a$，然后再写入了 $x = b$，此时依然需要将这两次操作视为存在因果关系，由此其他节点需要满足先观察到 $x = a$ 再观察到 $x = b$。至此，我们已有结论，其他节点必须先观察到 $x = a$，再观察到 $x = b$ 和 $x = c$，但后两次数据变更的可见顺序没有要求，上图中的 P3 和 P4 就代表了这两种合法的数据变更同步顺序，因此上图的时序是满足因果一致性的。 思考题 1：对于一个满足因果一致性的系统，有下面这张操作时序图 请问： 节点 P3 在时间点 5 读出的 $x$ 值是？ 节点 P4 在时间点 5 读出的 $y$ 值是？ 思考题 2：满足顺序一致性的分布式系统是否一定满足因果一致性？ 最终一致性 - Eventual Consistency最终一致性是最弱的一致性模型，它既不保证系统完成数据一致的时延，也不保证写入可见的顺序，仅保证系统 总能在某一时间后 收敛到一致。 线性一致性 - Linearizability最后我们要介绍的是线性一致性。线性一致性在定义上对顺序一致性模型针对现实场景进行了一定的补充和扩展，包括： 加入了对来自不同节点的 并发写入 的考虑 加入了对 不同操作的全局顺序 的考虑 可以说，线性一致性是一种更具现实意义的顺序一致性模型，而我们通常会将满足线性一致性的分布式系统视为满足 CAP 理论中的 C。例如，在我此前解析过的 Raft 论文 的第八章就明确提到了 Raft 为客户端提供线性一致性的保证。 与上述几种较理论的一致性模型不同，线性一致性会将一次操作拆分为 客户端发起调用 和 系统返回响应 两个瞬时事件，两次事件的发生存在先后的时间差，而操作将在这两个事件之间的某一个时间点上实际生效。 例如我们来看下面这样一张 FIFO 队列的线性一致操作时序图（以水平为时间轴，不同的操作行属于不同的节点）： 首先我们可以留意到，这张时序图与以往的有所不同，在于来自两个客户端的入队操作同时发生。考虑到线性一致性认为操作会在操作开始和结束之间的某一时刻上生效，因此对于这个时序，两次入队操作的两种先后次序都是可能的。 我们再来看这样一张时序图： 我们可以看到，在这张时序图中，P2 节点在 P1 节点完成最初的入队 $a$ 操作后才发起入队 $b$ 的操作，$a$ 实际入队的时间点必然早于 $b$，因此 P1 节点后续先出队 $b$ 是不合理的，这张时序图也是不满足线性一致性的。 这也就是为什么说，线性一致性模型考虑了不同操作的全局顺序。我们可以看到，尽管操作来自两个不同的节点，但从一个全局时钟的视角来看，P2 节点入队 $b$ 的操作是晚于 P1 节点入队 $a$ 的操作的，因为前者的开始时间晚于后者的结束时间，而线性一致性则要求对于这样的操作，系统需要保留它们的先后生效顺序。 保留操作的全局顺序的好处在于，对于同一个客户端来说，它可以将自己不同的操作派发给不同的系统节点处理，只要它串行地进行操作，这些操作在系统上就会以相同的顺序生效；而对于仅满足因果一致性的系统来说，要做到这一点会要求客户端始终将自己的操作发送给同一个系统节点处理。 面向客户端的一致性模型前面我们提到的几种一致性模型，都属于 面向服务端的一致性模型：它们都是从服务端的视角出发，要求系统对不同节点/并发访问的客户端提供特定的一致性保证。 除了面向服务端的一致性模型以外，还有 面向客户端的一致性模型：它们是从单个客户端的视角出发，要求系统对同一个客户端先后发起的读写操作提供特定的一致性保证。 面向客户端的一致性模型包括 4 种： 单调读一致（Monotonic Reads）：客户端后续发起的 读操作 能够感知到先前 读取 到的或更新的版本 写读一致（Read Your Writes）：客户端后续发起的 读操作 能够感知到先前 写入 的或更新的版本 读写一致（Writes Follow Reads）：客户端后续发起的 写操作 能够感知到先前 读取 到或更新的版本 单调写一致（Monotonic Writes）：客户端后续发起的 写操作 能够感知到先前 写入 的或更新的版本 与面向服务端一致性模型不同的是，上述 4 中面向客户端的一致性模型在定义上互无交集，分布式系统可以选择提供上述的任意多个面向客户端一致性保证。以 MongoDB 的 可调一致性 为例，甚至可以让客户端通过不同的查询参数配置选择不同的客户端一致性保证，详见 https://docs.mongodb.com/manual/core/causal-consistency-read-write-concerns/。 结语在这篇文章中，我梳理了以往我做过的一次技术分享的内容，给大家介绍了什么是一致性模型，以及几种常见的一致性模型，想必对于大家后续阅读更多的分布式系统论文会有不少的帮助。 附录：思考题答案 思考题 1：对于一个满足因果一致性的系统，有下面这张操作时序图 请问： 节点 P3 在时间点 5 读出的 $x$ 值是？ 节点 P4 在时间点 5 读出的 $y$ 值是？ P3 读出的是 $x = a$，P4 读出的 $y$ 值不确定。 基于节点 P2 的时序，我们可以知道 P3 P4 会先观察到 $x = a$，再观察到 $y = b$。P3 在时间点 4 的读取到 $y = b$，意味着此时它已经能观察到 $x = a$，因此它在时间点 5 将读出 $x$ 的值为 $a$。P4 在时间点 4 读取到了 $x = a$，但缺少足够的信息判断 $y = b$ 的变更是否已在 P4 所连接的节点上可见，因此无法判断 P4 在时间点 5 将读出什么 $y$ 值。 思考题 2：满足顺序一致性的分布式系统是否一定满足因果一致性？ 是。回忆因果一致性要求在同一节点上完成的操作要以相同的顺序在其他节点上可见，而顺序一致性则要求所有操作要以相同的顺序在所有节点上可见。在一个满足顺序一致性的系统中，如果我们只考虑它其中的某个节点，先后完成了一些操作，这些操作是需要以相同的顺序在其他节点上可见的，因为这些操作已经以该顺序在该节点上可见，顺序一致性的保证要求了这些操作需要以相同的顺序在其他节点上可见，这恰好就满足了因果一致性的要求。由此可见，在定义上，顺序一致性是包含因果一致性的。","link":"/consistency-models/"},{"title":"Jython：在 Java 程序里运行 Python 代码","text":"教你如何使用 Jython 在 Java 程序中嵌入 Python 代码。 前言众所周知，JVM 在大数据基础架构领域可以说是独占鳌头，当我们需要开发大数据处理的相关组件时，首先会想到要使用的语言便是 Java 和 Scala。相比于 Java，Scala 的代码会更加简洁，但也有着高得多的入门门槛，因此为了保证核心组件的稳定和易于维护，我们多数时候都会更倾向于使用 Java 进行开发。 不过，组件中相对稳定的基本功能和框架尚且不谈，对于那些需要快速灵活变化的部分，使用 Java 进行开发则会有些捉襟见肘。例如，我们在为业务方开发一套通用的实时作业时，业务方需要作业在特定的处理环节中支持通过配置自定义的代码来指定算子的行为，并且在配置发生变化时需要可以在不重启实时作业的情况下进行热更新。直接使用 Java 实现这样的功能无疑会有点力不从心，为此我们就需要借助动态语言的力量了。 实际上，在 JVM 平台上使用动态语言的场景并不少见：Groovy 便是为此而生的一门语言。尽管在部分场景下 Groovy 确实是不错的选择，但对于大数据分析来说，Groovy 并不为多数数据开发人员所熟知，相比之下 Python 会是更好的选择。 目前也有不少的大数据框架支持用户提交运行 Python 代码： Hadoop MapReduce 借助 Hadoop Streaming，使用标准输入流和标准输出流进行进程间的数据交换，可以运行包括 Python 在内任意语言写成的可执行文件 Apache Spark 提供了 pyspark 编程入口，其使用了 Py4J 来实现 JVM 与 Python 进程间的高效数据传输 Apache Flink 则使用了 Jython 来运行用户的 Python 代码。 最终，我们选择了使用 Jython 来实现这样的功能。Jython 类似于 Groovy，能够与宿主 Java 程序在同一个 JVM 进程中运行，相比于 Hadoop Streaming 或是 Py4J 的方案减少了进程间数据传输的损耗，以换来更高的性能。 但在使用 Jython 的时候我们仍然需要注意几点： 部分 PyPI 包可能无法在 Jython 中运行，尤其是那些包含 C 语言扩展的包 和 Groovy 一样，随意地使用 Jython 可能会导致内存泄漏 目前，Jython 已在 2017 年 6 月发布了 2.7.1 版，支持所有 Python 2.7 语法。尽管距离其上一次发布更新已经过去了很长一段时间，但如果你有兴趣看一下它的源代码仓库的话，你会发现它仍在持续迭代中。 Jython 基本使用本文剩下的内容会集中介绍如何在 Java 程序中使用 Jython。关于其他使用 Jython 的方式，可以参考 Jython 官方给出的 Jython Book，这里我们便不再赘述。 实际上，Jython 的官方文档也给出了在 Java 中嵌入 Jython 的基本示例，极其简单： 1234567891011121314151617181920import org.python.util.PythonInterpreter; import org.python.core.*; public class SimpleEmbedded { public static void main(String[] args) throws PyException { PythonInterpreter interp = new PythonInterpreter(); System.out.println(&quot;Hello, brave new world&quot;); interp.exec(&quot;import sys&quot;); interp.exec(&quot;print sys&quot;); interp.set(&quot;a&quot;, new PyInteger(42)); interp.exec(&quot;print a&quot;); interp.exec(&quot;x = 2+2&quot;); PyObject x = interp.get(&quot;x&quot;); System.out.println(&quot;x: &quot;+x); System.out.println(&quot;Goodbye, cruel world&quot;); }} 简单，但并不可用。 首先，PythonInterpreter 是个非常重的类，其中包含了 Jython 用于编译 Python 代码所需的所有资源和上下文信息。你不会想要大量创建这样的实例的。 此外，Jython 的实现导致对 PythonInterpreter.eval 方法的重复调用会对相同的 Python 代码不断重复编译运行，导致内存泄漏。 要解决以上问题，我们需要复用 PythonInterpreter 对象，并尽可能不要调用 PythonInterpreter.eval 方法。 复用 PythonInterpreter 对象十分简单：将其实现为单例维护起来即可。你可以以任何形式实现这样的单例模式，简单起见我们这里直接将其设置为一个 private static final 变量： 123456789101112public class PythonRunner { private static final PythonInterpreter intr = new PythonInterpreter(); public PythonRunner(String code) { // ... } public Object run() { // ... }} 要想绕过 PythonInterpreter.eval 并不容易，毕竟这是 PythonInterpreter 提供给我们唯一可以运行指定 Python 代码并获取结果的方法。 Groovy 提供了 GroovyShell.parse 方法，可以对给定的 Groovy 代码进行编译，并返回一个 Script 对象。Groovy 这里做的事情实际上是把客户端给定的 Groovy 代码封装在了一个新的 Java 类中（这个类继承了 Script），因此实际上程序可以使用这个 Script 对象的类创建出新的 Script 对象，即可复用这段 Groovy 代码。 我们同样可以在 Jython 这边实现类似的功能 —— 实际上官方的 Jython Book 有提到类似的做法，名为对象工厂模式。按照 Jython Book 中给出的示例，你可以将你需要使用的 Python 代码放到一个 Python 类中，再进行编译，但考虑到我们的场景比较简单，这里我们就简单地将代码放在一个 Python 函数中即可： 123456789101112131415161718192021222324252627282930313233import org.python.core.PyFunction;import org.python.util.PythonInterpreter;public class PythonRunner { private static final PythonInterpreter intr = new PythonInterpreter(); private static final String FUNC_TPL = String.join(&quot;\\n&quot;, new String[]{ &quot;def func():&quot;, &quot; %s&quot;, &quot;&quot;, }); private final PyFunction func; public PythonRunner(String code) { // 渲染函数内容 String[] lines = code.split(&quot;\\n&quot;); for (int i = 1; i &lt; lines.length; i++) lines[i] = &quot; &quot; + lines[i]; code = String.join(&quot;\\n&quot;, lines); code = String.format(FUNC_TPL, code); // 编译并获取 PyFunction 对象 intr.exec(code); func = (PyFunction) intr.get(funcName); } public Object run() { // 使用 PyFunction 对象的 __call__ 方法，调用指定的 Python 代码 return func.__call__(); }} 功能扩展目前，你已经学到了如何在 Java 程序中使用 Jython 安全地运行 Python 代码，你可以对上述代码进行进一步的扩展来满足你的需求。这里我再简单介绍下我们做的两个比较有用的扩展。 在 Python 代码中使用 Java 对象在你使用编译后得到的 PyFunction 对象时，你可能会注意到它的 __call__ 方法可以接收任意个类型为 PyObject 的参数。这是不是说，我们得把我们的 Java 对象转换成 PyObject，我们的 Python 代码才能使用这些 Java 对象呢？ 答案是否定的，实际上 Jython 已经实现了类似的自动转换功能。如果你提供的是“标准的” Java 对象，那么 Jython 就会把它 “mock” 成对应的 Python 基本类型对象： 所有的 Java 基本数据类型都会被转换为对应的 Python 基本数据类型（例如 short 转 int、boolean 转 bool） 可以像使用普通 Python dict 对象那样使用 java.util.Map 实例 可以像使用普通 Python list 对象那样使用 java.util.List 实例 举个例子，我们的项目需要使用到 FastJSON 的 JSONObject，而这个类实现了 java.util.Map，因此在我们的 Python 代码中，我们只要将它当做一个普通的 Python dict 来使用就好了: 1234def func(json): if not json['test']: json['test'] = True return True 值得注意的是，Jython 并不会改变你的对象的类型：如果你在你的 Python 代码中使用 instanceof 的话就会发现，实际上传入对象的类型并未改变。除外，如果你对一个 Java bool 值在 Python 代码中使用 is True 或 is False 判断时，你都会得到 False 结果。实际上 Jython 仅仅是为你给定的 Java 对象模拟出了对应的 Python 类型的行为（鸭子类型），但实际上它们依然是不同的类型。 引入 PyPI 包为了进一步减少我们需要写的 Python 代码量，我们也可以把部分公用的 Python 代码维护在统一的包中，然后在自定义的 Python 代码中 import 并使用它。要做到这一点，首先我们要设置好 sys.path。 Jython 默认会把当前工作目录放到 sys.path 中（实际上这应该是所有 Python 解释器的标准行为），所以如果我们需要复用某个自制的 Python 库文件，我们只要将它放在当前工作目录下然后 import 就可以了。但如果我们想要使用 PIP 安装的包，我们就需要额外做一些配置了。 实际上，我们只要把本地的 PIP 安装目录路径放到 Jython 的 sys.path 中即可。有很多种方法可以做到这一点，但最安全的做法就是直接询问本地安装好的 Python： 12345678910111213141516171819202122232425public class PythonRunner { private static final PythonInterpreter intr = new PythonInterpreter(); static { intr.exec(&quot;import sys&quot;); try { // 启动子进程，运行本地安装的 Python，获取 sys.path 配置 Process p = Runtime.getRuntime().exec(new String[]{ &quot;python2&quot;, &quot;-c&quot;, &quot;import json; import sys; print json.dumps(sys.path)&quot;}); p.waitFor(); // 从中获取到相关的 PIP 安装路径，放入 Jython 的 sys.path String stdout = IOUtils.toString(p.getInputStream()); JSONArray syspathRaw = JSONArray.parseArray(stdout); for (int i = 0; i &lt; syspathRaw.size(); i++) { String path = syspathRaw.getString(i); if (path.contains(&quot;site-packages&quot;) || path.contains(&quot;dist-packages&quot;)) inter.exec(String.format(&quot;sys.path.insert(0, '%s')&quot;, path)); } } catch (Exception ex) {} } // ...} 正如我在一开始所说的那样，并不是所有 PyPI 包都能在 Jython 中运行，尤其是那些包含 C 语言代码的包。因此，在你做更多的尝试前，不妨先在 Jython Shell 中 import 一下你想使用的包，验证一下。 结语这篇博文一方面是对最近我们在做的工作进行一次总结，同时希望这些经验也能够帮助到大家。 不过，我不会认为 Jython 是个 100% 安全的解决方案 —— 实际上，你在使用的过程中有可能会遇到十分诡异的 Bug，而且 Jython 的 API 和文档也还远算不上是“友好”。但不管怎么说，如果你有和我们类似的需求的话，也不妨尝试一下 Jython。","link":"/embedding-jython-ch/"},{"title":"Embedding Python in Java using Jython","text":"In the past few days, I managed to find a way to dynamically load and run Python code in a Java program. In this post, I will briefly explain how I achieve this. BackgroundRecently, I needed to add a new feature to the project I’ve been working on: it needs to be able to dynamically load Python code into a Spark Streaming program and use it to process real-time messages. In addition, this module need to support hot-reload: the program should be able to switch to a new Python code configuration without restarting the whole Spark Streaming job. It is not unusual to use dynamic language in the Java plaform: the Groovy language was actually born for this, to some extent. Unfortunately, Groovy is not widly used among data developers, and Python would be a better choice in comparison. Using Python in JVM-based computing framework is not uncommon either: Hadoop MapReduce can invoke ANY executable (not only Python) using Hadoop Streaming; it uses stdin/stdout to transmit data between JVM and the designated program, which can be written in any language. Apache Spark provides pyspark using Py4J, which enables efficient communication between JVM and Python program. Apache Flink uses Jython to run user Python code. In the end, We chose Jython to achieve the highest performance possible. Jython is similar to Groovy, running in the same JVM process as the host Java program, in comparison to Hadoop Streaming or Py4J. But there are still a few things need to be considered when using Jython: Some PyPI packages might not be able to run in Jython, especially those with C extensions. Using Jython without cautions might lead to memory leak, like Groovy. For now, Jython has released its 2.7.1 version in June, 2017, and supports up to Python 2.7 syntax. Although it hasn’t been updated for a very long time, Jython is actually still under active development, if you could take a look at its source code repository. Using JythonThe Jython Book has given a detailed explaination on how to use Jython, so I would not repeat it here. This post will mainly focus on how to use Jython in Java. Actually, the official documentations has already mentioned how to embed Jython in Java, and it’s pretty simple: 1234567891011121314151617181920import org.python.util.PythonInterpreter; import org.python.core.*; public class SimpleEmbedded { public static void main(String[] args) throws PyException { PythonInterpreter interp = new PythonInterpreter(); System.out.println(&quot;Hello, brave new world&quot;); interp.exec(&quot;import sys&quot;); interp.exec(&quot;print sys&quot;); interp.set(&quot;a&quot;, new PyInteger(42)); interp.exec(&quot;print a&quot;); interp.exec(&quot;x = 2+2&quot;); PyObject x = interp.get(&quot;x&quot;); System.out.println(&quot;x: &quot;+x); System.out.println(&quot;Goodbye, cruel world&quot;); }} Simple, but not practical: PythonInterpreter is a super heavy class: it maintains all the context and resources needed to compile and run Python code. You do not want to create too many instances of it. Possibly related to how Jython is implemented, repeated invocations of PythonInterpreter.eval will compile the same Python code over and over again, which can lead to memory leak. To solve these problems, the key is to reuse PythonInterpreter object, and try the best to avoid invoking PythonInterpreter.eval. Reusing PythonIntrepreter is simple: just hold it somewhere in your program as a singleton. You can implement this singleton pattern in any way you like: Spring Application Context, double-check locking, you name it. We will just make it a private static final variable here: 123456789101112public class PythonRunner { private static final PythonInterpreter intr = new PythonInterpreter(); public PythonRunner(String code) { // ... } public Object run() { // ... }} It’s a bit trickier to avoid invoking PythonInterpreter.eval, as it is the only method we can use to run designated Python code and get its result. Groovy provides GroovyShell.parse, which takes a Groovy script as input and returns a Script instance. Groovy here actually wrap the given script in a newly created Java class (which extends Script), and so we can use this class to create new Script instances, and reuse the same compiled Groovy code. We can implement the same technique in Jython – actually the Jython Book has also mentioned this kind of usage, which named Object Factory. You can choose to compile your Python code in a Python class, but we will just use it as a Python function here, which suffices in our scenario: 12345678910111213141516171819202122232425262728293031import org.python.core.PyFunction;import org.python.util.PythonInterpreter;public class PythonRunner { private static final PythonInterpreter intr = new PythonInterpreter(); private static final String FUNC_TPL = String.join(&quot;\\n&quot;, new String[]{ &quot;def __call__():&quot;, &quot; %s&quot;, &quot;&quot;, }); private final PyFunction func; public PythonRunner(String code) { // Render the function body String[] lines = code.split(&quot;\\n&quot;); for (int i = 1; i &lt; lines.length; i++) lines[i] = &quot; &quot; + lines[i]; code = String.join(&quot;\\n&quot;, lines); code = String.format(FUNC_TPL, code); intr.exec(code); func = (PyFunction) intr.get(funcName); } public Object run() { return func.__call__(); }} Note that you need to use PythonInterpreter.exec to compile the Python function here, and get the Python function object later. So far, you have learned how to load and run Python code safely in a Java program. You can easily extend this code to meet your own requirement. Next I will introduce you some other extensions I made to make it more useful. Using Java Object in JythonWhen you are using the PyFunction object, you may notice that its __call__ method accepts PyObject as parameter. Does that mean we need to convert our Java object to PyObject so that our Python code can use it? The answer is no, Jython has already implemented such function. If what you provide is a “standard” Java object, Jython will “mock” it into the corresponding Python primitive types: All Java primitive types can used as the corresponding Python primitive types java.util.Map instances can be used as normal Python dict java.util.List instances can be used as normal Python list Take my code as an example, we use JSONObject from FastJSON in our project, which implements java.util.Map, so in my Python code, I can access its members in the way I would do to a normal Python dict: 1234def __call__(json): if not json['test']: json['test'] = True return True DO note that Jython won’t change the type of your object: if you try to do instanceof comparison in your Python code, it would return False; if you do is False or is True to a Java bool value, it would return False. Jython just mimic the behavior of standard Python entities for these Java objects (duck typing), but essentially they are not the same type. Importing PIP PackagesTo further reduce the amount of Python code we need to write, we might want to maintain some common Python code in a package and reuse it by importing it. Jython also supports importing external Python source file, but first we will need to set the sys.path accordingly. By default, Jython (or Python) will automatically include the current working directory in the sys.path, so if we want to use some self-made Python file, we can just add it to the current working directory and import it. But if we want to use those packages installed by PIP, we will need some extra configuration. The key is to add the local PIP install directory to the sys.path of Jython. You can do this in many ways, but the safest solution is to just ask the local Python interpreter: 1234567891011121314151617181920212223public class PythonRunner { private static final PythonInterpreter intr = new PythonInterpreter(); static { intr.exec(&quot;import sys&quot;); try { Process p = Runtime.getRuntime().exec(new String[]{ &quot;python2&quot;, &quot;-c&quot;, &quot;import json; import sys; print json.dumps(sys.path)&quot;}); p.waitFor(); String stdout = IOUtils.toString(p.getInputStream()); JSONArray syspathRaw = JSONArray.parseArray(stdout); for (int i = 0; i &lt; syspathRaw.size(); i++) { String path = syspathRaw.getString(i); if (path.contains(&quot;site-packages&quot;) || path.contains(&quot;dist-packages&quot;)) inter.exec(String.format(&quot;sys.path.insert(0, '%s')&quot;, path)); } } catch (Exception ex) {} } // ...} As I mentioned earlier at the beginning of this post, not all PyPI packages can run in Jython, especially those contain C code. Try to import the package you want to use in Jython shell and test its fuctions before going further. ConclusionThis post is mainly an epilogue to the work I’ve been doing in the past few days. I would not consider Jython as an 100% safe solution: the truth is, you may run into some very subtle bug when you use it, and its API and documentations is nothing close to “friendly”. But if you want to achieve something like mine, Jython definitely worth a look.","link":"/embedding-jython/"},{"title":"有效的 Git 分支模式","text":"本文译自 Vincent Driessen 的《A Successful Git Branch Model》，转载时请注明原文链接。 在这篇文章中我将为大家介绍我这一年以来在工作项目和个人项目上都有在使用的开发模式，而事实也证明该模式十分有效可行。实际上，我很久以前就打算要写这篇文章了，但直到现在我才终于抽出时间来把它写完。在这篇文章中，我不会讲述任何有关具体项目的细节，但我会详细地讲述我所使用的分支策略和版本发布管理。 这一切都要从我们以 Git 作为项目的版本管理工具开始说起。 为什么要用 Git ？如果你想了解 Git 对比于其他集中式源代码管理系统有怎么样的优势和劣势的话，你在网上稍微浏览一下便能找到答案。这么久以来，两党之间可谓是战火不断。作为一个开发者，我本人更倾向于 Git。我认为 Git 极大地改变了开发者对分支与合并的认知。在我的印象里，用 CVS 或是 SVN 进行分支或合并着实是有些吓人（当心合并冲突，他们会咬你的 :D），而且这样的操作也很少会进行。 但对于使用 Git 的人来说，这样的操作却是相当的方便快捷，这样的操作也早已成为他们的日常工作流程中的一部分。举个例子，在 CVS/Subversion 的相关书籍中，分支与合并一直到书的后半部分才作为高级功能首次出现，而大部分的 Git 书籍早在第三章之前便将其作为基本功能进行介绍。 正是由于它们的简便性，我们不再需要惧怕分支和合并。版本控制工具本来就应该更好地支持分支与合并。 好了，我们还是谈谈开发模型吧。接下来我要介绍的模型实际上不过是一些简单的操作，但若您的开发团队中的每一个开发人员都能严格遵循这些操作要求的话，您的软件开发过程将从此变得有条不紊。 分布而又集中这个分支模型会使用一个中央仓库，尽管 Git 作为一个 DVCS（译注：分布式版本控制系统），从技术层面上来讲并不存在什么“中央仓库”。接下来我会以 origin 来指代这个仓库，相信所有 Git 使用者对这个名字应该也是相当熟悉了。 开发人员会从 origin 上 pull 或者是 push 提交，但除了这种集中式的 push 和 pull 以外，开发人员还可以相互之间组成一些功能开发小分队，从他的其他小伙伴那 pull 一些代码改动过来。比如说，如果有那么几个开发者需要一起开发一个新功能，那么使用 Git 的这种分布式关系则可以有效避免过早地将改动 push 到 origin 上。在下面的示例图中，我们就看到了分别由 Alice 和 Bob、Alice 和 David 以及 Clair 和 David 组成的三个小分队。 但，从技术上来讲的话，这仅仅意味着 Alice 在自己的机器上定义了一个叫做 bob 的 Git Remote，它指向 Bob 机器上的仓库。反之亦然。 主要分支 这个开发模型的核心部分实际上更多的是受到了其他已有模型的启发。中央仓库包含着两个将永久存在的主要分支： master develop 大家应该相当熟悉 origin 上的 master 分支了。平行于 master 分支的另一条分支被称为 develop。 位于 origin/master 的 HEAD 的源代码在任何时候都应是可作为产品直接发布的代码。 位于 origin/develop 的 HEAD 的代码应包含所有最新提交的准备在接下来的版本中发布的代码改动。有些人也把它称为“集成分支”。它也是用于“每夜构建”（译注：nightly build。有的项目每次构建需要耗费大量的计算机资源，如果让项目在每天深夜的时候自动构建则能够有效节省这些资源）的分支。 当 develop 分支的代码逐渐达到一个稳定的状态，准备好发布的时候，所有的改动都应被合并到 master 分支上，并标注（tag）上发布的版本号。我们后面再讲解这个过程具体应该怎么做。 因此，每次改动被合并到 master 分支上都必须意味着一次新版本的发布。在这一点上我们必须十分严格，因为如果能做到的话，理论上来讲我们可以写一个脚本，在每次 master 分支出现新的提交时自动地将 master 分支上的代码构建并发布到生产服务器上。 其他分支除了 master 和 develop 这两个主要分支以外，我们的开发模式中还会使用很多其他不同的分支，分别用来帮助不同开发小组之间的平行开发、简化新功能开发的跟进、准备新的版本发布和进行生产系统的快速修复。和主要分支不同，这些分支的生命周期是有限的，它们最终都会被删除。 这包括如下几种分支： 功能分支（Feature Branch） 发布分支（Release Branch） 修复分支（Hotfix Branch） 这几个分支都有着自己各自的用处，同时也需要严格遵循一些规则，包括它们可以来自哪些分支，它们又能合并到哪些分支。接下来我便会对它们分别进行介绍。 功能分支 可分离自： develop 必须合并至： develop 分支命名规范： 除以下命名外均可：master、develop、release-*、hotfix-* 功能分支用于为产品未来的版本开发新的功能。在功能分支中开发新的功能时，具体会在产品的哪个版本中发布该功能多半是不可知的。功能分支的本质在于它仅仅用于开发新的功能，并最终在新功能准备发布前被合并到 develop 分支，或是在新功能前景不佳时被直接删除。 功能分支在大多数情况下不应存放在 origin 上，而应存放在开发者本地的 Repo 中。 创建功能分支当需要开发一项新功能时，从 develop 分支创建一个新的分支： 12$ git checkout -b myfeature develop# Switched to a new branch &quot;myfeature&quot; 将新功能发布到 develop 分支开发完毕的功能分支应被合并到 develop 分支以明确要将新功能在下一个版本中发布： 12345678$ git checkout develop# Switched to branch 'develop'$ git merge --no-ff myfeature# Updating ea1b82a..05e9557# (Summary of changes)$ git branch -d myfeature# Deleted branch myfeature (was 05e9557).$ git push origin develop --no-ff 标识位使得 Git 即使在能够使用 fast-forward 完成合并操作的情况下也会在 develop 分支上留下一个代表合并操作的 commit 记录。如此一来，我们便能够保留功能分支曾经存在的事实，同时也把和该功能相关的提交关联到了这个合并提交中。看下图： 在右边的例子中，--no-ff 标识位未被使用，Git 的 fast-forward 功能使得功能分支上的所有 commit 记录被直接复制到了 develop 分支上。如此一来，你只能慢慢地阅读每个提交记录的日志才能分辨出哪些提交是用来实现这个新功能的了。同时，由于添加的新功能的相关改动分布在了多个 commit 记录中，回退新添加的功能也变得十分困难。相比之下，使用 --no-ff 功能后便没有了这些问题。 没错，使用 --no-ff 合并分支确实会多产生一些空的提交记录，但换来的好处却是不容忽视的。 发布分支 可分离自： develop 必须合并至： develop 和 master 分支命名规范： release-* 发布分支用于进行产品新版本发布的准备工作。它们可以用来对新的版本修改一些小 bug，或者是准备包括版本号、构建日期之类的新版本元数据。通过在发布分支上完成这类改动能确保 develop 分支随时能够继续接收下一次版本发布的新功能。 我们只应在 develop 分支几乎快要准备好可以发布新版本时才创建发布分支，在这个时候所有应在该版本中发布的新功能都应已被合并到 develop 分支上。除外，计划在以后的版本中发布的功能必须在发布分支创建后再合并到 develop 分支上。 当且仅当在创建了发布分支以后，我们才应该将版本号赋予即将发布的版本，决不能过早。在这一刻之前，develop 分支所包含的是即将在“下一个版本”中发布的改动，但仍然不清楚所谓的“下一个版本”具体是 0.3 还是 1.0。具体的版本号只能在发布分支创建时才能够根据项目的版本号递增规则赋予给即将发布的版本。 创建发布分支发布分支应创建自 develop 分支。举个例子，当前已发布的最新的版本为 1.1.5，而且我们即将发布一个大的新版本。新版本的所有改动已经被合并到了 develop 分支上，我们也想好了将其作为 1.2 版本发布。那么，我们就从 develop 分支创建一个发布分支，并以新的版本号命名该分支： 1234567$ git checkout -b release-1.2 develop# Switched to a new branch &quot;release-1.2&quot;$ ./bump-version.sh 1.2# Files modified successfully, version bumped to 1.2.$ git commit -a -m &quot;Bumped version number to 1.2&quot;# [release-1.2 74d9424] Bumped version number to 1.2# 1 files changed, 1 insertions(+), 1 deletions(-) 在创建了新的发布分支并切换到该分支以后，我们将版本号提高到了 1.2。这里我使用了一个 bump-version.sh 脚本，它会直接修改项目中的某些文件以写入新的版本号。当然你也可以手工完成这个操作，重点在于在这里某些文件会发生变化。然后，我们提交这次改动，新的版本号便被写入到了即将发布的版本中。 发布分支并不需要这个时候就被删除，它完全可以一直保留直到新的版本被顺利地发布到生产环境中。在它存活的这段时间里，我们还可以利用它来修复新版本的一些小 bug，而不是将这些修复改动直接提交到 develop 分支上。往这个分支上添加大的新功能是决不允许的 —— 新的功能只能够合并到 develop 分支上并等待下一次的新版本发布。 结束发布分支当发布分支完全准备好发布新版本时，我们需要进行以下操作。首先，将发布分支合并到 master 分支上（master 分支上的每次提交都必然意味着一次新版本的发布）。然后，合并后在 master 上所产生的提交记录必须被正确地标注（tag）以便以后能够便捷地引用到该历史版本。最后，发布分支上的所有改动需要被重新合并到 develop 分支，以确保以后的新版本中都能包含出现在发布分支中的 bug 修复。在 Git 里，前两步应该这样做： 123456$ git checkout master# Switched to branch 'master'$ git merge --no-ff release-1.2# Merge made by recursive.# (Summary of changes)$ git tag -a 1.2 如此一来，新版本便被顺利发布，同时也被正确地标注（tag）以便以后引用。 注：你可能还会想使用 -s 或 -u &lt;key&gt; 来加密你的标注信息。 然后，我们还需要把发布分支上的改动合并到 develop 分支上： 12345$ git checkout developSwitched to branch 'develop'$ git merge --no-ff release-1.2Merge made by recursive.(Summary of changes) 现在，我们的发布工作就全部做完了，也可以顺利删除发布分支了： 12$ git branch -d release-1.2# Deleted branch release-1.2 (was ff452fe). 修复分支 可分离自： master 必须合并至： develop and master 分支命名规范： hotfix-* 修复分支和发布分支很相似，它们都是用来进行一次新版本发布的准备工作，但修复分支所对应的版本发布是属于非预先计划的。当在某个版本的生产环境发生了很严重的错误需要马上进行修复时，修复分支便会直接通过该版本号对应的标注（tag）从 master 分支上创建出来。 这么做的关键在于我们在修复分支上修复 bug 时，其他开发人员仍然能够在 develop 分支上开发新的功能。 创建修复分支修复分支创建自 master 分支。还是举个例子，我们的生产环境的版本号为 1.2，而且发生了一个很严重的 bug 需要马上修复，但目前提交到 develop 分支上的改动尚未准备好新一次发布。这时我们就可以创建一个修复分支来修复这个 bug 了： 1234567$ git checkout -b hotfix-1.2.1 master# Switched to a new branch &quot;hotfix-1.2.1&quot;$ ./bump-version.sh 1.2.1# Files modified successfully, version bumped to 1.2.1.$ git commit -a -m &quot;Bumped version number to 1.2.1&quot;# [hotfix-1.2.1 41e61bb] Bumped version number to 1.2.1# 1 files changed, 1 insertions(+), 1 deletions(-) 不要忘了在创建分支后改变版本号！ 然后，修复 bug 并提交： 123$ git commit -m &quot;Fixed severe production problem&quot;# [hotfix-1.2.1 abbe5d6] Fixed severe production problem# 5 files changed, 32 insertions(+), 17 deletions(-) 结束修复分支 修复完成后，我们需要将修复改动合并到 master 分支，但同时也要合并到 develop 分支以确保未来的版本中也包含了这些修复改动。这一步的操作和发布分支很相似。 首先，更新 master 分支并进行标注（tag） 123456$ git checkout master# Switched to branch 'master'$ git merge --no-ff hotfix-1.2.1# Merge made by recursive.# (Summary of changes)$ git tag -a 1.2.1 注：你可能还会想使用 -s 或 -u &lt;key&gt; 来加密你的标注信息。 然后，还要把改动合并到 develop 分支上： 12345$ git checkout develop# Switched to branch 'develop'$ git merge --no-ff hotfix-1.2.1# Merge made by recursive.# (Summary of changes) 这里有一点例外：如果这个时候存在一个发布分支，修复分支的改动应被合并到该发布分支而不是 develop 分支。如此一来，发布分支完成并被合并到 develop 分支上时，这些修复改动也会被合并到 develop 分支上。如果 develop 分支需要马上添加这些修复改动而又等不及发布分支完成时，你也可以直接把修复分支合并到 develop 分支。 最终，我们删除该分支： 12$ git branch -d hotfix-1.2.1# Deleted branch hotfix-1.2.1 (was abbe5d6). 总结尽管这个分支模式中并没有出现什么特别创新的东西，但文章开头出现的那幅总览图对我们的项目开发确实带来了莫大的好处，我们的开发人员也因此能够快速地在分支和发布操作上达成共识。 如果你需要的话，这里有那张图的高清无码 PDF 版。把它打印下来并挂到你公司的墙上吧！ Update: And for anyone who requested it: here ’ s thegitflow-model.src.key of the main diagram image (Apple Keynote).","link":"/git-branching-model/"},{"title":"吃了兴奋剂的 Go Channel","text":"为什么？Channel 是 Go 语言的主要同步和通信原语，它们必须速度快且可扩展。 目标： 令单线程（无竞争）的 Channel 操作更快 令有竞争带缓存（生产者消费者）的 Channel 操作更快 令无阻塞失败操作（如检查 Channel 是否已关闭）更快 令信号量 Channel（chan struct{}）更快 令 select 语句更快 非目标： 令 Channel 完全无锁（这会导致实现的复杂度大幅提升且在普通使用场景下变得泵满） 令有竞争的同步 Channel 操作更快。 本文接下来的内容会详细介绍这个设计的细节。 怎么做？Channel 的类型Go 语言有 3 种不同类型的 Channel： 同步 Channel。它们不需要任何缓冲以及缓冲控制代码。而且它们实现了直接传递的语义（一个 Goroutine 会直接选择它的接收方并与其完成通信） 异步 Channel。这实际上就是基于环状缓冲的传统生产者消费者队列。它们没有实现传递语义：一个被解锁的消费者会和其他消费者一起竞争，如果它没能胜利就会重新被阻塞 带 0 体积元素的异步 Channel（chan struct{}）。这实际上就是信号量。它们不需要缓冲区（只占用 O(1) 的内存），也不实现传递语义 同步发送/接收在我们深入到 select 之前，我们先来想想一般的发送/接收是怎么工作的。 同步 Channel 在多数时候都是由互斥锁保护的，除非是在执行无须阻塞的快速失败代码路径（如从一个空的 Channel 中进行无阻塞接收）。同步 Channal 包含如下信息： 123456struct Hchan { Lock; bool closed; SudoG* sendq; // waiting senders SudoG* recvq; // waiting receivers}; 发送操作会占用互斥锁，并检查它是否需要阻塞或满足一个反向操作： 123456789101112131415161718192021222324252627282930313233bool syncchansend(Hchan *c, T val, bool block) { if(c-&gt;closed) // 快速失败路径 panic(&quot;closed&quot;); if(!block &amp;&amp; c-&gt;recvq == nil) // 快速失败路径 return false; lock(c); if(c-&gt;closed) { unlock(c); panic(&quot;closed&quot;); } if(sg = removewaiter(&amp;c-&gt;recvq)) { // 找到一个正在阻塞的接收方，与之通信 unlock(c); sg-&gt;val = val; sg-&gt;completed = true; unblock(sg-&gt;g); return true; } if(!block) { unlock(c); return false; } // 阻塞并等待接收方 sg-&gt;g = g; sg-&gt;val = val; addwaiter(&amp;c-&gt;sendq, sg); unlock(c); block(); if(!sg-&gt;completed) panic(&quot;closed&quot;); // 被 close 操作解锁 // 由一个接收方解锁 return true;} 异步发送/接收异步发送/接收在不需要操作等待队列时是无锁的，而等待队列由一个互斥锁保护。非阻塞失败操作同样是短路的。 我们首先来看看非阻塞操作时怎么进行的。 一个异步 Channel 包含以下数据： 1234567891011121314151617struct Hchan { uint32 cap; // Channel 容量 Elem* buf; // 大小为 cap 的环状缓冲 // 发送和接收位置 // 低 32 位代表在 buf 中的位置 // 高 32 位代表环状缓冲的当前圈数 uint64 sendx; uint64 recvx;};struct Elem { // 当前圈数 // 当前圈数为 0, 2, 4, ... 时，元素可读 // 当前圈数为 1, 3, 5, ... 时，元素可写 uint32 lap; T val; // 用户数据}; 发送操作通过使用 CAS 递增 sendx 来实现同步，成功递增 sendx 的 Goroutine 得以写入元素。发送与接收间的同步通过元素的 lap 变量实现，基本而言，lap 值表示该元素在当前圈数（sendx/recvx 的高 32 位）是否可读/可写。 如下即为发送操作： 1234567891011121314151617181920212223242526272829303132333435bool asyncchansend_nonblock(Hchan* c, T val) { uint32 pos, lap, elap; uint64 x, newx; Elem *e; for(;;) { x = atomicload64(&amp;c-&gt;sendx); pos = (uint32)x; lap = (uint32)(x &gt;&gt; 32); e = &amp;c-&gt;buf[pos]; elap = atomicload32(&amp;e-&gt;lap); if(lap == elap) { // 该元素已可在该圈可写 // 尝试获得写入该元素的权利 if(pos + 1 &lt; c-&gt;cap) // 获取下一个 pos newx = x + 1; // 直接加 else newx = (uint64)(lap + 2) &lt;&lt; 32; // 下一圈 if(!cas64(&amp;c-&gt;sendx, x, newx)) continue; // 输掉了，重试 // 获得了元素的所有权，可以非原子地写入 e-&gt;val = val; // 使元素可读 atomicstore32(&amp;e-&gt;lap, elap + 1); return true; } else if((int32)(lap - elap) &gt; 0) { // 该元素还未被上一圈读出， // Channel 满 return false; } else { // 该元素已在该圈上被写入， // 这意味着 c-&gt;sendx 也已经改变了 // 重试 } }} 接收操作则是完全对称的，除了 recvs 由第 1 圈开始而且是读元素而不是写元素。 现在我们来看看阻塞操作是怎么实现的。Channel 结构体还包含一个互斥锁和发送方/接收方等待队列： 123456struct Hchan { // ... Lock; SudoG* sendq; SudoG* recvq;}; 要实现阻塞发送，一个 Goroutine 首先尝试进行非阻塞发送。如果它成功了，那么它会查看是否有接收方等待，如果有的话就解锁其中一个接收方。 如果非阻塞发送失败了（Channel 已满），它会锁定互斥锁，将自己添加到发送方等待队列，然后重新检查 Channel 是否仍满。若果 Channel 仍满，那么 Goroutine 阻塞；否则，Goroutine 将自己从等待队列中移除，解锁互斥锁并重试。 阻塞接收的过程完全一致，除了 s/send/recv/ 、 s/recv/send/（笑）。 要实现这样一个阻塞算法最巧妙的地方在于确保不会发生死锁（一个发送方被无限期地阻塞在一个未满 Channel 上，或者一个接收方被无限期地阻塞在一个非空 Channel 上）。通过这样检查、保存、再检查，我们确保（1）发送方看到一个接收方等待者并解锁它，或（2）接收方看到缓冲中的元素并消费它，或（3）情形 1 和 2 同时存在（在这种情况下我们通过使用互斥锁来解决竞争）；但不会发生（4）发送方看不到接收方等待者或接收方但不到缓冲里的元素并无限期阻塞。 以下是阻塞发送的算法： 12345678910111213141516171819202122232425262728void asyncchansend(Hchan* c, T val) { for(;;) { if(asyncchansend_nonblock(c, val)) { // 发送成功，看看我们要不要解锁一个接收方 if(c-&gt;recvq != nil) { lock(c); sg = removewaiter(&amp;c-&gt;recvq); unlock(c); if(sg != nil) unblock(sg-&gt;g); } return; } else { // 队列已满 lock(c); sg-&gt;g = g; addwaiter(&amp;c-&gt;sendq, sg); if(notfull(c)) { removewaiter(&amp;c-&gt;sendq, sg); unlock(c); continue; } unlock(c); block(); // 重新尝试发送 } }} struct{} 发送/接收0 体积异步 Channel 大体上与非 0 体积异步 Channel 相同： 在非阻塞情形下，操作是无锁的 等待队列仍由互斥锁保护 非阻塞失败操作是短路的 区别在于 Hchan 只包含一个计数器而不是发送/接收位置和环状缓冲，该计数器代表 Channel 中的元素数量 非阻塞发送/接受会使用 CAS 循环来更新计数器 满/空判断只需要检查计数器的值 其他的部分，包括阻塞算法，则是一样的。 close关闭操作会锁定互斥锁，将设置 closed 标志位并解锁所有等待者。异步发送/接收操作在阻塞前会检查 closed 标志位。 这实现了与异步发送/接收阻塞相同的保证，即（1）关闭操作看到一个等待者，或（2）一个等待者看到 closed 标志位被设置，或（3）情形 1 与 2 同时发生（此时通过互斥锁来避免竞争） select现在我们可以来学习 select 了。 Select 操作不会立刻锁定所有相关 Channel 的互斥锁，而是会对每个 Channel 进行细粒度的操作。 Select 包含 4 个阶段： 对所有相关的 Channel 进行乱序以提供一个伪随机次序保证（接下来的每一步都会在这个乱序 Channel 列表的基础上工作） 一个一个地检查每个 Channel，看看它们之中是否有人已经准备好通信了，如果是的话就进行通信并退出。这使得 select 语句不需要更早地阻塞且有更好的可扩展性，因为它们不需要排序并锁定这些互斥锁。除此之外，这样的 select 如果发现第一个 Channel 已经准备好的话甚至不需要接触所有的 Channel 准备阻塞所有 Channel 阻塞。返回第 1 步 对于第 2 步我们需要再解释一下。 本质上来讲，它的工作原理和异步发送/接受的阻塞操作时相同的。也就是，锁定 Channel 的互斥锁，将 Goroutine 放入到发送方/接收方等待队列，然后重新检查 Channel 是否已经准备好通信。如果 Channel 还没有准备好，那就继续下一个 Channel；否则，就将自己从等待队列中移除并回到第 1 步。 还有另一个有趣的地方，select 让我们成为了多个 Channel 的等待者，但我们不希望多个同步 Channel 操作都能利用该 select 完成通信（对于同步 Channel 来说，解锁即完成通信）。为了避免这样的情况发生，由 select 放入到等待队列中的实体包含一个指向 select 全局状态字的指针。在解锁这样的一个等待者时，其他 Goroutine 会首先尝试 CAS（statep 、 nil 、 sg），以获得解锁等待者或与等待者通信的权利。如果 CAS 失败了，Goroutine 会无视这个等待者（它已经被其他人唤醒了）。 这个算法要求所有类型的 Channel 实现 isready(c) 函数，但这不是什么很大的问题。一次 select 操作的高层算法如下： 123456789101112131415161718192021222324252627282930313233343536Scase *select(Select *sel) { randomize channel order; for(;;) { // 第 1 步 foreach(Scase *cas in sel) { if(chansend/recv_nonblock(cas-&gt;c, ...)) return cas; } // 第 2 步 selectstate = nil; foreach(Scase *cas in sel) { lock(cas-&gt;c); cas-&gt;sg-&gt;g = g; cas-&gt;sg-&gt;selectstatep = &amp;selectstate; addwaiter(&amp;cas-&gt;c-&gt;sendq/recvq, cas-&gt;sg); if(isready(cas-&gt;c)) { unlock(c); goto ready; } unlock(cas-&gt;c); } // 第 3 步 block();ready: CAS(&amp;selectstate, nil, 1); foreach(Scase *cas in sel) { lock(cas-&gt;c); removewaiter(&amp;cas-&gt;c-&gt;sendq/recvq, cas-&gt;sg); unlock(cas-&gt;c); } // 如果我们是被同步 Channel 操作解锁的， // 那么通信已经完成 if(selectstate &gt; 1) return selectstate; // 代表完成了的 case }}","link":"/go_channels_on_steroid/"},{"title":"Go Runtime 浅析","text":"在 GDC Sigma 小组为期两个月的实习已告一段落，本人也十分有幸能在实习期间在团队内部完成了三次 Go 语言相关的分享。由于个人的不足，很遗憾没能向组员分享更多深入的内容，但尽管内容粗浅，这三次分享仍在组内起到了很好的科普作用，收获了组员的一致好评。受组员委托，我将把这三次分享上与 Go 运行时有关的内容整理成文，希望更多的人能从中受益。 本文将完整描述本人在三次组内 Go 分享中提及的与 Go 运行时有关的内容。分享初期关于 Go 语言入门的内容将不在此处赘述。 Go 运行时概览阅读 Go 运行时的代码并不如大多数人想象中的那么可怕。实际上，作为与 C/C++ 同级的语言，Go 在很久以前便完成了自举（自己写自己的编译器）的过程，因此当你打开 Go 的 Github Repository 时你会发现，Go 运行时绝大部分代码由 Go 编写，少部分由汇编语言编写，C/C++ 完全没有。 Go 运行时的核心代码就位于该 Repository 的 src/runtime 目录中，包含如下几个主要模块： 内存分配（malloc.go） 垃圾回收（mgc.go） Goroutine 调度（proc.go） Go 复合类型的实现（slice.go、hashmap.go、chan.go、error.go） 总体而言，阅读这些代码时应当不会在语言方面遇到太多的阻力，源文件中也有大量的注释解释每个模块所使用的算法，本文的大多数内容也源自这些注释。但是完全理解这些代码需要读者十分了解操作系统底层的系统调用和优化方法 – 如果你已经能够熟练使用 C/C++ 开发高性能程序，这些内容对你来说应该不难理解。 接下来我将分模块简单介绍 Go 运行时各个主要模块所采用的算法。 Goroutine 调度熟悉 Go 并发编程的人都了解，Go 以 Goroutine 作为执行调度单位，有别于 Python 或 Java 直接使用操作系统线程作为调度单位。在 Goroutine 变长调用栈及用户级调度的共同作用下，一个 Go 程序可以轻易地同时拥有成千上万个 Goroutine 且确保 Goroutine 间的切换不会导致程序的性能下降。本节将以 Morsing 的《The Go scheduler》一文为基础，简单讲述 Go 语言调度器实现用户级 Goroutine 调度的方式。 首先，Go 调度器主要利用如下 3 种实体进行调度工作： 三角形为 M（Machine），代表一个操作系统线程 正方形为 P（Processor），代表一个操作系统线程要运行 Go 代码时必须用到的资源 圆形为 G（Goroutine），代表一个 Goroutine 众所周知，Go 语言实现的是 M:N 调度，即将 M 个 Goroutine 分配到 N 个操作系统线程上运行，有别于 NodeJS 的 N:1 调度或 Java 的 1:1 调度。在稳定的情况下，M、P、G 间的关系如下： M 持有一个 P，并利用其中的资源运行 Goroutine；任何时刻，一个 M 上都至多有一个 G 在运行，其他 G 将被放置在 P 的本地队列中等待执行。由正在运行的 G 创建的新 Goroutine 通常会被直接放入对应的本地队列中。 由于 Go 实现的是抢占式的调度，P 在运行某个 G 一段时间后就会将其暂停，放入到本地队列中，并从本地队列中获取另一个 G 执行。 如上描述的是 Goroutine 稳定运行时的状态，有两种情况可以打破这种稳定状态：系统调用及本地队列耗尽。 首先是系统调用： 当正在运行的 G 进行系统调用后，运行该 G 的操作系统线程（M）将进入阻塞态，等待系统调用完成。此时该 M 由于处于阻塞状态，不再运行 Go 代码，因此 Go 调度器会剥夺该 M 所持有的 P 并分配给另一个 M，由新的 M 负责继续运行其他 Goroutine。 当处于阻塞状态的 M 和 G 完成系统调用后，该 G 会被放入到全局等待队列中，M 也会被放入到线程池中，等待调度器再次使用。每个 P 在对正在运行的 G 进行抢占时会不时地检查全局等待队列中是否有已可运行的 G，以避免处于全局等待队列中的 G 发生饥饿现象。 当某个 P 耗尽自己的本地队列后，它会从其他 P 的本地队列中窃取大约一半的 G，以实现负载均衡： Go 内存分配Go 运行时的内存分配算法主要源自 Google 为 C 语言开发的 TCMalloc 算法，全称 Thread-Caching Malloc。该算法的特色在于其将可用的堆内存采用二级分配的形式进行管理：每个线程都会自行维护一个独立的内存池，进行内存分配时优先从该内存池中分配，当内存池不足时才向全局内存池申请，以避免不同线程对全局内存池的频繁竞争。除此以外，该算法会对小对象和大对象采用不同的内存分配过程。 Go 运行时的内存分配算法在很大程度上与该算法保持一致。首先，Go 在为小对象（大小小于 32 KB）分配内存时会对对象的实际大小向上取整，将对象分类到大约 70 个不同大小的 Size Class 中，并按照 Size Class 的大小为对象分配空间。每个 Size Class 的具体数值系考虑各项约束后自动生成，最小的 Size Class 为 8B，最大为 32KB。详见 mksizeclasses.go 和 sizeclasses.go。 在明确这一概念后，我们便可以开始了解 Go 内存分配算法主要使用的数据结构了： mheap：代表 Go 程序所持有的所有堆空间，可视为由若干个大小为 8 KB 的内存页组成的数组 mspan：一个 mspan 从属于某个指定的 Size Class，在 mheap 上占据若干个连续的内存页，其内部根据所属 Size Class 的大小被平均划分为若干个 object。每个 mspan 会使用一个 bitmap 来标记其内部尚可用的 object mcache：Goroutine 本地缓存的可用 mspan，是上一节所提到的 P 的一部分 mcentral：全局可用的 mspan 列表。Goroutine 在需要时会从 mcentral 获取 mspan 如此一来，Go 运行时进行内存分配的过程就十分清晰了。当 Go 需要为小对象分配对象时，小对象会被向上取整至最近的 Size Class，并执行如下步骤： 从当前 P 的 mcache 中获取属于该 Class 且仍有空闲位置的 mspan 若 mcache 已空，则从 mcentral 获取一整个 mspan 到当前 P 的 mcache 中 若 mcentral 已空，则从 mheap 中获取若干个连续内存页，构建新的 mspan 并放入到 mcentral 中 若 mheap 已空，则从操作系统申请若干个内存页到 mheap 中 对于大对象而言，Go 则会跳过 mcache 和 mcentral，直接在 mheap 上构建一个合适大小的 mspan 进行分配 Go 垃圾回收在了解了 Go 如何为对象分配内存后，我们便可以开始学习 Go 是如何进行垃圾回收的了。 当前 Go 的最新版本为 1.8.3，Go 采用的是并发、三色的标记 - 清除垃圾收集器。这个垃圾收集器在 Go 1.5 版的时候引入，并在当时将 Go 的 GC Pause 时间缩短到了 1.4 版的几百分之一。尽管做出了不少的修改，Go 的垃圾收集算法参考了 Dijkstra 在 1978 年写的论文：《On-the-Fly Garbage Collection: An Exercise in Cooperation》。 标记 - 清除算法可以说是最经典的垃圾回收算法。该算法的回收过程分为两个步骤： 标记：从 GC Root 对象开始，沿着对象中包含的所有指针递归地标记所有可达的对象。GC Root 对象包括所有在标记前便确定可达的对象，如全局变量、位于栈帧中的本地变量等 清除：在标记阶段结束后，未被标记的对象意味着不可达。清除阶段将清除所有未被标记的对象，释放它们所占用的内存。 标记 - 清除算法作为最经典也是最基础的算法存在着它的不足，最主要的不足在于它在清除阶段会对未被标记的对象原地进行释放，被释放对象所留下的空隙便形成了内存碎片，而内存碎片的存在会导致程序的内存空间利用率下降。 实际上，Go 所谓的并发、三色的标记 - 清除垃圾收集算法并不新鲜，JVM 和 V8 中都有类似的收集算法。在 JVM 中，该收集器被称为 CMS 收集器（Concurrent Mark-Sweep）。JVM 的 CMS 收集器执行过程与 Go 的收集器类似，也有着和 Go 的收集器相似的特性：以降低程序计算吞吐量为代价，减少 GC Pause 的时间。 Go 垃圾收集器的一次收集过程可归纳为如下几个步骤： _GcOff：两次 GC 间，Go 程序将处于 _GcOff 状态。GC 发生的过程中会把所有处于 mcache 中的 mspan 放回 mcentral，以让 Goroutine 申请内存时需要重新从 mcentral 获取 mspan。Goroutine 获取 mspan 时会 lazy 地清除 mspan 中在上一次 GC 中未被标记的对象。除此以外，另一个 GC Bg Worker Goroutine 也会主动地清扫未被清扫地 mspan； 清除终止：开始 GC 前的准备工作。此时程序会 Stop the world，并清扫所有仍未被清扫的 mspan。通常 GC 会在程序的内存占用达到一定阈值时被触发，通常此时应当已经不存在仍未被清扫的 mspan。若此次 GC 是由 runtime.GC() 等方式手动触发的则情况可能有所不同； _GcMark：标记阶段。此时 Go 收集器会利用之前开启的 Stop the world，为所有用户 Goroutine 启动写屏障（Write Barrier）。然后，Go 收集器会把 GC Root 对象的标记工作放入到标记作业队列（置为灰色）。之后 Go 收集器便会恢复用户 Goroutine 的执行。开启了写屏障的 Goroutine 在每次修改指针变量的值时会使得新旧指针指向的对象均被置为灰色，而新创建的对象这会直接被置为黑色（已标记）。除此以外，位于后台运行的 Mark Worker Goroutine 会开始从标记作业队列中获取颜色为灰色的对象，对其进行标记（置为黑色），并将其指向的其他结点置为灰色（放入标记作业队列），直到作业队列被耗尽； _GcMarkTermination：标记阶段的收尾工作。Stop the world，并完成队列中剩余的标记作业。通常此时队列已为空。完成标记作业后将继续完成其他 GC 收尾工作，如将 Goroutine mcache 中的 mspan 放回到 mcentral； _GcOff：GC 结束，恢复用户 Goroutine 的执行，由用户 Goroutine 和 GC Worker Goroutine 对 mspan 中未被标记的对象进行回收 下图显示了 Go 垃圾回收的大致过程： 比较无奈的是，Go 现在所采用的垃圾回收算法存在着一定的不足。原因主要在于 Go 没有对堆中的对象进行分代，每次 GC 发生时都需要对堆中的所有对象进行标记工作，因此标记的工作量将与堆的大小呈线性相关。Go 的垃圾回收算法的目标是降低 GC Pause 时间，但需要做的工作并不会因为 GC Pause 变短而消失不见，因此在这种情况下，Go 的 GC Bg Mark Worker 可能会占用不少的计算资源来完成全堆的标记工作。详见这个由 Uber 给出的 Go GC 展示。 结语以上便是我在组内进行 Go 分享时介绍的有关 Go 运行时的全部内容。未来如果有机会能够继续深入学习这一块的内容，我也会把最新的收获发布在博客中，敬请期待。","link":"/go_runtime_intro/"},{"title":"Groovy 模块 - JSON 解析与生成","text":"这是一篇译文，读者可前往 Groovy Module Guide - Parsing and producing JSON 阅读原文。 Groovy 内置了从 Groovy 对象到 JSON 之间相互转换的功能类，而这些被用于进行 JSON 序列化或解析的类都被放在了 groovy.json 包中。 1 JsonSlurper JsonSlurer 类主要用于解析 JSON 文本并转换成 Groovy 数据结构（对象），如映射、列表或 Integer、Double、Boolean、String 等基本数据类型。 该类包含了大量不同版本的 parse 方法以及如 parseText、parseFile 的特殊方法。在下一个例子中我们将使用 parseText 方法，该方法会对给定的 JSON String 进行解析并递归地将其转换为列表或映射。其他的 parse* 方法也是类似，同样会返回解析后的 Groovy 对象，只是它们接受不同类型的参数： 12345def jsonSlurper = new JsonSlurper()def object = jsonSlurper.parseText('{ &quot;name&quot;: &quot;John Doe&quot; } /* some comment */')assert object instanceof Mapassert object.name == 'John Doe' 值得注意的是，解析的结果是一个普通的映射而且可以被当做普通的 Groovy 实例那样处理。JsonSlurper 支持对由 ECMA-404 JSON 交换标准定义的 JSON 格式进行解析，同时还对 JavaScript 注释和日期格式提供了额外的支持。 除了映射，JsonSlurper 同样支持 JSON 数组并将其转换为列表。 123456def jsonSlurper = new JsonSlurper()def object = jsonSlurper.parseText('{ &quot;myList&quot;: [4, 8, 15, 16, 23, 42] }')assert object instanceof Mapassert object.myList instanceof Listassert object.myList == [4, 8, 15, 16, 23, 42] JSON 标准包含对如下几种基础类型的支持：字符串、数字、对象、true、false 和 null。JsonSlurper 会将这些 JSON 类型转换成对应的 Groovy 类型。 1234567891011def jsonSlurper = new JsonSlurper()def object = jsonSlurper.parseText ''' { &quot;simple&quot;: 123, &quot;fraction&quot;: 123.66, &quot;exponential&quot;: 123e12 }'''assert object instanceof Mapassert object.simple.class == Integerassert object.fraction.class == BigDecimalassert object.exponential.class == BigDecimal 鉴于 JsonSlurper 能够返回完全转换后的 Groovy 对象而无需显式借助其他的特殊 JSON 类，我们可以说 JsonSlurper 的使用体验是透明的。事实上，JsonSlurper 的结果遵从 GPath 表达式。GPath 是一门十分强大的表达式语言，包括 JsonSlurper 在内的其他不同数据格式的 Slurper 均支持该语言（如用于 XML 的 XmlSlurper）。 有关 GPath 的更多内容请查阅这里。 下表给出了 JSON 类型与其对应的 Groovy 数据类型： JSON Groovy 字符串 java.lang.String 数字 java.lang.BigDecimal 或 java.lang.Integer 对象 java.util.LinkedHashMap 数组 java.util.ArrayList true true false false null null 日期 基于 yyyy-MM-dd'T'HH:mm:ssZ 日期格式的 java.util.Date 当 JSON 中出现 null 值时，JsonSlurper 则会在转换结果中的对应位置放入一个 Groovy null 值。这一点和部分用特定的单例对象表示 JSON null 值的 JSON 解析库有所不同。 1.1 解析器变体 JsonSlurper 自带了各种不同的解析器实现。每种解析器有着不同的特性，考虑到 JsonSlurper 默认的解析器可能不是在任何情况下都是最好的。如下是对各种不同解析器实现的简单介绍： JsonParserCharArray 解析器接受传入的 JSON 字符串并直接对其底层的字符数组进行操作。在进行值转换时它会复制字符子数组（利用一种叫做“斩断”的机制）并对其进行操作。 JsonFastParser 是 JsonParserCharArray 的变体，也是最快的解析器，但它未被用作默认的解析器是有原因的。JsonFastParser 实际上是所谓的索引覆盖解析器。在对给定的 JSON String 进行解析时，它会尽可能地不去创建新的 String 或字符数组实例。它只会维持一些指向底层字符数组的指针。除此之外，它也会尽可能地推迟对象的创建。如果解析出来的结果映射会被长时间用作缓存，那么你就需要意识到映射中的对象很可能还未被创建，映射本身只包含了对原本的字符缓冲的指针。不过，JsonFastParser 还包含一种特殊的斩断模式，在这个模式中它会很早就对原本的字符缓冲进行切分并维持对其的一小部分拷贝。如此，我们更推荐你对小于 2MB 的 JSON 缓冲使用 JsonFastParser 并记住其在长时间缓存方面的限制。 JsonParserLax 是 JsonParserCharArray 的特殊变体。它的性能特征和 JsonFastParser 十分相似，但它不止依赖于 ECMA-404 JSON 语法。例如，它还能处理注释和不带引号的字符串等。 JsonParserUsingCharacterSource 是一种用于大文件的特殊解析器。它使用了一种叫做“字符窗口”的技术来解析较大的 JSON 文件（所谓“较大”即指大小在 2MB 以上的文件）并使得其性能特征保持恒定不变。 JsonSlurper 默认的解析器实现是 JsonParserCharArray。JsonParserType 枚举中包含了对应上述所有实现的常量： 实现 常量 JsonParserCharArray JsonParserType#CHAR_BUFFER JsonFastParser JsonParserType#INDEX_OVERLAY JsonParserLax JsonParserType#LAX JsonParserUsingCharacterSource JsonParserType#CHARACTER_SOURCE 改变解析器实现只需要调用 JsonSlurper#setType() 改变 JsonParserType 值即可： 123456def jsonSlurper = new JsonSlurper(type: JsonParserType.INDEX_OVERLAY)def object = jsonSlurper.parseText('{ &quot;myList&quot;: [4, 8, 15, 16, 23, 42] }')assert object instanceof Mapassert object.myList instanceof Listassert object.myList == [4, 8, 15, 16, 23, 42] 2 JsonOutput JsonOutput 用于将 Groovy 对象序列化为 JSON 字符串。它可以被视为 JSON 解析器 JsonSlurper 的伴生对象。 JsonOutput 包括几种不同版本的静态 toJson 方法，每种方法都接受不同的参数类型。这些静态方法可以被直接使用，也可以使用静态引入语句进行引入。 调用 toJson 方法的结果为包含结果 JSON 代码的 String。 123def json = JsonOutput.toJson([name: 'John Doe', age: 42])assert json == '{&quot;name&quot;:&quot;John Doe&quot;,&quot;age&quot;:42}' JsonOutput 并不支持基本数据类型，还支持映射和列表，甚至还能对 POGO 进行序列化，也就是普通的 Groovy 对象。 12345class Person { String name }def json = JsonOutput.toJson([ new Person(name: 'John'), new Person(name: 'Max') ])assert json == '[{&quot;name&quot;:&quot;John&quot;},{&quot;name&quot;:&quot;Max&quot;}]' 在上一个例子中我们看到，输出的 JSON 字符串默认是没有任何换行或缩进之类的格式符号的。通过调用 JsonOutput 的 prettyPrint 方法即可完成此任务： 123456789def json = JsonOutput.toJson([name: 'John Doe', age: 42])assert json == '{&quot;name&quot;:&quot;John Doe&quot;,&quot;age&quot;:42}'assert JsonOutput.prettyPrint(json) == '''\\{ &quot;name&quot;: &quot;John Doe&quot;, &quot;age&quot;: 42}'''.stripIndent() prettyPrint 方法只接受一个 String 作为参数，因此它可用于任意的 JSON String 而不局限于 JsonOutput.toJson 的结果。 除此之外，在 Groovy 中创建 JSON 还可以使用 JsonBuilder 或 StreamingJsonBuilder。两种 Builder 都提供了各自的 DSL 用于构建对象图并最后将其转换为 JSON。 有关这些 Builder 的详细信息请查阅 JsonBuilder 和 StreamingJsonBuilder。","link":"/groovy-module-json/"},{"title":"Java String Formatting","text":"Printing or producing a simple String message has been really trivial in Java. Most of the time, we use concatenation to create String instance we want: 12int a = 3;String str = &quot;Integer `a` has value `&quot; + a + &quot;`&quot;; Though modern Java compiler uses StringBuilder to optimize statements like these, using concatention to construct String has its limitations, which include: The pattern of the String is not reusable; The statements can be unacceptably long if we try to construct a complicated message; It is impossible to designate the precision of a floating point number. Fortunately, Java SE 5.0 brought back the venerable printf method from the C library, which also come with the basic feature of string formatting. Elements of Formatting Patternsprintf method in Java is much like its counterpart in C. For example, the call 1System.out.pinrlnt(&quot;%8.2f&quot;, x); prints x with a field width of 8 characters and a precision of 2 characters. You can supply multiple parameters to printf. For example: 1System.out.printf(&quot;Hello, %s. Next year, you'll be %d.&quot;, name, age); The formatting patterns is written in a String constant, where format specifiers that start with a % character is replaced with the corresponding argument. A format specifier is composed of flag and/or conversion character. Flags are used to control the apperance of the formatted string, while the conversion character that ends a format specifier indicates the type of the value to be formatted: f is a floating-point number, s a string, and d a decimal integer. Conversion Character Type Example d Decimal integer 159 x Hexadecimal integer 9f o Octal integer 237 f Fixed-point floating-point 15.9 e Exponential floating-point 1.59e+01 g Genral floating-point (the shorter of e and f) a Hexadecimal floating-point 0x1.fccdp3 s String Hello c Character H b boolean true h Hash code 42628b2 % The percent symbol n The platform-dependent line seperator | Flag | Purpose | Example || + | Prints sign for positive and negative numbers. | +3333.33 || space | Adds a space before positive numbers. | | 3333.33| || 0 | Adds leading zeroes. | 003333.33 || - | Left-justifies field. | |3333.33 | || ( | Encloses negative numbers in parentheses. | (3333.33) || , | Adds group separators. | 3,333.33 || # (for f format) | Always includes a decimal point. | 3,333. || # (for x or o format) | Adds 0x or 0 prefix. | 0xcafe || $ | Specifies the index of the argument to be formatted; for example,%1$d %1$x prints the first argument in deximal and hexadecimal. | 159 9F || &lt; | Formats the same value as the previous secification; for example,%d %&lt;x prints the same number in decimal and hexadecimal. | 159 9F | You can use the static String.format method to create a formatted string without printing it: 1String message = String.format(&quot;Hello, %s. Next year, you'll be %d.&quot;, name, age); Conversion characters for Date formatting are also available, which start with t: 12System.out.printf(&quot;%tc&quot;, new Date());// Mon Fex 09 18:05:19 PST 2004 Conversion Character Type Example c Complete date and time Mon Feb 09 18:05:19 PST 2004 F ISO 8601 date 2004-02-09 D U.S. formatted date (month/day/year) 02/09/2004 T 24-hour time 18:05:19 r 12-hour time 06:05:19 pm R 24-hour time, no seconds 18:05 Y Four-digit year (with leading zeroes) 2004 y Last two digits of the year (with leading zeroes) 04 C First two digits of the year (with leading zeroes) 20 B Full month name February b or h Abbreviated month name Feb m Two-digit month (with leading zeroes) 02 d Two-digit day (with leading zeroes) 09 e Two-digit day (without leading zeroes) 9 A Full weekday name Monday a Abbreviated weekday name Mon j Three-digit day of year (with leading zeroes), between 001 and 366 069 H Two-digit hour (with leading zeroes), between 00 and 23 18 k Two-digit hour (without leading zeroes), between 0 and 23 18 I Two-digit hour (with leading zeroes), between 01 and 12 06 l Two-digit hour (without leading zeroes), between 1 and 12 6 M Two-digit minutes (with leading zeroes) 05 S Two-digit seconds (with leading zeroes) 19 L Three-digit milliseconds (with leading zeroes) 047 N Nine-digit nanoseconds (with leading zeroes) 047000000 P Uppercase morning or afternoon marker PM p Lowercase morning or afternoon marker pm z RFC 822 numeric offset from GMT -0800 Z Time zone PST s Seconds since 1970-01-01 00:00:00 GMT 1078884319 Q Milliseconds since 1970-01-01 00:00:00 GMT 1078884319047 As you can see in the preceding table, some of the formates yield only a part of a given date – for example, just the day or just the month.It would be a bit silly if you had to supply the day multiple times for format each part. For that reason, a format string can indicatethe index of the argument to be formatted. The index must immediately follow the %, and it must be terminated by a $. For example: 12System.out.println(&quot;%1$s %2$tB %2$te, %2$tY&quot;, &quot;Due date:&quot;, new Date());// Due date: February 9, 2004 Alternatively, you can use the &lt; flag. It indicates that the same argument as in the preceding format specification should be used again.That is, the statement 1System.out.println(&quot;%s %tB %&lt;te, %&lt;tY&quot;, &quot;Due date:&quot;, new Date()); yields the same output as the preceding statement.","link":"/java_string_formatting/"},{"title":"Google MapReduce 总结","text":"这篇文章是本人在按照 MIT 6.824 的课程安排学习 Google MapReduce 并完成对应 Lab 的基础之上总结而成。本文会详细介绍 Google MapReduce 的原理，但考虑到 Lab1 较为简单，本文不会提及 Lab1 的相关内容。部分有关 Google MapReduce 和具体代码实现的细节不会在本文中提及，读者可自行查阅 Google MapReduce 的论文原文以及本人的 MIT 6.824 Lab 代码仓库。 Google MapReduce 总结MapReduce 编程模型总的来讲，Google MapReduce 所执行的分布式计算会以一组键值对作为输入，输出另一组键值对，用户则通过编写 Map 函数和 Reduce 函数来指定所要进行的计算。 由用户编写的Map 函数将被应用在每一个输入键值对上，并输出若干键值对作为中间结果。之后，MapReduce 框架则会将与同一个键 $I$ 相关联的值都传递到同一次 Reduce 函数调用中。 同样由用户编写的 Reduce 函数以键 $I$ 以及与该键相关联的值的集合作为参数，对传入的值进行合并并输出合并后的值的集合。 形式化地说，由用户提供的 Map 函数和 Reduce 函数应有如下类型： $$\\begin{array}{lll}\\textrm{map} &amp; (k_1,v_1) &amp; \\rightarrow \\textrm{list}(k_2,v_2) \\\\textrm{reduce} &amp; (k_2,\\textrm{list}(v_2)) &amp; \\rightarrow \\textrm{list}(v_2) \\\\end{array}$$ 值得注意的是，在实际的实现中 MapReduce 框架使用 Iterator 来代表作为输入的集合，主要是为了避免集合过大，无法被完整地放入到内存中。 作为案例，我们考虑这样一个问题：给定大量的文档，计算其中每个单词出现的次数（Word Count）。用户通常需要提供形如如下伪代码的代码来完成计算： 1234567891011121314map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, “1”);reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); 函数式编程模型了解函数式编程范式的读者不难发现，MapReduce 所采用的编程模型源自于函数式编程里的 Map 函数和 Reduce 函数。后起之秀 Spark 同样采用了类似的编程模型。 使用函数式编程模型的好处在于这种编程模型本身就对并行执行有良好的支持，这使得底层系统能够轻易地将大数据量的计算并行化，同时由用户函数所提供的确定性也使得底层系统能够将函数重新执行作为提供容错性的主要手段。 MapReduce 计算执行过程每一轮 MapReduce 的大致过程如下图所示： 首先，用户通过 MapReduce 客户端指定 Map 函数和 Reduce 函数，以及此次 MapReduce 计算的配置，包括中间结果键值对的 Partition 数量 $R$ 以及用于切分中间结果的哈希函数 $hash$。用户开始 MapReduce 计算后，整个 MapReduce 计算的流程可总结如下： 作为输入的文件会被分为 $M$ 个 Split，每个 Split 的大小通常在 16~64 MB 之间 如此，整个 MapReduce 计算包含 $M$ 个Map 任务和 $R$ 个 Reduce 任务。Master 结点会从空闲的 Worker 结点中进行选取并为其分配 Map 任务和 Reduce 任务 收到 Map 任务的 Worker 们（又称 Mapper）开始读入自己对应的 Split，将读入的内容解析为输入键值对并调用由用户定义的 Map 函数。由 Map 函数产生的中间结果键值对会被暂时存放在缓冲内存区中 在 Map 阶段进行的同时，Mapper 们周期性地将放置在缓冲区中的中间结果存入到自己的本地磁盘中，同时根据用户指定的 Partition 函数（默认为 $hash(\\textrm{key})$ $\\textbf{mod}$ $R$）将产生的中间结果分为 $R$ 个部分。任务完成时，Mapper 便会将中间结果在其本地磁盘上的存放位置报告给 Master Mapper 上报的中间结果存放位置会被 Master 转发给 Reducer。当 Reducer 接收到这些信息后便会通过 RPC 读取存储在 Mapper 本地磁盘上属于对应 Partition 的中间结果。在读取完毕后，Reducer 会对读取到的数据进行排序以令拥有相同键的键值对能够连续分布 之后，Reducer 会为每个键收集与其关联的值的集合，并以之调用用户定义的 Reduce 函数。Reduce 函数的结果会被放入到对应的 Reduce Partition 结果文件 实际上，在一个 MapReduce 集群中，Master 会记录每一个 Map 和 Reduce 任务的当前完成状态，以及所分配的 Worker。除此之外，Master 还负责将 Mapper 产生的中间结果文件的位置和大小转发给 Reducer。 值得注意的是，每次 MapReduce 任务执行时，$M$ 和 $R$ 的值都应比集群中的 Worker 数量要高得多，以达成集群内负载均衡的效果。 MapReduce 容错机制由于 Google MapReduce 很大程度上利用了由 Google File System 提供的分布式原子文件读写操作，所以 MapReduce 集群的容错机制实现相比之下便简洁很多，也主要集中在任务意外中断的恢复上。 Worker 失效在 MapReduce 集群中，Master 会周期地向每一个 Worker 发送 Ping 信号。如果某个 Worker 在一段时间内没有响应，Master 就会认为这个 Worker 已经不可用。 任何分配给该 Worker 的 Map 任务，无论是正在运行还是已经完成，都需要由 Master 重新分配给其他 Worker，因为该 Worker 不可用也意味着存储在该 Worker 本地磁盘上的中间结果也不可用了。Master 也会将这次重试通知给所有 Reducer，没能从原本的 Mapper 上完整获取中间结果的 Reducer 便会开始从新的 Mapper 上获取数据。 如果有 Reduce 任务分配给该 Worker，Master 则会选取其中尚未完成的 Reduce 任务分配给其他 Worker。鉴于 Google MapReduce 的结果是存储在 Google File System 上的，已完成的 Reduce 任务的结果的可用性由 Google File System 提供，因此 MapReduce Master 只需要处理未完成的 Reduce 任务即可。 Master 失效整个 MapReduce 集群中只会有一个 Master 结点，因此 Master 失效的情况并不多见。 Master 结点在运行时会周期性地将集群的当前状态作为保存点（Checkpoint）写入到磁盘中。Master 进程终止后，重新启动的 Master 进程即可利用存储在磁盘中的数据恢复到上一次保存点的状态。 落后的 Worker如果集群中有某个 Worker 花了特别长的时间来完成最后的几个 Map 或 Reduce 任务，整个 MapReduce 计算任务的耗时就会因此被拖长，这样的 Worker 也就成了落后者（Straggler）。 MapReduce 在整个计算完成到一定程度时就会将剩余的任务进行备份，即同时将其分配给其他空闲 Worker 来执行，并在其中一个 Worker 完成后将该任务视作已完成。 其他优化在高可用的基础上，Google MapReduce 系统现有的实现同样采取了一些优化方式来提高系统运行的整体效率。 数据本地性在 Google 内部所使用的计算环境中，机器间的网络带宽是比较稀缺的资源，需要尽量减少在机器间过多地进行不必要的数据传输。 Google MapReduce 采用 Google File System 来保存输入和结果数据，因此 Master 在分配 Map 任务时会从 Google File System 中读取各个 Block 的位置信息，并尽量将对应的 Map 任务分配到持有该 Block 的 Replica 的机器上；如果无法将任务分配至该机器，Master 也会利用 Google File System 提供的机架拓扑信息将任务分配到较近的机器上。 Combiner在某些情形下，用户所定义的 Map 任务可能会产生大量重复的中间结果键，同时用户所定义的 Reduce 函数本身也是满足交换律和结合律的。 在这种情况下，Google MapReduce 系统允许用户声明在 Mapper 上执行的 Combiner 函数：Mapper 会使用由自己输出的 $R$ 个中间结果 Partition 调用 Combiner 函数以对中间结果进行局部合并，减少 Mapper 和 Reducer 间需要传输的数据量。","link":"/mapreduce_summary/"},{"title":"Mesos 总结","text":"这篇文章的内容主要基于 Mesos 的论文总结而来。未来若有机会继续深入使用 Mesos，我会直接更新这篇文章的内容。 Mesos 的背景及需求当下，有越来越多如 Hadoop、Spark、Storm 等各不相同的分布式计算框架出现。每种计算框架都有自己的适用场景，不存在哪个框架可以用于解决所有的问题，因此如此多数的公司都会同时维护多种不同的分布式计算框架，根据不同的需要选择最优的框架。为了提高资源的利用率，很多公司都会需要在同一个集群上运行不同的框架。 传统的集群共享方案包括两类： 将集群分为不同的分区，在每个分区上运行各个框架 在集群上启动虚拟机，将虚拟机分配给框架 上述方案的问题在于，其资源分配的粒度过大，和多数计算框架是不匹配的 – 如 Hadoop 等框架采用的都是细粒度的资源共享模型，会将作业切分为若干个短时子任务并分配到各个结点上，以提高框架的资源利用率，并通过将子任务分配到存储数据的结点上执行以提高数据本地性。 Mesos 则是一个通过提供统一接口来实现跨框架细粒度资源共享的系统。除此以外，与 Yarn 相比，Mesos 也有意简化了自身的设计，通过将更多的调度逻辑委托给上层框架以实现自身的高兼容性，同时确保自身的可扩展性以及可用性。 Mesos 资源分配流程本节主要讲述 Mesos 的集群基本组成，并描述 Mesos 资源分配的主要过程，以让读者对 Mesos 有大致的了解。 一个 Mesos 集群主要包括如下几个组件： Master：Mesos 的 Master 结点，保存当前的集群信息，负责进行集群资源分配。同一时间只会有一个 Master 起作用 Slave：Mesos 的 Slave 结点，运行在集群的各个机器上，负责管理宿主机的可用资源，并启动容器运行 Framework Executor Framework：如 Hadoop 等使用 Mesos 申请集群资源的上层框架 Scheduler：属于 Framework，负责接收来自 Mesos Master 的 Resource Offer，并指定所需资源运行自身的任务 Executor：属于 Framework，运行在 Mesos Slave 中，在 Scheduler 进行任务分配后负责运行所分配的任务 Mesos 的资源分配机制采用了一种叫做 Resource Offer（资源配给）的抽象概念来确保自身的高兼容性：Scheduler 向 Master 注册后，Master 会不时地向 Scheduler 发送 Resource Offer；此时，Scheduler 可以选择接受这个 Offer 并分配任务执行，也可以选择拒绝该 Offer。 如此一来，Mesos 实际上是将部分资源分配的控制逻辑移交到了上层框架，其所实现的实际上是去中心化的分布式资源调度模型。 这样的做法主要有两点好处： 这使得上层框架可以根据不同的需要（如高数据本地性或强耐错）采用不同解决方案，并能够独立地迭代这些解决方案 这也使得 Mesos 的功能得以最小化，减小 Mesos 系统进行功能迭代的需要 一次资源分配的过程如下图所示： Slave 会向 Master 报告其可用的资源 Master 调用自身的资源分配模块计算要提供给各个框架的资源，然后向对应框架的 Scheduler 发出 Resource Offer Scheduler 收到 Resource Offer 后向 Master 进行响应，指定要在哪个 Slave 上用多少资源运行什么任务 Master 会把任务转发给 Slave，由 Slave 将指定的资源分配给框架的 Executor，再由 Executor 执行所分配的任务 Mesos 机制详解本节将在上一节的 Mesos 资源分配基本过程的基础之上，详细解释 Mesos 的运行机制。 Master 资源分配模块Master 中可以配置一个可插拔式的资源分配模块（Pluggable Allocation Module），用于决定如何将集群可用的资源分配给各个框架，使用不同的分配模块则可以采用不同的分配策略。目前已经支持的分配模块包括按优先级分配和公平分配两种。 通常情况下，Mesos 会认为框架提交的任务都是执行时间较短的小任务，因此 Mesos 只会在任务完成时执行上述的资源分配过程，进行资源的再分配。如果某个任务运行时间过程，Mesos 的资源分配模块可能会直接杀死该任务，而选择具体杀死哪个任务的策略也是由所使用的资源分配模块所决定的。 对于多数如 Hadoop 这般的框架来说，少数几个任务被杀死不会对框架的运行造成影响，但其他一些框架的任务间可能存在相互依赖关系，任务被杀死则可能对框架产生较大的影响。Mesos 为此情况提供了 Guaranteed Allocation 机制，允许框架向 Mesos 集群申请一定的资源：当框架所占用的总资源在该资源阈值之下时，框架的任务不会被杀死；反之，当框架使用的资源超过该阈值时，框架的任务则有可能被杀死。 除此以外，为了能让 Mesos 判断出何时该杀死任务，框架在无可用资源时也需要通过 API 向 Mesos 告知其对资源的需求。 Slave 资源隔离模块为了确保 Slave 上 Executor 间的资源隔离和准确分配，Mesos Slave 采用容器技术来为 Executor 分配资源。Mesos Slave 同样可以通过配置可插拔式的资源隔离模块来选用不同的容器技术，目前可用的实现包括了 Linux Container、Solaris 和 Docker。 容错机制Mesos 的容错机制主要体现在其 Master 上。首先，Mesos Master 采用的是软状态（Soft State）容错机制：集群的完整信息会分散在 Scheduler 和 Slave 上，Mesos Master 则可以利用这些信息重建其内部状态。具体而言，Master 会保存的信息包括 Slave 列表、Framework 列表以及当前正在执行的任务列表。与此同时，Mesos Master 会借由 ZooKeeper 集群进行多机热备，当一个 Master 不可用时另一个 Master 就会接替它的位置，Slave 和 Scheduler 也会连接到新的 Master 上。 除了 Master 的容错以外，Mesos 还会把结点故障和 Executor 失效等信息回报给 Scheduler，由 Scheduler 来负责处理这部分失效事件。 为了让上层框架能够处理 Scheduler 失效的问题，Mesos 还允许框架向 Mesos 注册多个 Scheduler，并由 Mesos Master 在某个 Scheduler 失效时通知另一个 Scheduler 接替它的位置。 Mesos 最佳场景如前文所述，Mesos 所实现的是去中心化的分布式资源调度模型，交由各个框架的 Scheduler 来决定接受哪些 Offer，实际的资源使用将由框架进行控制，而不是由 Mesos 本身进行控制。这样的做法无疑在确保 Mesos 高兼容性的同时也降低了 Mesos 的复杂度，但正如其他去中心化方案一样，它的表现是有可能比中心化调度器要差的。因此，在使用 Mesos 时，了解 Mesos 表现最佳的场景是有必要的。 从论文第 4 章的论证可以得出结论：如果上层框架的规模能够弹性地伸缩、任务持续时间均大体相当，且框架都会均等地选择使用各个结点时，Mesos 能有十分出色的表现；如果不同的框架倾向于使用不同的集群结点，Mesos 的行为近似于一个在各个框架间公平分配资源的中心化调度器；除此以外，只要框架能确保在大多数时候使用短任务，那么即使任务的持续时间存在差异也不会对框架的性能产生很大的影响。 由此，Mesos 实际上鼓励框架满足以下几点性质以提高其作业的响应时间： 短任务：框架使用短任务时，Mesos 也更容易为短任务预留资源；除此，在一个任务中只做少量工作也能减小 Mesos 主动杀死任务或结点失效时对框架的影响 弹性伸缩：框架应能够在获得资源时立刻使用它们，而不是花上较长的时间等到可分配资源达到某个最小阈值 不接受不明资源：框架不应接受其不能使用的资源，因为这会对多数资源分配策略造成影响 可能存在的问题与解决方案Mesos 的机制无疑是简单高效且可行的，但这样的设计固然也存在着它的不足，论文中也对这部分的内容进行了一定的讨论。本节将分别讨论各种可能存在的问题以及 Mesos 的解决方案。 Resource Offer 轮询由于 Mesos 采用 Resource Offer 推送的方式进行资源分配，Mesos 无法预先得知某个框架是否会接受某个 Resource Offer，必须进行一次推送请求。事实是，Mesos 有可能在轮询过多个框架后才能找到接受该 Offer 的框架，浪费过多的时间。 为此，框架能以布尔表达式的形式向 Mesos 注册 Offer Filter，不通过该 Filter 判断代表框架将永远不接受这样的 Offer。如此，Mesos 便能避免过多的网络请求。 实际上，当确保框架的任务都是细粒度时，即便不使用 Filter 也不会对集群的性能产生较大的影响。 Offer 响应等待Mesos 的 Resource Offer 推送本质上是网络请求，那么便存在网络延时、网络超时等问题。实际上，当 Offer 发出后，Mesos 就会认为这部分资源已被分配给该框架，直到该框架进行响应。这也就要求 Scheduler 在收到 Offer 后必须尽快响应，或者通过 Filter 来过滤部分 Offer。 除此以外，当超出一定时间后 Master 仍未收到 Offer 响应时，Master 就会撤回这部分资源分配并将其提供给其他框架。 资源碎片化如果任务所需要的资源各不相同的话，那么 Mesos 对不同框架的资源分配可能无法像中心化的调度器那样对资源分配进行优化。不过值得注意的是，在这种情形下产生的资源浪费实际上和最大任务体积与最小结点体积间的比例相关，因此即便存在这样的情况，对于使用“较大”结点（如多核机器）运行“较小”任务的集群来说，使用分布式调度仍然能够提供较高的资源利用率。 另一个可能存在的问题是：假设集群目前有大量资源需求较小任务需要运行，如果此时某个框架有较大的资源需求，那么该框架可能会发生饥饿现象（Starvation），长时间得不到资源分配，因为当现有的小任务完成后，其释放出来的资源可能会被另一个小任务立刻抢走。为了解决这样的问题，Master 资源分配模块可以为每个 Slave 设定最小供给资源量（Minimum Offer Size），一直到 Slave 上的可用资源达到该阈值时才向框架发出 Resource Offer。 上层框架复杂度增加将资源调度逻辑交由上层框架完成固然会增加框架的复杂度，但这样的复杂度增加并不一定是坏事。 首先，无论是使用 Mesos 还是其他的中心化调度方案，框架都必须清楚自己的资源使用偏好：在使用中心化的调度器时，框架需要向该调度器告知其资源偏好，而使用 Mesos 时则需要框架基于这些偏好来决定接受哪些 Resource Offer。 除此以外，许多现有框架的调度策略采用的都是即时算法，因为框架无法预测每个任务的所需时间，它们需要能够对失效的任务或进度落后的任务（Straggler）作出处理。使用 Resource Offer 机制，框架能够很好地实现这些调度策略。","link":"/mesos_summary/"},{"title":"MongoDB Aggregation","text":"在之前的文章中，我总结了 MongoDB CRUD 操作的基本方法，而本文将会介绍 MongoDB 的 Aggregation Framework。 MongoDB 的 Aggregation 操作的灵感主要源于 SQL 的 Aggregation 操作。在 SQL 中，我们可以通过 count 、 sum 等运算符来为某张表的数据进行统计。比如，为了统计每个电子设备制造厂商所发行的设备的种数，我们可能会这样写： 12SELECT manufacturer, count(*) FROM products 由此，我们便能获得统计结果，比如苹果发行了 10 种不同的电子设备。MongoDB 同样也为用户提供了对 Aggregation 操作的支持。通过运用 MongoDB 的这项功能，我们同样可以达成如上述 SQL 语句那般的效果。接下来我们就来学习一下 MongoDB Aggregation 的基本使用方法。 Aggregation PipelineMongoDB Aggregation 使用 Pipeline 的形式来组织用户指定的操作。使用过 Unix 或 Linux 的读者应该对 Shell 的管道操作十分熟悉了，不过即使你没有学过也没有关系。接下来将通过实际操作来演示 MongoDB 的 Aggregation Pipeline。 假设我们有一个叫做 zips 的 Collection（数据文件可在这里下载到），这个 Collection 的模式大致如下： 12345678910{ &quot;_id&quot;: 35004, &quot;city&quot;: &quot;ACMAR&quot;, &quot;loc&quot;: [ -86.51557, 33.584132 ], &quot;pop&quot;: 6055, &quot;state&quot;: AL} 可以看到，zips 中的一条 Document 以城市的邮政编码（zip）作为 _id，并给出了城市名 city 、城市坐标 loc 、城市人口 pop 以及城市所属州的缩写 state。 上述 Collection 的模式并不复杂，如果忽略 loc 字段，剩余的模式完全可以直接作为关系型数据库的表模式。那么假设我们有这么一条 SQL 语句： 1234 SELECT city, sum(pop) AS population FROM zips WHERE state = &quot;NY&quot;GROUP BY city 不难看出，上述语句计算的是纽约州每个城市的总人口（我并不是很懂 SQL，写错了别打我）。那么在 MongoDB 中，同样的操作是这样写的： 1234567891011121314151617db.zips.aggregate([{ $match: { state: &quot;NY&quot; } }, { $group: { _id: &quot;$city&quot;, population: { $sum: &quot;$pop&quot; } } }, { $project: { _id: 0, city: &quot;$_id&quot;, population: 1, } }]) 首先这里出现了三个 Aggregation 专用的运算符：$match 、 $group 和 $project。它们具体的作用我会在后文详述。我们之所以说 MongoDB 的 Aggregation 使用的是 Pipeline 来组织用户的操作，正是因为 db.zips.aggregate 方法接受的是一个由 Aggregation 操作组成的数列，数列中的每个操作将按顺序执行，前一个操作的结果将作为后一个操作的输入。 上面这条语句中，首先第一个 $match 运算符相当于之前的 SQL 语句中的 WHERE 子句，它从 Collection 中筛选出所有属于纽约州的邮政编码，并将其作为下一个操作的输入。下一个操作为 $group 操作，它相当于 SQL 中的 GROUP BY，并以一个 _id 来指明，我们将以 city 字段来进行 group。同时在该操作中还搭配使用了 $sum 运算符，将各个城市的 pop 字段值进行求和，赋给了新的 population 字段。最后的 $project 则相当于 SQL 中的 SELECT AS，将上一个操作传来的结果集中的 _id 重新改名为 city，并保留了 population 字段。 尽管这么说其实还是比较模糊，但正如我所说，我将在下文逐个讲述每个 Aggregation 操作符的作用，这里我们只需要了解到 MongoDB Aggregation 的 Pipeline 意味着所有 Aggregation 操作将以流水线的形式来处理数据即可。 Aggregation Pipeline 中的每一次操作被称为一个 stage。Stage的操作种类包括如下： 名称 作用 $project 改变 Pipeline 中的 Document 的模式，如添加一个新的字段、改变字段值或删除字段等 $match 过滤传入的 Document，并不做改变地输出匹配的 Document $redact 综合 $project 和 $redact 的功能，对 Document 进行改写 $limit 给定一个数字 n，仅输出传入的前 n 个 Document $skip 给定一个数字 n，跳过传入的前 n 个 Document $unwind 拆散输入 Document 中指定的一个数组字段，为数组中的每个元素生成一个新的 Document，并用该元素作为该字段的值 $group 根据给定的标识表达式组织传入的 Document，并在声明了累积操作符的情况下将其应用于每一组 Document $sort 根据指定的字段和顺序，对输入的所有 Document 进行排序 $geoNear 根据 Document 与给定地理坐标的远近程度进行排序后输出 $out 将 Aggregation 的结果写入到指定的 Collection 中。$out 只能作为 Aggregation 的最后一个 Stage 接下来我将逐个介绍上述的所有 Stage。 $project$project 只会将设定好的字段值传递给 Pipeline 的下一个 Stage，这些字段可以来自原有的字段，也可以是新创建的字段。从形式上，$project 的标准使用格式如下： 1{ $project: { &lt;specifications&gt; } } 可以看到，$project 的参数为一个 Document，该 Document 说明哪些字段该输出、如何得出这些字段以及哪些字段该被删除。该 Document 的格式如下： 语法 说明 &lt;field&gt;: &lt;1 or true&gt; 指定结果 Document 包含原有的某个字段 _id: &lt;0 or false&gt; 指定结果 Document 不包含原有的 _id 字段 &lt;field&gt;: &lt;expression&gt; 根据给定表达式为结果 Document 创建一个新的字段 默认情况下，输入 Document 的 _id 字段将会保留在输出 Document 中，除非显式地将其声明为 _id: 0 或修改为其他值。同时，除 _id 外的其他所有字段默认是不保留的，如果需要保留在输出 Document 中则必须通过上述语法显式地指定。 如果你用 &lt;field&gt;: &lt;1 or true&gt; 语法指定包含某个原本不存在的字段，$project 会忽略你的这项设置，即 $project 不会因你这项设置而为输出 Document 新增一个字段。 使用 &lt;field&gt;: &lt;expression&gt; 为输出 Document 新增字段时，我们可以指定新增字段的字段名，同时用表达式给出字段的值。更多有关表达式的内容，详见这里。 如果要为某个字段设置一个数字或布尔值，必须使用$literal操作符。否则，$project 会认为你只是在指定包含或删除某个原有字段。 举个例子，假设我们有一个叫做 orders 的 Collection，其中的一个 Document 模式如下： 123456{ _id: 5, product_id: 123, price: 50, quantity: 5} 订单给出了用户购买的物品的单价 price 以及用户购买的数量 quantity。我们完全可以通过 $project 来生成只包含订单总价的 Document： 1234567db.orders.aggregate([{ $project: { product_id: true, amount: { $multiply: [ $price, $quantity ] } } }]) 如此，新的 Document 中保留了原有的商品 idproduct_id，同时利用原有的商品单价和商品数量计算出了订单总价 amount。 实际上，$project 所接受的参数之所以被叫做 specifications，是因为它正是输出 Document 的模式的 specification，输出 Document 的模式将与其保持一致。我们完全可以利用这一特性使输出 Document 的某个字段包含一个子 Document 或数组： 12345678910db.orders.aggregate([{ $project: { product: { product_id: &quot;$product_id&quot;, price: &quot;$price&quot; }, amount: { $multiply: [ $price, $quantity ] } } }]) 如此一来，新的 Document 中的 product 字段的值便是一个包含了商品 id 和商品单价的 Document 了。 $match$match 接受一个表示查询条件的 Document 作为参数，只把匹配该查询条件的 Document 传递到下一个 Stage。$match 的标准使用格式如下： 1{ $match: { &lt;query&gt; } } 其中，用于表示查询条件的 query 使用与 find 和 findOne 方法中的查询条件完全相同的格式。有关查询语法，详见这里。 将 $match 放在 Pipeline 中尽可能靠前的位置，可以更早地降低 Pipeline 中 Document 的数量，因为 Pipeline 实际上是在内存中做运算的。如果你将 $match 作为第一个 Stage，它就可以像 find 和 findOne 那样利用上 Collection 中的索引了。 你不能在 $match 中使用$where操作符。同时，想要在 $match 中使用$text操作符，必须确保 $match 为 Pipeline 的第一个 Stage。 举个例子，还是上述那个 orders Collection，我们可以编写如下 Aggregation： 1234567891011121314db.orders.aggregate([{ $project: { product: { product_id: &quot;$product_id&quot;, price: &quot;$price&quot; }, amount: { $multiply: [ $price, $quantity ] } } }, { $match: { amount: {$gt: 500} } }]) 如此一来，我们就只会得到总价大于 500 的订单了。 $sort$sort 对传入的 Collection 进行排序后传递到下一个 Stage。$sort 的标准使用格式如下： 1{ $sort: { &lt;field1&gt;: &lt;sort order&gt;, &lt;field2&gt;: &lt;sort order&gt; ... } } 实际上这些参数并没有看上去那么复杂，它的格式和 sort() 方法的参数是完全一致的。比如： 12345678910111213db.orders.aggregate([{ $project: { product: { product_id: &quot;$product_id&quot;, price: &quot;$price&quot; }, amount: { $multiply: [ $price, $quantity ] } }, { $sort: { amount: -1 } }]) 我们就获得了总价按降序排列的订单列表了。 $limit$limit 接受一个正整数参数 n，只把传入的前 n 个 Document 传递到下一个 Stage。$limit 的标准使用格式如下： 1{ $limit: &lt;positive integer&gt; } 从功能上讲，$limit 和 limit() 方法是完全一致的。举个例子： 123db.orders.aggregate( { $limit : 5 }); 这样一来我们便可以获得前 5 个订单了。 值得注意的是，如果 $limit 紧接着一个 $sort，$sort 将会采用 Lazy 的排序方式，在选出前 n 个 Document 以后便结束排序，而不会对整个 Collection 进行排序。 $skip$skip 接受一个正整数参数n，跳过传入的前n个 Document 后，将剩余的 Document 原封不动地传给下一个 Stage。$skip 的标准使用格式如下： 1{ $skip: &lt;positive integer&gt; } 从功能上讲，$skip 和 skip() 方法是完全一致的。举个例子： 123db.orders.aggregate( { $skip : 5 }); 这样一来我们便跳过了前 5 个订单了。 $unwind$unwind 接受一个字段名作为参数，拆散指定的数组字段，为数组中的每一个元素生成一个新的 Document，并以该元素作为新的 Document 中该数组字段的值。$unwind 的标准使用格式如下： 1{ $unwind: &lt;field path&gt; } 注意，在指定字段名时，字段名前面要加上一个 $ 符号。举个例子，假设我们有 Document 如下： 1{ a : 0, b : 0, c : [ 0 1 2 ] } 我们执行 { $unwind: &quot;$c&quot; } 操作后，将得到如下几个 Document： 123{ a : 0, b : 0, c : 0 }{ a : 0, b : 0, c : 1 }{ a : 0, b : 0, c : 2 } 在使用 $unwind 时，有几点需要注意一下： 如果传入的某个 Document 的指定字段的值不是一个数组，aggregate 方法会抛出一个错误 如果传入的某个 Document 不包含你所指定的字段，$match 会忽略该 Document，不会为其生成任何 Document 如果传入的某个 Document 的该字段的值为空数组（[]），$match 同样会忽略该 Document，不为其生成任何 Document $group$group 基于给定的规则将 Document 分入不同的分组中，为每个分组产生一个新的 Document，该 Document 的字段值将由累积表达式给出。$group 的标准适用格式如下： 1{ $group: { _id: &lt;expression&gt;, &lt;field1&gt;: { &lt;accumulator1&gt; : &lt;expression1&gt; }, ... } } 其中，我们需要显式地给出 _id 的值的计算方式，被计算出拥有相同的 _id 值的 Document 将被放入到同一组中。如果你想要让所有 Document 都被分入同一组，将 _id 设为 null 即可。 其他字段的值将由累积表达式给出，而累积表达式由累积操作符和普通的表达式组成。可选的累积操作符如下： 名称 作用 $num 返回每组 Document 的表达式所得值的和。自动忽略非数字的值 $avg 返回每组 Document 的表达式所得值的平均数。自动忽略非数字的值 $first 返回每组中第一个 Document 的表达式所得值 $last 返回每组中最后一个 Document 的表达式所得值 $max 返回每组 Document 的表达式所得值的最大值 $min 返回每组 Document 的表达式所得值的最小值 $push 以一个数组包含一组所有 Document 的表达式所得值 $addToSet 以一个集合包含一组所有 Document 的表达式所得值 举个例子，假设我们有 Document 如下： 12345678{ a : 0, b : 0, c : 0 }{ a : 0, b : 0, c : 1 }{ a : 0, b : 1, c : 0 }{ a : 0, b : 1, c : 1 }{ a : 1, b : 0, c : 0 }{ a : 1, b : 0, c : 1 }{ a : 1, b : 1, c : 0 }{ a : 1, b : 1, c : 1 } 我们执行如下操作： 123456{ $group: { _id: {a: &quot;$a&quot;, b: &quot;$b&quot;}, c: {$max: &quot;$c&quot;} }} 即可获得结果如下： 1234{ _id : { a : 0, b : 0 }, c : 1 }{ _id : { a : 0, b : 1 }, c : 1 }{ _id : { a : 1, b : 0 }, c : 1 }{ _id : { a : 1, b : 1 }, c : 1 } 其他累积运算符的用法也是类似，这里不再赘述。具体的用法可以参考这里。 Aggregation 的局限尽管 MongoDB Aggregation 和 SQL Aggregation 相同，都会是我们日常生产所必须用到的工具，但在使用的时候，MongoDB Aggregation 相关的几个条件限制也是我们所需要考虑的。 最大内存占用：100MBMongoDB Aggregation 为单个 Stage 所能分配的最大内存为 100MB，当某个 Stage 的内存占用超过 100MB 时，你可能就拿不到结果了。因此，我们应将 $match 等可削减 Document 数量的 Stage 放在尽可能靠前的位置，以免某个 Stage 产生了过大的中间结果。如果无论如何都需要使用上超过 100MB 的内存，可以在为 aggregate 方法加上 allowDiskUse: true 参数，允许其使用磁盘空间来辅助计算。 更多有关 allowDiskUse 的内容，详见这里。 单个结果 Document 最大体积：16MBMongoDB Document 不能超过 16MB 大小这一限制来自于其所使用的 BSON 格式的大小限制。鉴于 Aggregation 产生的结果固然也会以 BSON Document 的形式传递给客户端，自然单个结果 Document 也不能超过 16MB。 更多有关 Aggregation 限制的内容，详见这里。","link":"/mongodb_aggregation/"},{"title":"MongoDB Sharding","text":"在上一篇博文中，我详细讲解了 MongoDB Replica Set 相关的概念。作为 MongoDB 分布式解决方案之一，Replica Set 主要用于提高 MongoDB 集群的可用性，但不难发现，同一个 Replica Set 中的 Primary 和 Secondary往往承受着大致相同的写压力，因此 Replica Set 实际上并不能用来提高集群的处理能力。 在这篇博文中，我将详细介绍另一种 MongoDB 分布式解决方案 —— Sharding 的相关概念，并介绍如何利用 Sharding 来对数据库进行水平拓展。 Why Sharding?MongoDB 提供了 Sharding 机制来为数据库系统提供横向扩展（Horizontal Scaling），生产系统可利用 Sharding 机制来存储庞大的数据集并提高系统的数据吞吐量。 当应用程序需要数据库存储更多的数据并提供更高的吞吐量时，往往单一机器的处理能力就成了数据库系统的瓶颈：高吞吐量意味着高 CPU 占用，而日渐庞大的数据集也会挑战机器的磁盘容量。 当我们拓展一个系统的性能时，往往有两种拓展方向，分别是纵向拓展（Vertical Scaling）和横向拓展（Horizontal Scaling）。 纵向拓展即为机器换上更强的 CPU 或者加内存加硬盘。在一定程度内，纵向拓展是可行的，但一旦超过某种程度就会出现限制：越高性能的硬件往往性价比越低。除此之外，如亚马逊和阿里云等云服务器提供商往往不会为单个实例提供过高的性能。比如，阿里云的单个 ECS 服务实例最高的配置只能去到 16 核 CPU + 64G 内存。综合考虑上述两个因素，不难看出纵向扩展是存在极限的，而且实例越接近该极限，扩展的性价比就越低。 横向扩展则是在不改变单个实例的配置的情况下，通过增加新的实例来扩展系统的处理能力。横向扩展的案例有很多，比如目前十分热门的分布式计算，或者只是简单的负载均衡。横向扩展允许每个实例的配置相对较低，因此横向扩展有着高得多的性价比，不会再受到限制。 具体到 MongoDB 上，其所提供的 Sharding 便是横向扩展的典型代表。采用 Sharding 构成的高可用 MongoDB 架构由多个 Shard 组成，每个 Shard 可以是一个单一的 mongod 实例，也可以是一个 Replica Set。Sharding 将 Collection 里的数据分成若干个块（Chunk），再将每个块分散到 Shard 中。 总的来讲，Sharding 能为 MongoDB 集群带来如下优势： 更高的吞吐量：Sharding 将数据集分散到了不同的 Shard 中，同时也将针对不同 Chunk 的读写压力分散到了这些 Shard 上 更高的存储容量：通过将数据集分散到不同的 Shard 中，每个 Shard 只需要存储部分数据集，因此横向扩展时也能够线性地提高 MongoDB 集群的存储容量 高可用性：Shard 集群在部分 Shard 不可用时仍然可以完成客户端发来的操作。尽管 Shard 上的数据在 Shard 不可用时也无法访问了，但针对其他 Shard 上的数据的操作仍然可以顺利完成 Shard 集群成员一个 MongoDB Shard 集群由如下几种成员组成： Query Router（查询路由），即 mongos 实例，是客户端与 Shard 之间沟通的桥梁，客户端只应该通过它们来访问 MongoDB 集群。Query Router 接收来自客户端的查询请求，将请求分发到对应的Shard，并收集结果返回至客户端。通常，为了减轻 Query Router 的压力，生产系统可以有多个 Query Router。 有关 Query Router 的更多内容，详见这里。 Config Server（配置服务器）保存着集群的元数据和配置信息，记录着每个 Shard 上保存的 Chunk 以及每个 Chunk 所关联的 Shard Key 范围。Query Router 会缓存并使用这些元信息来对接收到的读写请求进行分发，同时也在集群 Shard 发生变化时对这些信息进行修改。 从 MongoDB 3.2 版开始，Shard 集群中的 Config Server 还可以是一个 Replica Set 而不是 3 个内容完全相同的 Config Server。而从 MongoDB 3.4 版开始，对后一种方法的支持被移除，Config Server 必须是一个 Replica Set。 有关 Config Server 的更多内容，详见这里。 Shard 负责存储数据。它可以是一个 mongod 实例，也可以是一个 Replica Set。但为了在生产环境下提供高可用，每个 Shard 必须是一个 Replica Set。 除了一般的 Shard 以外，Shard 集群会为每一个数据库分配一个 Primary Shard 用于保存数据库内那些没有 Shard 的 Collection 的数据。Query Router 会在创建新数据库时自动选择当前存储数据最少的 Shard 作为新数据库的 Primary Shard。 有关 Shard 的更多内容，详见这里。 Shard 集群数据切分MongoDB 将 Collection 内的数据分散到 Shard 上，而如何分配这些数据则取决于数据的 _Shard Key_。 Shard Key在对一个 Collection 进行 Shard 操作之前，我们必须先为其指定一个 Shard Key。为了支撑 Shard Key，Collection 必须在指定的域上已经建有索引，或者指定的域是该 Collection 某个复合索引的前缀，且在所有 Document 都必须存在该域。如果该 Collection 的内容为空且在 ShardKey 指定的域上不存在索引，MongoDB 则会自动创建一个索引。 MongoDB 根据 Shard Key 的值将每个 Document 放入到不同的 Chunk 中，再将这些 Chunk 平均地分配到每个 Shard 上。在将 Document 放入到 Chunk 时，MongoDB 提供了两种不同的算法，分别是基于值域分割和基于哈希值分割。 值得注意的是，一旦开始 Sharding，Shard Key 便不能再被修改，每个 Document 中 Shard Key 所关联的域的值也不能再被修改。 有关 Shard Key 的更多内容，详见这里。 基于值域分割Range-based Sharding 将 Shard Key 所处的值域空间分为若干个子域，Shard Key 值位于某个子域中的 Document 则被分配到对应的 Chunk 中。例如，我们考虑一个由数字组成的 Shard Key，那么 Shard Key 本身可属的值域自然是从全局最小值直到全局的最大值。MongoDB 将这个值域分成若干个不重叠的子域，比如其中有一个子域是从 $[25, 175)$，那么所有 Shard Key 在这个范围之间的 Document 就会被分配到对应的 Chunk 之中。 基于值域的分割模型可以让比较“接近”的 Document 有很高的几率被分配到同一个 Chunk 中，从而被分配到同一个 Shard 中。如此一来， Query Router 在接收到基于 Shard Key 大小比较的查询时也可以立刻得知自己应该将请求分发到哪些 Shard 中，而无需向所有 Shard 广播请求。 基于值域的分割适合有以下性质的域： 取值范围大 重复频率低 非单调变化 基于值域的分割模型的不足在于其可能无法把数据平均地分配在所有 Chunk 上。 更多有关值域分割的内容，详见这里。 基于哈希值分割Hash-based Sharding 为每个 Document 的 Shard Key 计算哈希值，并将其放入到对应哈希值域的 Chunk 中。如此一来，Document 会被分配到哪个 Chunk 可以视为是随机的，即使是值比较“接近”的 Document 也不大可能会被放入到同一个 Chunk 中。 在对一个空的 Collection 进行基于哈希值分割时，MongoDB 会自动为每个 Shard 创建两个空的 Chunk。 基于哈希值分割能够更好地将数据平均地分散在每个 Shard 上，但这样的模型无法像基于值域分割那样维持一个集群范围内的索引，当系统请求基于 Shard Key 域的范围查询时，Query Router 只能把该请求广播到每个 Shard 上了。 在使用基于哈希值分割时应尽量选择取值范围较广的域作为 Shard Key。事实上，基于哈希值的分割很适合用于那些会单调变化的域，如默认的 _id 或者时间戳。 更多有关哈希分割的内容，详见这里 数据均衡在生产系统的日常使用中，新的数据会加入到数据库中，也有可能会有新的 Shard 加入集群。这样的事件会导致数据分布的不均衡，比如某个 Chunk 特别大，或者某个 Shard 包含特别多的 Chunk。 MongoDB 维持数据分布均衡的方法可分为两种：Split 和 Balance。 Splitting在某个 Chunk 的大小超过了某个特定的数值时，MongoDB 将对其进行 Split 操作，将其分为若干个 Chunk。 Splitting 并不会带来太多的元数据变动，因为该过程实际上不会改变 Document 所处的 Shard。 更多有关 Chunk 分割的内容，详见这里。 BalancingMongoDB Balancer 会监控每个 Shard 上的 Chunk 数，并在其发现某个 Shard 上的 Chunk 数量到达迁移阈值时，便会试图对 Chunk 进行迁移使得每个 Shard 拥有相同数量的 Chunk。 从 MongoDB 3.4 版本开始，Balancer 会作为后台进程运行在 Config Server Replica Set 的 Primary 结点上。 Chunk 的迁移过程涉及元数据的大量改动。整个过程可以分为如下几个步骤： Balancer 计算出迁移的计划。单次的迁移计划包括从哪个 Shard 把哪个 Chunk 转移到哪个 Shard 迁移过程作为后台进程在源 Shard 和目标 Shard 上启动，指定的 Chunk 开始把当前的所有 Document 复制到目标 Shard，目标 Shard 同时构建所需的索引 发送完毕后，目标 Shard 将迁移期间发生在该 Chunk 上的改动应用到它本地的 Chunk 副本中。这个过程类似于 Replica Set 的同步 最后，修改 Config Server 的元数据，迁移完成，源 Shard 可以删除它的 Chunk 副本了 整个过程可能会花费大量的时间，因此 Config Server 数据的修改和源 Shard 对该 Chunk 的删除被安排在了最后。在整个过程顺利完成之前，对该 Chunk 的请求仍然会被发到源 Shard 中。在这个过程中如果发生了错误，MongoDB 也会立刻终止该过程，源 Shard 上的 Chunk 依然完好如初。","link":"/mongodb_sharding/"},{"title":"MongoDB 存储引擎","text":"2015 年 3 月份，MongoDB 发布了 3.0.1 版，从原本的 2.2、2.4、2.6 升级到了最新的 3.0。大量的新功能在 3.0 版本中引入，其中包括了 MongoDB Java 驱动的大幅更新。但对于 MongoDB 数据库本身来说，可更换的数据存储引擎算得上是 3.0 最重大的更新之一。 在 3.0 之前，MongoDB 是不能像 MySQL 那样随意选择存储引擎的。而到了 3.0，MongoDB 的所使用的存储引擎可由用户自行指定。目前，用户可选择的存储引擎包括 MMAPv1 和 WiredTiger。 什么是存储引擎？存储引擎是数据库与底层硬件沟通的桥梁，数据库通过调用存储引擎提供的接口来完成增删查改等操作。可以说，存储引擎也是一种硬件驱动。 对于“硬件”，这里不仅指的是计算机不同型号的 CPU、内存和磁盘的系统调用，同时还包括了 MongoDB 在硬件上存储数据的方式。比如，对于 MMAPv1 和 WiredTiger，它们所使用的索引格式就有所不同。 不难想到，数据结构和算法从来都不是面面俱到的，有些时候为了在某些方面表现出出色性能，其他方面必然就会有所欠缺。ACM 中常说的“空间换时间”也大概是这么一个道理。因此，只有了解不同存储引擎的特性才能够更好地优化数据库系统的性能。 MMAPv1MMAPv1 实际上就是 MongoDB 在 3.0 以前原有的存储引擎，在 3.0 版本它也继续作为 MongoDB 的默认存储引擎（注：MongoDB 3.2 版本将会把默认存储引擎改为 WiredTiger）。之所以叫 MMAP，实际上是因为这个存储引擎会把数据直接映射到虚拟内存上，即 “memory mapping”。我们知道，MongoDB 的客户端与服务器传输数据都是通过 BSON 格式完成的，而 MMAPv1 则会不做修改地将 BSON 数据直接保存在磁盘中。这一点通过观察 /data/db 文件夹下的文件即可获知。而 MMAPv1 通过将 BSON 数据直接映射到虚拟内存上，实际上也利用操作系统帮助自己完成了不少的工作。 提起 MMAPv1，我们这里要先讲讲它的记录分配机制。 MMAPv1 记录分配机制在 MongoDB 中，每条数据以 Document 的形式进行存储，并通过 Collection 来管理 Document。通过观察 /data/db 文件夹即可得知，同一个 Collection 中的 Document 会根据插入（insert）的先后顺序，连续地写入到磁盘的同一个区域（region）上。考虑到 MongoDB schemaless 的特性，即使是同一个 Collection，也会存在某些 Document 特别大，某些特别小的情况。除此之外，我们还要考虑到在数据库使用过程中，某些 Document 会因为update操作而变大。 MMAP 在第一次插入时会为每个 Document 开辟一小块专属的区域，你可以管它叫一个”record”（记录），或一个”slot”（record 这个名字容易和别的东西混淆，所以后面我会管它叫 slot），其他新插入的 Document 则必须从这一小块区域的结尾处开始写入。 首先，一个 slot 为了能完整放入一个 Document，首先它的大小必须大于等于这个 Document 的初始大小，但它的大小一旦确定，且尾部被写入了新的 Document 以后，它的大小就固定了。上面我们提到，MongoDB 的 Document 有可能因为update操作而变大。如果在 update 以后，Document 的大小超过了当前的 slot 怎么办？ MongoDB 采取的做法，是在 Collection 的尾部申请一个更大的 slot，并把新的 Document 整个移动过去，同时还要 update 与该 Document 相关的索引，使其指向 Document 所在的新位置。不难想象，这样的操作是比单纯的写入费时得多的，而且 Document 原本的空间被释放以后，很可能就会形成一个空间碎片。 为了减少这样的操作的发生，MongoDB 采取的做法是在创建 slot 时，不仅使其能够放入一个 Document，同时也会预留一定的空间，称之为 padding（内边距）。这样一来，Document 在自己的专属空间中有了一定的发展空间，合适地选择 padding 的大小便能有效地降低这种操作发生的几率。在 3.0 版之前，MongoDB 尝试根据 Document 大小增加的方式来预估合适的 Padding 大小，而 3.0 则改而使用新的两种选择 padding 的策略，分别是二次幂分配（Power of 2 Sized Allocation）和无 Padding 分配（No Padding Allocation）。 二次幂分配策略二次幂的空间分配策略实际上不止在 MongoDB，在其他系统或语言中都十分常见。大家接触得最多的例子，包括了 C++中的 vector 和 Java 中的 ArrayList。MongoDB 的二次幂空间分配策略，使得为每个 Document 分配的 slot 的大小从 32B 开始，不断地乘以 2，直到能够放入该 Document：即从 32B 开始，依次增加至 64B、128B、256B… 1MB、2MB，直到能够放入该 Document。当 slot 所需空间超过 2MB 以后，slot 空间的增加策略不再是依次乘以二，而只是单纯的加 2MB：即从 2MB 开始，后面依次是 4MB、6MB… 直到能够放入该 Document 或达到 16MB 的上限。相对固定的 slot 大小保证了每个 slot 为 Document 预留了一定的增长空间，同时使得 slot 扩容时，Document 未必需要移动位置。 首先，slot 扩容是因为当前 slot 无法再放入该 Document，但 slot 的扩容未必就意味着 Document 需要移动：也许该 slot 的尾部正好跟着足够多的空闲空间，那么只要直接加大当前 slot 即可，Document 的位置无需移动，同理其相关的索引也无需修改，这样便能一定程度上较小 slot 扩容的花销，同时有效地利用空间碎片。二次幂分配机制使用相对固定的 slot 大小，使得这种情况发生的几率大大增加，因为如果假设每个 slot 的初始大小都是 n 字节，如果有一个 slot 扩容时移动到了 Collection 的尾部，那么它之前的前一个 slot 便直接获得了一个 n 字节的空余空间，当那个 slot 也需要扩容时就可以正好利用上这个空间了。 无 Padding 分配策略无 Padding，顾名思义，MongoDB 在采用该分配策略时，所有的 slot 都不会为 Document 预留任何的增长空间，slot 的大小与 Document 所需的大小完全一致。由于完全没有增长空间，每次 Document 的大小增加时，几乎是必然会导致 Document 移位，因此这样的分配策略并不适用于update操作频繁发生的 Collection。但很多情况下，Document 的大小可能确实不会变化。这意味着该 Collection 可能发生的操作只包含查询、插入、删除和不会导致大小增加的更新，如使用 $inc 令某个计数器自增。在这种情况下，Document 的移位可以说是不可能会发生，无 Padding 的分配策略最大地利用了磁盘的空间，使得 Document 与 Document 之间更加紧密，同理也使得查询操作的性能得到了提升。 MongoDB 默认情况下使用的是二次幂分配策略。要为某个 Collection 使用无 Padding 的分配策略，我们需要使用如下指令： db.runCommand({collMod: &lt;collection-name&gt;, noPadding: true}) 或使用如下指令显式创建一个新的 Collection：db.createCollection(&lt;collection-name&gt;, {noPadding: true}) MMAPv1 锁机制像 MongoDB 这样可以同时接受多个客户端发来请求的数据库系统，并发自然是个需要处理的问题，而数据同步的方法，自然是对数据文件进行加锁了。MongoDB 使用的是 Multiple-Reader-Single-Writer 锁：这意味着你可以有任意多的 Reader 同时读取数据，但这个时候其他 Writer 都会被阻塞；而当一个 Writer 获得锁时，其他所有的 Reader 和 Writer 都会被阻塞。加锁状态的转移明确了以后，还有一点需要明确的就是单次加锁的范围。 了解过 /data/db 目录下的文件的人都知道，每个 Collection 会被独立的存放在各自的区域里。除了 Collection 以外，还会有一个独立的区域保存着数据库的元数据，如索引信息等。对 Collection 的读写，冲突的发生大多数时候都存在于单个 Document 中，毫无疑问我们这时候只需要对涉及的相关 Document 加锁即可。这种加锁方式叫做以 Document 为单位的加锁（Document-wise locking）。而事实是，MMAPv1 不支持以 Document 为单位的加锁。 但这只是一方面。有时候，我们会在 Collection 以外的地方发生冲突。比如，两个操作涉及完全不同的两个 Document，但这两个操作却涉及到了同一个索引，于是在这个索引上便发生了冲突。除了索引外，如日志（Journal）等数据库元信息都是有可能发生冲突的。这个时候我们就需要比单个 Document 更大范围的锁了。 在 3.0 版本之前，MMAPv1 对锁请求的做法是，以 Database 为单位加锁（database-wise locking），对同一个 Database 的其他 Collection 所做的操作也会被阻塞。而到了 3.0 版本，MMAPv1 则开始使用以 Collection 为单位的加锁（collection-wise locking）。如此一来，MongoDB 3.0 便拥有了更好的并发性能。 更新更快：WiredTiger综合上述对 MMAPv1 的介绍，我们不难发现 MMAPv1 存在着两个缺点：1. 即使是 3.0 版本更新后的 MMAPv1 最小也只能支持到以 Collection 为单位的加锁。由于缺乏以 Document 为单位的加锁机制，这注定 MMAPv1 的并发性能比较有限；2. MMAPv1 对 Document 的 slot 的分配机制使得 Document 的移动时常发生。尽管升级后的分配策略一定程度上减少了这种操作的发生，但这种操作依然会发生，而且发生时依然会在磁盘上留下空间碎片。这使得 MMAPv1 的磁盘利用率有限。 在 2014 年的 12 月，MongoDB 正式收购了 WiredTiger 公司。之后，WiredTiger 便为 MongoDB 3.0 开发了一个专用版本的存储引擎。WiredTiger 引擎的架构和算法是和 MMAPv1 完全不同的，结果就是使用 WiredTiger 引擎的 MongoDB 比起 MMAPv1 有了极其显著的性能提升。 我们接下来逐条看一下 WiredTiger 的优点。 WiredTiger 的 Document 级锁机制我们已经见识到，MMAPv1 对于所有操作都会使用至少为 Collection 级以上的共享互斥锁机制，这样的机制会使得整个数据库系统的并发性能下降。WiredTiger 在这一点上则截然不同。在平常的使用中，大多数对数据库的更新操作都只会对某个 Collection 中的少量 Document 进行更新。对多个 Collection 进行同时更新的情况已是十分稀有，对多个 Database 进行同时更新则是更为罕见了。由此可见，加锁粒度最小只支持到 Collection 是远远不够的。相对于 MMAPv1，WiredTiger 使用的实际为 Document 级的乐观锁机制。 WiredTiger 的乐观锁机制与其他乐观锁机制实现大同小异。WiredTiger 会在更新 Document 前记录住即将被更新的所有 Document 的当前版本号，并在进行更新前再次验证其当前版本号。若当前版本号没有发生改变，则说明该 Document 在该原子事件中没有被其他请求所更新，可以顺利进行写入，并修改版本号；但如果版本号发生改变，则说明该 Document 在更新发生之前已被其他请求所更新，由此便触发了一次“写冲突”。不过，在遇到写冲突以后，WiredTiger 也会自动重试更新操作。 但这并不代表 WiredTiger 对所有操作都会使用如此松散的乐观锁机制。对于某些的全局的操作，WiredTiger 仍然会使用 Collection 级、Database 级甚至是 Instance 级的互斥锁，但这样的全局操作实际上甚少发生，通常只会在 DBA 需要对数据库进行维护时才会被触发。在产品运行的过程中，支撑应用程序的绝大多数数据库访问和修改都不属于全局操作。 WiredTiger 的压缩机制相比于 MMAPv1 只是单纯地将 BSON 数据直接存储在磁盘上，WiredTiger 则会在在数据从内存存储到磁盘前进行一次数据压缩。毫无疑问，这样的处理可以更好地利用磁盘的空间，但也为服务器带来了额外的 CPU 负荷。WiredTiger 目前使用 snappy 压缩和前缀压缩两种压缩算法，其中 snappy 是默认的用于所有 Collection 的压缩算法，而前缀压缩则默认用于对索引的压缩。 WiredTiger 的未来8 个月后的今天，MongoDB 宣布即将发布 3.2 版本，而在 3.2 版本中，WiredTiger 替代 MMAPv1 成为了 MongoDB 默认的存储引擎。至此，MMAPv1 彻底走下神坛。尽管从 3.0 到 3.2，WiredTiger 的不稳定导致它暂且不能直接用于生产环境，但相信在 MongoDB 3.2 发布后，性能几倍于 MMAPv1 的 WiredTiger 将成为大家的首选。","link":"/mongodb_storage_engine/"},{"title":"Primary-Backup Replication 总结","text":"这篇文章是本人按照 MIT 6.824 的课程安排阅读《The Design of a Practical System for Fault-Tolerant Virtual Machines》一文以及相关课程资料并总结而来。 这篇论文主要讲述了 VMware 公司如何利用一种协议来实现两个虚拟机之间的主从备份（Primary-Backup Replication）。接下来我会结合课程给出的资料给大家总结论文中的主要内容，然后会给出课程所给出的论文 FAQ 的部分翻译。 背景为实现可容错的服务器，主从备份是一种常用的解决方案：在开启了主动备份的系统中，备份服务器的状态需要在几乎任何时候都与主服务器保持一致，这样当主服务器失效后备份服务器才能立刻接管。实现主备间的状态同步主要包括以下两种方式： State Transfer（状态转移）：主服务器将状态的所有变化都传输给备份服务器。这样的方案会较为简单，但需传输的数据量较大 Replicated State Machine（备份状态机）：将需要备份的服务器视为一个确定性状态机 —— 主备以相同的状态启动，导入相同的输入，最后它们就会进入相同的状态、给出相同的输出。这样的方案较为复杂，但需要传输的数据量会小很多 VMware 选用了状态机方法，因为对于虚拟机而言，其状态过于复杂，使用状态转移方法也不会简单多少。在虚拟机上应用备份状态机方法也会有一定的顾虑 —— 我们难以保证在虚拟机上运行的应用（即操作系统）是确定性的。实际上，在物理机上应用状态机方法是极其困难的，其所能接收到的输入很多都是不确定的（如定时器中断等），但虚拟机是运行在 Hypervisor（虚拟机管理程序）之上的抽象机器，通过 Hypervisor 这一隔离层便能很好地将非确定性的输入转变为确定性的输入（笔者注：这种将非确定转为确定的思路在数据库中也很常见，参考 MongoDB oplog）。 Deterministic Replay尽管目标很直白，但考虑到虚拟机的很多输入事件本身是不确定的，如时钟中断、网络中断和磁盘中断，这会为在虚拟机上实现状态机方法带来第一个挑战。这个问题可以被细分为三个问题： 正确地捕获所有的输入事件以及它们的不确定性，以确保有足够地信息能够确定性地重放这些事件 正确地在备份虚拟机上重放这些输入事件和不确定性 保证性能不会因此降级 VMware 实现的主从备份方法名为 VMware vSphere Fault Tolerance，简称 VMware FT，它是基于 VMware Deterministic Replay 实现的。Deterministic Replay 解决了上面的前两个问题，而第三个问题则会由 VMware FT 来解决。 Deterministic Replay 会以日志记录的形式记录主服务器接收到的输入，这些日志信息则会由 VMware FT 传输到备份服务器并被重放。正如上面所提到的那样，真正的麻烦实际上来源于那些会产生不确定作用的输入。对于那些不确定的输入操作，Deterministic Replay 会记录足够多的信息，确保其在备份虚拟机上重新执行能够得到相同的状态和输出；而对于那些不确定的输入事件，如定时器、IO 操作完成中断等，Deterministic Replay 则会记录这些事件发生在哪个指令之后，这样在重放时备份服务器便能在指令流中相同的位置重放这些事件。 VMware vSphere Fault Tolerance由上节可见，Deterministic Replay 不过是一个可让虚拟机产生可重放日志信息的解决方案，单靠它是还不足以实现虚拟机主从备份的，而 VMware vSphere Fault Tolerance（简称 VMware FT）则是在使用 Deterministic Replay 为虚拟机生成日志信息的基础上实现了主备虚拟机间的同步备份。 首先我们先来看看 VMware FT 主从备份的架构： 如图所示，整个架构由一主一备组成，两个虚拟机运行在两个不同的物理机上，通过一个 Logging Channel 传输 Deterministic Replay 产生的日志信息，同时两个虚拟机都能访问一个 Shared Disk。 论文首先讲解了设计者对 VMware FT 的核心要求： Output Requirement: if the backup VM ever tasks over after a failure of the primary, the backup VM will continue executing in a way that is entirely consistent with all outputs that the primary VM has sent to the external world. 输出要求：若在主虚拟机失效且由备份虚拟机接手后，备份虚拟机必须以一种与原主虚拟机已发送到外部的输出完全一致的方式运行 可见在发生故障切换后，VMware FT 并不要求备份虚拟机一定要以与主虚拟机完全一致的方式继续运行，只需要它能够一致地处理所有已经发出的输出即可。 为了实现这个核心要求，最简单的解决方案就是延迟主虚拟机的输出操作，直到备份虚拟机已经接收到所有至少足以让它重放至该输出操作的所有日志信息。要做到这一点，简单地确保备份虚拟机已接收到该输出操作之前的所有日志信息是不够的，因为如果主虚拟机在完成输出操作后立刻失效的话，备份虚拟机在重放该输出操作前仍可能出现其他不确定的事件（如计时中断）导致备份虚拟机进入其它执行路径。 为此，VMware FT 实现了如下功能： Output Rule: the primary VM may not send an output to the external world, until the backup VM has received and acknoledged the log entry associated with the operation producing the output. 输出规则：主虚拟机必须延后将输出发送到外部世界的动作，直到备份虚拟机已经接收并 ack 与产生该输出的操作相关联的日志信息 由此，需要进行输出前，主备虚拟机间的时序关系大约是这样的： 值得注意的是，这项规则并不要求主虚拟机在接收到来自备份虚拟机的 ack 之前完全停止运行 —— 这项规则仅要求主虚拟机延迟输出操作。 由于 VMware FT 在处理与输出操作相关的日志时没有采用二阶段提交机制，VMware FT 并不保证输出是 Exactly Once 的。所幸的是，网络基础服务（如 TCP）通常能很好地处理重复的包，对磁盘同一位置的重复写入往往也不会有什么问题。 主从切换首先，VMware FT 会使用两种不同的方式来检测虚拟机节点的失效事件： 两个虚拟机所寄宿的物理机会相互之间发送 UDP 心跳信息，以判断对方是否仍存活 持续监控 Logging Channel 上的流量：由于周期定时中断的存在，主虚拟机发往备份虚拟机的日志和备份虚拟机发往主虚拟机的 ack 应该是持续不断的，信息中断即意味着节点的失效 如果备份虚拟机失效，主虚拟机就会 go live，即退出日志记录模式，不再产生和发送日志信息，并以普通的方式继续运行；如果主虚拟机失效，备份虚拟机就会在消费完 Logging Channel 中的所有日志信息后升级为主虚拟机并 go live，开始对外界发送输出。 然而上述两种方案无法避免架构在主从虚拟机间发生网络隔离时出现 Split-Brain Syndrome（裂脑综合征，指在连接大脑左右脑的胼胝体受损到一定的程度后发生的因左右脑冲突导致的症状，在分布式系统领域指一个集群中存在多个 Master 角色所带来的问题）：主从虚拟机间的网络不通会导致备份虚拟机误以为主虚拟机已经宕机而自动升级为主虚拟机，导致集群中存在两个主虚拟机。 为了解决这个问题，无论主备，虚拟机在 go live 前首先会对存储在 Shared Disk 上的一个字段进行原子的 test-and-set 操作：如果操作成功，那么它便可以 go live；否则就意味着已经有另一个虚拟机 go live，它便会立刻关闭自己（形如因自身失效而导致另一个虚拟机 go live）。 需要考虑的问题上一节中提到的只是 VMware FT 协议的核心设计。然而，为了确保该协议真实有用，还有其他的一些功能需要被添加进去。 虚拟机恢复为了确保架构的高可用，VMware FT 在发生虚拟机失效后会自动地在另一台宿主机上启动备份虚拟机。如此便涉及到了另一个问题：如何以与主虚拟机相同的状态启动一个虚拟机。论文只中提到 VMware FT 用了 VMware VMotion 的变种来实现该功能，并未详细描述这一过程。感兴趣的同学可以参考《Fast Transparent Migration for Virtual Machines》一文。 除外，VMware vSphere 还实现了一个可以监控并管理集群资源的服务，可用于选取适合启动新虚拟机的宿主机。 Logging Channel首先，VMware FT 的 Logging Channel 会在发送和接收端启用 Buffer 机制来减少网络传输对虚拟机执行速度的影响。主虚拟机会在 Buffer 满时暂停运行，而备份虚拟机则会在 Buffer 空时暂停运行。 由于 Buffer 的存在，在某一时刻上，备份虚拟机有可能落后于主虚拟机。这样的延迟必然存在，但不宜过大，源于 Buffer 中堆积的数据不仅可能导致主虚拟机时常暂停运行，还会延长主虚拟机失效后备份虚拟机消费完所有日志并 go live 的耗时。要保证 Buffer 中不会产生过多的日志堆积，必须确保备份虚拟机的性能大于等于主虚拟机。为此，VMware FT 会采集每一条日志从发送到 ack 之间的延时，并动态地通过 Hypervisor 调整主虚拟机的 CPU 频率，最终确保主虚拟机以与备份虚拟机相近的速度运行。 论文 FAQ Q: Both GFS and VMware FT provide fault tolerance. How should we think about when one or the other is better? Q：GFS 和 VMware FT 都提供了容错性。我们该怎么比较它们呢？ FT 备份的是计算，你能用它为任何已有的网络服务器提供容错性。FT 提供了相当严谨的一致性而且对客户端和服务器都是透明的。例如，你可以将 FT 应用于一个已有的邮件服务器并为其提供容错性。 相比之下，GFS 只为存储提供容错性。因为 GFS 只针对一种简单的服务提供容错性，它的备份策略会比 FT 更为高效：例如，GFS 不需要让所有的中断都以相同的顺序应用在所有的 Replica 上。GFS 通常只会被用作一个对外提供完整容错服务的系统的一部分：例如，VMware FT 本身也依赖了一个在主备虚拟机间共享的有容错性的存储服务，而你则可以用类似于 GFS 的东西来实现这个模块（虽然从细节上来讲 GFS 不太适用于 FT）。 Q: What is “an atomic test-and-set operation on the shared storage”? Q：论文中提到的“对共享存储的原子 test-and-set 操作”是什么？ 意思就是说一个计算机可以在一个原子的操作中读取并且写入指定的磁盘块。如果两个计算机都调用了这个操作，那么两组读写操作不会交织在一起，而是其中一个计算机的读写操作会在另一个计算机的读写操作完成后再执行。 Q: What happens if the primary fails just after it sends output to the external world? Q：如果主虚拟机在向外界进行输出后立刻就失效了会怎么样？ 这个输出可能会执行两次：主虚拟机一次，备份虚拟机一次。对于网络和磁盘 IO 来说，这个重复不会产生任何问题。如果输出的是一个网络包，那么接收端的 TCP 栈会丢弃掉这个重复的包；如果是磁盘 IO，那么磁盘 IO 实际上是幂等的（两次操作会在同一个位置写入相同的数据，这之间也不会有其他的 IO）。 Q: Is it reasonable to address only the fail-stop failures? What are other type of failures? Q：只去解决宕机失效的场景是合理的吗？还存在其他类型的失效吗？ 这是合理的，因为现实世界中的多数失效本质上来讲都是宕机失效，例如各种网络失效和电源失效。要做到更好的话就需要处理那些似乎正在正常运行但其实正在产生错误结果的计算机 —— 在最坏的情况下，这样的失效可能是来源于一个恶意的攻击者。这类非宕机失效的失效通常被称为“拜占庭”（Byzantine）。我们实际上是有方法去应对拜占庭失效的，我们会在这节课的末尾学习这些方法，但 6.824 的主要内容还是关于宕机失效。 译者注：关于更多类型的失效，可参考《Failure Modes in Distributed Systems》 一文。 结语鉴于虚拟机和计算机底层原理不是我所感兴趣的内容，论文 3.3 节以后的内容我也只是简单地浏览了一遍，这篇文章中也没有总结这些内容，有兴趣的读者可自行查阅论文的原文。 尽管如此，此文仍然是主从备份话题的一次不错的 case study，也确实系统地描述和对比了 Primary-Backup Replication 两种常见的解决思路。稍微了解过市面上常用的数据库系统的同学就会了解，主从备份可以说是数据库系统必备的高可用方案之一。论文第一章的背景部分讲述了目前常用的两种主从备份方法，课程资料中也稍微对比了一下两种方案的优缺点，尽管目前常见的数据库备份方式所使用的几乎都是这两种方案的结合（如 Redis、MongoDB）。即便如此，通过此次 case 来系统地学习主从备份的相关理论也是十分不错的。","link":"/primary-backup-replication/"},{"title":"Spark RDD 论文简析","text":"遥想我第一次参加实习的时候，我接手的第一个项目便是 Spark 插件的开发。当时为了做好这个工作，自己看了 Spark RDD 和 SparkSQL 的论文，还在阅读 Spark 源码的同时写了好多 Spark 源码分析的文章。过去了那么久，现在便趁着学习 MIT 6.824 的机会，再来整理一下 Spark RDD 论文的内容吧。 本文由我按照 MIT 6.824 的课程安排阅读 Spark RDD 的论文以及相关课程资料并总结而来，内容会更偏向于从科研的角度介绍 Spark RDD 诞生时所需要解决的问题以及对其基本工作方式的简单介绍。 背景在 Apache Spark 广泛使用以前，业界主要使用 Hadoop MapReduce 来对大数据进行分布式处理。诚然 Hadoop MapReduce 为企业及组织利用大量普通消费级机器组建集群进行数据处理成为可能，但随着需求的不断扩展，Hadoop MapReduce 也存在着这样那样的局限： MapReduce 编程模型的表达能力有限，仅靠 MapReduce 难以实现部分算法 对分布式内存资源的使用方式有限，使得其难以满足最近大量出现的需要复用中间结果的计算流程，包括： 如迭代式机器学习算法及图算法 交互式数据挖掘 Spark RDD 作为一个分布式内存资源抽象便致力于解决 Hadoop MapReduce 的上述问题： 通过对分布式集群的内存资源进行抽象，允许程序高效复用已有的中间结果 提供比 MapReduce 更灵活的编程模型，兼容更多的高级算法 接下来我们便详细说说 Spark RDD 是如何达成上述目标的。 RDD：分布式内存资源抽象RDD（Resilient Distributed Dataset，弹性分布式数据集）本质上是一种只读、分片的记录集合，只能由支持的所数据源或是由其他 RDD 经过一定的转换（Transformation）来产生。通过由用户构建 RDD 间组成的产生关系图，每个 RDD 都能记录到自己是如何由还位于持久化存储中的源数据计算得出的，即其血统（Lineage）。 相比于 RDD 只能通过粗粒度的“转换”来创建（或是说写入数据），分布式共享内存（Distributed Shared Memory，DSM）是另一种分布式系统常用的分布式内存抽象模型：应用在使用分布式共享内存时可以在一个全局可见的地址空间中进行随机的读写操作。类似的系统包括了一些常见的分布式内存数据库（如 Redis、Memcached）。RDD 产生的方式限制了其只适用于那些只会进行批量数据写入的应用程序，但却使得 RDD 可以使用更为高效的高可用机制。 除了 Transformation 以外，Spark 还为 RDD 提供了 Action，可对 RDD 进行计算操作并把一个结果值返回给客户端，或是将 RDD 里的数据写出到外部存储。 Transformation 与 Action 的区别还在于，对 RDD 进行 Transformation 并不会触发计算：Transformation 方法所产生的 RDD 对象只会记录住该 RDD 所依赖的 RDD 以及计算产生该 RDD 的数据的方式；只有在用户进行 Action 操作时，Spark 才会调度 RDD 计算任务，依次为各个 RDD 计算数据。 RDD 具体实现与计算调度前面我们提到，RDD 在物理形式上是分片的，其完整数据被分散在集群内若干机器的内存上。当用户通过 Transformation 创建出新的 RDD 后，新的 RDD 与原本的 RDD 便形成了依赖关系。根据用户所选 Transformation 操作的不同，RDD 间的依赖关系可以被分为两种： 窄依赖（Narrow Dependency）：父 RDD 的每个分片至多被子 RDD 中的一个分片所依赖 宽依赖（Wide Dependency）：父 RDD 中的分片可能被子 RDD 中的多个分片所依赖 通过将窄依赖从宽依赖中区分出来，Spark 便可以针对 RDD 窄依赖进行一定的优化。首先，窄依赖使得位于该依赖链上的 RDD 计算操作可以被安排到同一个集群节点上流水线进行；其次，在节点失效需要恢复 RDD 时，Spark 只需要恢复父 RDD 中的对应分片即可，恢复父分片时还能将不同父分片的恢复任务调度到不同的节点上并发进行。 总的来说，一个 RDD 由以下几部分组成： 其分片集合 其父 RDD 集合 计算产生该 RDD 的方式 描述该 RDD 所包含数据的模式、分片方式、存储位置偏好等信息的元数据 在用户调用 Action 方法触发 RDD 计算时，Spark 会按照定义好的 RDD 依赖关系绘制出完整的 RDD 血统图，并根据图中各节点间依赖关系的不同对计算过程进行切分： 简单来说，Spark 会把尽可能多的可以流水线执行的窄依赖 Transformation 放到同一个 Job Stage 中，而 Job Stage 之间则要求集群对数据进行 Shuffle。Job Stage 划分完毕后，Spark 便会为每个 Partition 生成计算任务（Task）并调度到集群节点上运行。 在调度 Task 时，Spark 也会考虑计算该 Partition 所需的数据的位置：例如，如果 RDD 是从 HDFS 中读出数据，那么 Partition 的计算就会尽可能被分配到持有对应 HDFS Block 的节点上；或者，如果 Spark 已经将父 RDD 持有在内存中，子 Partition 的计算也会被尽可能分配到持有对应父 Partition 的节点上。对于不同 Job Stage 之间的 Data Shuffle，目前 Spark 采取与 MapReduce 相同的策略，会把中间结果持久化到节点的本地存储中，以简化失效恢复的过程。 当 Task 所在的节点失效时，只要该 Task 所属 Job Stage 的父 Job Stage 数据仍可用，Spark 只要将该 Task 调度到另一个节点上重新运行即可。如果父 Job Stage 的数据也已经不可用了，那么 Spark 就会重新提交一个计算父 Job Stage 数据的 Task，以完成恢复。有趣的是，从论文来看，Spark 当时还没有考虑调度模块本身的高可用，不过调度模块持有的状态只有 RDD 的血统图和 Task 分配情况，通过状态备份的方式实现高可用也是十分直观的。 结语总的来说，Spark RDD 的亮点在于如下两点： 确定且基于血统图的数据恢复重计算过程 面向记录集合的转换 API 比起类似于分布式内存数据库的那种分布式共享内存模型，Spark RDD 巧妙地利用了其不可变和血统纪录的特性实现了对分布式内存资源的抽象，很好地支持了批处理程序的使用场景，同时大大简化了节点失效后的数据恢复过程。 同时，我们也应该意识到，Spark 是对 MapReduce 的一种补充而不是替代：将那些能够已有的能够很好契合 MapReduce 模型的计算作业迁移到 Spark 上不会收获太多的好处（例如普通的 ETL 作业）。除外，RDD 本身在 Spark 生态中也渐渐变得落伍，Spark 也逐渐转向使用从 SparkSQL 开始引入的 DataFrame 模型了。后续有时间的话我也许也会再总结一下 SparkSQL 的论文。 不管怎么说，Spark RDD 依然是通过很简单的方式解决了大数据计算领域中的一大痛点，阅读其论文也是一次相当不错的 Case Study。","link":"/spark-rdd/"},{"title":"Spark SQL 论文简述","text":"先前在读过 Spark RDD 的论文后，我从 MIT 6.824 的课程笔记中了解到，RDD 在 Apache Spark 中已经不那么常用，开发重心渐渐转移到了 Spark SQL 的 DataFrame API 上。在我第一次实习的时候其实也有读过 Spark SQL 的论文，那这次就重新读读这篇论文，总结一下吧。 背景如 Hadoop MapReduce、Spark RDD 等分布式计算引擎所提供的编程接口尽管有效，但都相对底层、过程式，用户要让自己的程序有较好的性能需要自行进行较为复杂的优化。由此，如 Hive、Pig 等新系统开始选择通过暴露如 SQL 等声明式的查询接口来让程序的自动优化变得可能。 仅仅使用 SQL 进行查询是无法满足所有需求的。常见的不足包括以下两点： 用户会需要对大量的数据进行 ETL 操作，而这些源数据往往只是半结构化甚至是非结构化的，这就使得用户需要能够自行编写代码 SQL 查询无法表达更为复杂的计算逻辑，如机器学习算法和图处理算法等 实际上，Spark SQL 并不只是 SQL-on-Spark 的定位 —— 更早之前已有一个叫做 Shark 的框架做了这件事。为了解决以上问题，Spark SQL 的设计目标包括以下： 通过程序员友好的 API 提供针对 RDD 及外部数据源的关系型处理 利用已有的 DBMS 技术提供较高的处理性能 支持更多的外部数据源，包括半结构化与非结构化的数据源 支持对高级分析算法的扩展，包括图处理算法及机器学习算法 为了实现以上目标，Spark SQL 主要借助了两大核心组件：DataFrame API 和 Catalyst。 DataFrame APIDataFrame 是由拥有统一结构的记录所组成的集合，逻辑上等价于关系型数据库中的一张表。与 RDD 不同的地方在于，DataFrame 还会记录数据的模式信息。 DataFrame 的 API 支持过去 Spark 已有的过程式 API，还加入了新的关系型操作 API，如 select、groupBy 等。用户可以通过编写代码使用 DataFrame API，也可以通过 JDBC/ODBC 等方式启动 Spark SQL 查询。 在实现上，每个 DataFrame 对象都代表着计算出对应数据集的 Logical Plan。和 RDD 类似，DataFrame 的数据计算也是延后的，这使得 Spark SQL 能够针对构建 DataFrame 的完整操作链对计算进行优化。最终，DataFrame 会经过 Catalyst 优化后转换为对应的 RDD 计算流程，使用已有的 Spark RDD 计算引擎完成计算。 除外，由于 DataFrame 拥有数据模式信息，在对 DataFrame 进行缓存时，Spark SQL 会将数据转换为更加紧凑的列式格式，相比于 RDD 直接缓存 JVM 对象有着小得多的内存占用。 CatalystCatalyst 是一个可扩展的查询优化器。从上面的 Spark SQL 架构图可以看出，DataFrame 对应的 Logical Plan 会经过 Catalyst 进行优化、转换，最终变成对应的 RDD Physical Plan，再提交到 Spark 上进行计算。 Catalyst 对 DataFrame 的查询优化可以分为以下几个步骤： 首先，用户通过 DataFrame API 层层构建出的 DataFrame 对象中的信息可以构建出计算出对应数据的操作的 AST（Abstract Syntax Tree，抽象语法树）。如表达式 x + (1 + 2) 即可解析为如下 AST： Catalyst 的查询优化从用户通过 Scala 代码构建的 DataFrame 对象或是 SQL 解析器返回的 AST 开始。该输入结构被统称为 Unresolved Logical Plan，源于树中部分对属性/表的引用是未解析的（Unresolved，即 Spark 暂不知晓其所引用的属性、表为何）。那么第一步，Analysis，便是借助 Spark SQL 的 Catalog 对象以及其他外部数据源中存储的信息对这些引用进行解析。解析后便得到了 Logical Plan。 此时的 Logical Plan 代表的便是计算出 DataFrame 所指带数据的逻辑执行计划，其中已载入所有源数据的模式信息。Catalyst 的下一项处理便是 Logical Optimization，对该逻辑执行计划进行优化。 Catalyst 对逻辑执行计划所进行的优化过程是 Rule-based 的：Catalyst 会不断地递归整棵 Logical Plan 树，将其结构与预设的优化规则进行匹配，并对匹配的子树进行对应的转换。Catalyst 所使用的优化规则在其他关系型数据库中也十分常见，包括常量合并、条件过滤下推等。 值得一提的是，Catalyst 此部分的实现代码大量使用了 Scala 的 Pattern Matching 特性，使得优化规则的表达变得十分简洁清晰。这样的特性也使得其他代码贡献者能够以更低成本对 Catalyst 进行扩展，加入更多优化规则。 第三步，Physical Planning，Catalyst 便会将 Optimized Logical Plan 转换为若干个对应的 Physical Plan，并根据它们的计算代价选出最优的 Physical Plan（Cost-based 优化）：例如，对于 JOIN 操作，此步就会根据左右表大小的不同选取出不同的 JOIN 算法。 除外，Catalyst 在 Physical Planning 阶段会利用更多由外部数据源提供的统计信息，除了源数据的预计大小外还有外部数据源所支持的操作等。有了这些信息，Catalyst 在该阶段也会进行一些额外的 Rule-based 优化，应用那些能够利用这些信息的优化规则，例如将条件过滤、字段投影等操作下推到外部数据源中执行。 最后，Catalyst 会对选取出的最优 Physical Planning 使用 Scala 提供的 Quasiquotes 功能，生成出对应的 RDD 计算代码，并提交到 Spark 引擎上执行。实际上，Spark SQL 大可以将整棵 Physical Planning 树提交执行，但 Catalyst 还是选择了转换为对应的 Scala 代码后再提交，最主要的的原因是这使得计算时不会因为树的层级产生多余的中间对象，极大地提高了计算的效率。从 Spark SQL 论文中展示的性能对比来看，代码生成使得程序在执行时的效率与人类手写对应 Scala 代码的效率相近，远高于直接执行未转换的操作树。 文中还提及了很多使用 Scala Quasiquotes 所带来的好处，可惜笔者并未学习过 Scala 的这项功能，无法理解这部分细节。感兴趣的读者可自行查阅原文。 结语总的来说，Spark SQL 的论文并不像是一篇学术论文，文中大量描述了 Spark SQL 的实现细节，更像是一篇工程论文。原文提及了很多工程实现上的取舍及原因，如 Scala 作为函数式语言天生适合用于编写编译器，以及 Catalyst 巧用 Scala 的特性使得其他代码贡献者能够很方便地为 Catalyst 添加更多的优化规则等。实际上，Catalyst 作为 Scala 仅有的一个产品级查询优化器（据论文所说），其设计还是很值得我们去学习的。","link":"/spark-sql/"},{"title":"Spark Catalyst 源码解析：Intro","text":"我的上一个系列的 SparkSQL 源码解析已经完结了一段时间了。当时我出于实习工作的需要阅读了 SparkSQL HiveThriftServer 以及 Spark Scala Interpreter 的源代码，并顺势写下了那个系列的源码解析文章。但读 SparkSQL 源代码怎么能只读那些外围插件的源代码呢？于是我又开一个新坑了。 在上一个系列中也提到过，SparkSQL 实际上由 4 个项目组成，分别为 Spark Core、Spark Catalyst、Spark Hive 和 Spark Hive ThriftServer。这个系列的文章所要介绍的是 Spark Catalyst 项目。它在 SparkSQL 中担任的角色是优化器。这个系列的文章我将会按照标准的 SparkSQL 执行流程来解析源代码，因此文章中将不可避免地出现 Spark Core 的部分代码。 正如我所讲，本系列文章将按照 SparkSQL 的执行顺序来讲解代码，但很多人可能并不了解自己在调用了 SQLContext#sql 以后到底会发生什么。因此在阅读本文之前，我强烈建议各位先看一下这篇论文。这篇论文是 SparkSQL 的官方论文，其中提到了 SparkSQL Catalyst 的执行流程。通过完整阅读这篇论文并掌握其中出现的一些专属名词，将对你接下来的代码阅读工作大有裨益。 本文使用的是当下最新的 1.4.1 版本的 Spark。在该版本中，SparkSQL 的版本号为 2.10。 SQLContext毋庸置疑，一切的一切都从 SQLContext#sql 开始。不过，我们先来看看 SQLContext 这个类都包含了些什么变量。 123456789101112131415161718192021222324252627282930313233343536class SQLContext(@transient val sparkContext: SparkContext) extends org.apache.spark.Logging with Serializable {self =&gt; // ... @transient protected[sql] lazy val catalog: Catalog = new SimpleCatalog(conf) @transient protected[sql] lazy val analyzer: Analyzer = new Analyzer(catalog, functionRegistry, conf) { override val extendedResolutionRules = ExtractPythonUdfs :: sources.PreInsertCastAndRename :: Nil override val extendedCheckRules = Seq( sources.PreWriteCheck(catalog) ) } @transient protected[sql] lazy val optimizer: Optimizer = DefaultOptimizer @transient protected[sql] val ddlParser = new DDLParser(sqlParser.parse(_)) @transient protected[sql] val sqlParser = new SparkSQLParser(getSQLDialect().parse(_)) @transient protected[sql] val planner = new SparkPlanner // ...} 上述代码当然不是 SQLContext 的全部变量，但我们暂时只需要看到这些。首先 catalog 变量只要是看过论文的读者自然是不会陌生了，它用来存放所有 SQLContext 已经知晓的表，在对 Attribute、Relation 等进行 resolve 的时候就需要利用 Catalog 提供的信息。剩余的五个变量中我们看到了 4 个角色，分别为 Parser、Analyzer、Optimizer、Planner。同样，在论文中已经提及到了这些角色的作用，其中 parser 负责把用户输入的 SQL 语句进行解释，转变为 Unresolved Logical Plan。Unresolved Logical Plan 中会包含 SQL 语句中出现的变量名和表名，这些词素暂时来讲都会被标记为 unresolved，即“不知道是否存在这个表”或“不知道表中是否有这个字段”。这个时候轮到 Analyzer 登场，它利用 Catalog 提供的信息，对所有这些 unresolved 的词素进行 resolve，并在 resolve 失败时抛出错误。结束后便得到了 Analyzed Logical Plan。接下来轮到 Optimizer，它使用 rule-based 的优化规则对传入的 Analyzed Logical Plan 进行优化，得到一个 Optimized Logical Plan。最终 Optimized Logical Plan 传入到 Planner，生成物理执行计划，得到 Physical Plan。 这么多的废话，其实就变成这样一张图： 这就是 SparkSQL 的基本执行流程，一切由 SQLContext#sql 开始。那我们就先来看看起点吧： 123456def sql(sqlText: String): DataFrame = { DataFrame(this, parseSql(sqlText))} // 调用 parseSql 方法将传入的 sql 语句转变为了 unresolved logical plan，并用来实例化了一个 DataFrame// 调用了 ddlParser 的 parse 函数来解析传入的 sql 语句protected[sql] def parseSql(sql: String): LogicalPlan = ddlParser.parse(sql, false) 好，到此为止，Parser 的代码我们留到下次。我们先去看看 DataFrame 的那个构造函数都做了什么： 123456789101112131415161718192021222324class DataFrame private[sql]( @transient val sqlContext: SQLContext, @DeveloperApi @transient val queryExecution: SQLContext#QueryExecution) extends RDDApi[Row] with Serializable { /** * A constructor that automatically analyzes the logical plan. * * This reports error eagerly as the [[DataFrame]] is constructed, unless * [[SQLConf.dataFrameEagerAnalysis]] is turned off. */ def this(sqlContext: SQLContext, logicalPlan: LogicalPlan) = { this(sqlContext, { val qe = sqlContext.executePlan(logicalPlan) if (sqlContext.conf.dataFrameEagerAnalysis) { qe.assertAnalyzed() // This should force analysis and throw errors if there are any } qe }) } // ... } 如此看来，尽管传入的 LogicalPlan 仍然是个 unresolved logical plan，但 DataFrame 的这个构造函数立马就触发了 analyze 操作，并返回了一个 SQLContext#QueryExecution 类。我们就来看看 SQLContext 的这个内部类吧： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * :: DeveloperApi :: * The primary workflow for executing relational queries using Spark. Designed to allow easy * access to the intermediate phases of query execution for developers. */ @DeveloperApi protected[sql] class QueryExecution(val logical: LogicalPlan) { def assertAnalyzed(): Unit = analyzer.checkAnalysis(analyzed) // Unresolved Logical Plan -&gt; Analyzed Logical Plan -&gt; Optimized Logical Plan // -&gt; Physical Plan -&gt; Executed Physical Plan -&gt; RDD // 分析 unresolved 的 LogicalPlan，得到 Analyzed Logical Plan // Unresolved Logical Plan -&gt; Analyzed Logical Plan lazy val analyzed: LogicalPlan = analyzer.execute(logical) // 将 LogicalPlan 中的结点尽可能地替换为 cache 中的结果，得到 Analyzed Logical Plan with Cached Data lazy val withCachedData: LogicalPlan = { assertAnalyzed() cacheManager.useCachedData(analyzed) } // 对 Analyzed Logical Plan with Cached Data 进行优化，得到 Optimized Logical Plan // Analyzed Logical Plan -&gt; Optimized Logical Plan lazy val optimizedPlan: LogicalPlan = optimizer.execute(withCachedData) // 生成 PhysicalPlan // Optimized Logical Plan -&gt; Physical Plan lazy val sparkPlan: SparkPlan = { SparkPlan.currentContext.set(self) planner.plan(optimizedPlan).next() } // executedPlan should not be used to initialize any SparkPlan. It should be // only used for execution. // 准备好的 PhysicalPlan lazy val executedPlan: SparkPlan = prepareForExecution.execute(sparkPlan) /** Internal version of the RDD. Avoids copies and has no schema */ // 执行并返回结果 lazy val toRdd: RDD[Row] = executedPlan.execute() protected def stringOrError[A](f: =&gt; A): String = try f.toString catch { case e: Throwable =&gt; e.toString } def simpleString: String = s&quot;&quot;&quot;== Physical Plan == |${stringOrError(executedPlan)} &quot;&quot;&quot;.stripMargin.trim override def toString: String = { def output = analyzed.output.map(o =&gt; s&quot;${o.name}: ${o.dataType.simpleString}&quot;).mkString(&quot;, &quot;) // TODO previously will output RDD details by run (${stringOrError(toRdd.toDebugString)}) // however, the `toRdd` will cause the real execution, which is not what we want. // We need to think about how to avoid the side effect. s&quot;&quot;&quot;== Parsed Logical Plan == |${stringOrError(logical)} |== Analyzed Logical Plan == |${stringOrError(output)} |${stringOrError(analyzed)} |== Optimized Logical Plan == |${stringOrError(optimizedPlan)} |== Physical Plan == |${stringOrError(executedPlan)} |Code Generation: ${stringOrError(executedPlan.codegenEnabled)} |== RDD == &quot;&quot;&quot;.stripMargin.trim }} 从类的注释上就能看到，SQLContext#QueryExecution 这个类包含了一次 SQL 查询的整个生命周期，从 unresolved 到 analyzed 到 optimized 到 physical 到 RDD，全都包含在了一个类中，开发者也可以很方便地通过这一个类对整个计算过程进行监控。DataFrame 的构造函数通过调用该类的 assertAnalyzed方法，触发了 sqlContext.analyzer 对 logicalPlan 变量的 analyze 操作。光是调用 SQLContext#sql 方法，Logical Plan 的 Analysis 步骤就已经完成了。 总结知道 SQLContext#QueryExecution 这样一个类的存在对我们以后的代码阅读工作将带来大量的好处。下一次我将从 SparkSQL 的第一个步骤：Parse 进行讲解，敬请期待。","link":"/sparksql_catalyst_source_1/"},{"title":"Spark Catalyst 源码解析：Parser","text":"在上一篇文章中，我们了解了 SparkSQL 查询的基本执行过程，并了解到 SQLContext 的内部类 QueryExecution 包含了整个执行过程的每一个执行步骤。 在这篇文章中，我将开始讲解 SQL 语句如何通过 Parser 转变为 Unresolved Logical Plan。 DDLParser我们回到 SQLContext#parseSql 方法： 1234567@transientprotected[sql] val ddlParser = new DDLParser(sqlParser.parse(_))@transientprotected[sql] val sqlParser = new SparkSQLParser(getSQLDialect().parse(_))protected[sql] def parseSql(sql: String): LogicalPlan = ddlParser.parse(sql, false) 可以看到，parseSql 方法调用了 ddlParser 的 parse 方法。ddlParser 在初始化时传入了 sqlParser.parse 方法作为参数，而 sqlParser 在初始化时也传入了一个 SQL 方言的 parse 方法作为参数。这三个 parse 之间很有可能是一个 fallback 的关系。那我们先来看看 DDLParser： 123456789101112131415161718192021/** * A parser for foreign DDL commands. */private[sql] class DDLParser(parseQuery: String =&gt; LogicalPlan) extends AbstractSparkSQLParser with DataTypeParser with Logging { def parse(input: String, exceptionOnError: Boolean): LogicalPlan = { try { // 先尝试用 AbstractSparkSQLParser#parse 进行解析 parse(input) } catch { case ddlException: DDLException =&gt; throw ddlException // 解析失败则使用传入的解析函数 parseQuery 进行解析 case _ if !exceptionOnError =&gt; parseQuery(input) case x: Throwable =&gt; throw x } } // ...} 先不急着往下看，因为这里调用了 AbstractSparkSQLParser 的 parse 方法。我们先看看 AbstractSparkSQLParser： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849private[sql] abstract class AbstractSparkSQLParser extends StandardTokenParsers with PackratParsers { def parse(input: String): LogicalPlan = { // 将 Keyword 们作为保留字放入到 lexical 变量中 initLexical // 开始解释传入的字符串 phrase(start)(new lexical.Scanner(input)) match { case Success(plan, _) =&gt; plan case failureOrError =&gt; sys.error(failureOrError.toString) } } /* One time initialization of lexical.This avoid reinitialization of lexical in parse method */ protected lazy val initLexical: Unit = lexical.initialize(reservedWords) protected case class Keyword(str: String) { def normalize: String = lexical.normalizeKeyword(str) def parser: Parser[String] = normalize } protected implicit def asParser(k: Keyword): Parser[String] = k.parser // 通过反射机制将类的所有返回 Keyword 类型的函数结果注册为保留字（reserved word） protected lazy val reservedWords: Seq[String] = this.getClass .getMethods .filter(_.getReturnType == classOf[Keyword]) .map(_.invoke(this).asInstanceOf[Keyword].normalize) // Set the keywords as empty by default, will change that later. // SQL 词素 override val lexical = new SqlLexical protected def start: Parser[LogicalPlan] // Returns the whole input string protected lazy val wholeInput: Parser[String] = new Parser[String] { def apply(in: Input): ParseResult[String] = Success(in.source.toString, in.drop(in.source.length())) } // Returns the rest of the input string that are not parsed yet protected lazy val restInput: Parser[String] = new Parser[String] { def apply(in: Input): ParseResult[String] = Success( in.source.subSequence(in.offset, in.source.length()).toString, in.drop(in.source.length())) }} 我们看到，真正启动 parse 过程的实际上是如下代码块： 1234phrase(start)(new lexical.Scanner(input)) match { case Success(plan, _) =&gt; plan case failureOrError =&gt; sys.error(failureOrError.toString)} 这里调用的 phrase 方法实际上来自于 AbstractSparkSQLParser 的父类 PackratParsers。PackratParsers 和 StandardTokenParsers 实际上都是 Scala 自带的类。它们的功能较为复杂，而且 SparkSQL 本身的作用原理关系并不是很大，我在这里就简单讲述一下。 1234567891011121314151617// PackratParsers.scala/** * A parser generator delimiting whole phrases (i.e. programs). * * Overridden to make sure any input passed to the argument parser * is wrapped in a `PackratReader`. */override def phrase[T](p: Parser[T]) = { val q = super.phrase(p) new PackratParser[T] { def apply(in: Input) = in match { case in: PackratReader[_] =&gt; q(in) case in =&gt; q(new PackratReader(in)) } }} 可以看到，PackratParsers#phrase 方法接受一个 Parser 作为参数，并以其为参数调用了其父类 Parsers 的 phrase 方法，该方法同样返回一个 Parser。而后，PackratParsers#phrase 返回了一个 PackratParser，由 AbstractSparkSQLParser 调用这个对象的 apply 方法传入 SQL 语句。 我们回到 DDLParser： 1234567891011121314151617181920212223242526272829private[sql] class DDLParser(parseQuery: String =&gt; LogicalPlan) extends AbstractSparkSQLParser with DataTypeParser with Logging { def parse(input: String, exceptionOnError: Boolean): LogicalPlan = { // ... } // 这些 keyword 会在 initLexical 时被加载 protected val CREATE = Keyword(&quot;CREATE&quot;) protected val TEMPORARY = Keyword(&quot;TEMPORARY&quot;) protected val TABLE = Keyword(&quot;TABLE&quot;) protected val IF = Keyword(&quot;IF&quot;) protected val NOT = Keyword(&quot;NOT&quot;) protected val EXISTS = Keyword(&quot;EXISTS&quot;) protected val USING = Keyword(&quot;USING&quot;) protected val OPTIONS = Keyword(&quot;OPTIONS&quot;) protected val DESCRIBE = Keyword(&quot;DESCRIBE&quot;) protected val EXTENDED = Keyword(&quot;EXTENDED&quot;) protected val AS = Keyword(&quot;AS&quot;) protected val COMMENT = Keyword(&quot;COMMENT&quot;) protected val REFRESH = Keyword(&quot;REFRESH&quot;) protected lazy val ddl: Parser[LogicalPlan] = createTable | describeTable | refreshTable protected def start: Parser[LogicalPlan] = ddl // ... } 在接下来的代码中，AbstractSparkSQLParser 实现了三个 parser：createTable 、 describeTable 和 refreshTable，并将其重载为 AbstractSparkSQLParser#start 变量，由此 DDLParser 改变了 AbstractSparkSQLParser#start 的功能。 上述的这些 Keyword 全都是 Spark 所支持的 DLL keyword，没有包含 SQL 的保留字。不难想象 DDLParser 仅用于解析 DDL 语句，当遇到 SQL 语句时，解析器将 fallback 到实例化 DDLTask 时传入的 parseQuery 函数，而这个函数正是 SparkSQLParser#parse 函数。 通过查看 SparkSQLParser 的源代码，可以有如下发现： 123456789101112131415161718192021222324/** * The top level Spark SQL parser. This parser recognizes syntaxes that are available for all SQL * dialects supported by Spark SQL, and delegates all the other syntaxes to the `fallback` parser. * * @param fallback A function that parses an input string to a logical plan */private[sql] class SparkSQLParser(fallback: String =&gt; LogicalPlan) extends AbstractSparkSQLParser { // ... protected val AS = Keyword(&quot;AS&quot;) protected val CACHE = Keyword(&quot;CACHE&quot;) protected val CLEAR = Keyword(&quot;CLEAR&quot;) protected val IN = Keyword(&quot;IN&quot;) protected val LAZY = Keyword(&quot;LAZY&quot;) protected val SET = Keyword(&quot;SET&quot;) protected val SHOW = Keyword(&quot;SHOW&quot;) protected val TABLE = Keyword(&quot;TABLE&quot;) protected val TABLES = Keyword(&quot;TABLES&quot;) protected val UNCACHE = Keyword(&quot;UNCACHE&quot;) // ... } 从注释上看，SparkSQLParser 用于解析所有 SparkSQL 所支持的 SQL 方言所共有的关键字。当该解析器失败时，将会继续 fallback 到当时在 SQLContext 传入的 getSQLDialect().parse(_)，使用某个特定的 SQL 方言进行解析。 总结个人认为大多数人应该不会太在意 Parser 的原理，毕竟没什么人会需要去修改 SparkSQL 语句的解析逻辑，因此这一篇文章只能算是抛砖引玉，真正的解析逻辑还有待你们自己去发掘。在下一篇文章中我会先为大家讲解一下 LogicalPlan 的数据结构，敬请期待。","link":"/sparksql_catalyst_source_2/"},{"title":"Spark Catalyst 源码解析：Analyzer 与 Optimizer","text":"在上一篇文章中，我们详细了解了 SparkSQL 中特殊的 TreeNode 们以及核心类 LogicalPlan，完整理解了整个执行计划树的组成。 在这篇文章中，我将开始讲解 Unresolved Logical Plan 如何通过 Analyzer 转变为 Analyzed Logical Plan，再通过 Optimizer 转变为 Optimized Logical Plan。 Analyzer我们先来看看 SQLContext 为我们默认设置的 analyzer 吧： 123456789101112@transientprotected[sql] lazy val analyzer: Analyzer = new Analyzer(catalog, functionRegistry, conf) { override val extendedResolutionRules = ExtractPythonUdfs :: sources.PreInsertCastAndRename :: Nil override val extendedCheckRules = Seq( sources.PreWriteCheck(catalog) ) } 可以看到，SQLContext 通过匿名内部类的方式创建了一个 Analyzer 的子类实例。那我们就去看看 Analyzer 吧： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293/** * Provides a logical query plan analyzer, which translates [[UnresolvedAttribute]]s and * [[UnresolvedRelation]]s into fully typed objects using information in a schema [[Catalog]] and * a [[FunctionRegistry]]. */class Analyzer( catalog: Catalog, registry: FunctionRegistry, conf: CatalystConf, maxIterations: Int = 100) extends RuleExecutor[LogicalPlan] with HiveTypeCoercion with CheckAnalysis { // ... val fixedPoint = FixedPoint(maxIterations) /** * Override to provide additional rules for the &quot;Resolution&quot; batch. */ val extendedResolutionRules: Seq[Rule[LogicalPlan]] = Nil lazy val batches: Seq[Batch] = Seq( Batch(&quot;Substitution&quot;, fixedPoint, CTESubstitution :: WindowsSubstitution :: Nil : _*), Batch(&quot;Resolution&quot;, fixedPoint, ResolveRelations :: ResolveReferences :: ResolveGroupingAnalytics :: ResolveSortReferences :: ResolveGenerate :: ResolveFunctions :: ExtractWindowExpressions :: GlobalAggregates :: UnresolvedHavingClauseAttributes :: TrimGroupingAliases :: typeCoercionRules ++ extendedResolutionRules : _*) ) object CTESubstitution extends Rule[LogicalPlan] { // ... } object WindowsSubstitution extends Rule[LogicalPlan] { // ... } object TrimGroupingAliases extends Rule[LogicalPlan] { // ... } object ResolveGroupingAnalytics extends Rule[LogicalPlan] { // ... } object ResolveRelations extends Rule[LogicalPlan] { // ... } object ResolveReferences extends Rule[LogicalPlan] { // ... } object ResolveSortReferences extends Rule[LogicalPlan] { // ... } object ResolveFunctions extends Rule[LogicalPlan] { // ... } object GlobalAggregates extends Rule[LogicalPlan] { // ... } object UnresolvedHavingClauseAttributes extends Rule[LogicalPlan] { // ... } object ResolveGenerate extends Rule[LogicalPlan] { // ... } object ExtractWindowExpressions extends Rule[LogicalPlan] { // ... }}object EliminateSubQueries extends Rule[LogicalPlan] { // ...} 关于上述这个类，我们可以把目光放在如下几个点。首先 batches 变量内包含了两个 Batch 实例，分别被命名为了 “Substitution” 和 “Resolvation”。创建 Batch 实例的时候传入了大量的 Rule 子类，而 Analyzer 本身继承自 RuleExecutor。 RuleExecutor那么我们不妨先来看一下 RuleExecutor： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081abstract class RuleExecutor[TreeType &lt;: TreeNode[_]] extends Logging { // 执行策略，定义了 maxIterations。 // 我们知道 Optimize 的过程需要不断地重复迭代，Analyze 的过程也一样。 // 由此可见 Analyze 迭代停止的条件有两个： // 1. 达到 Strategy 指定的最大迭代数，或 // 2. 达到 fixed point（不动点，在数学中即指满足 f(x) = x 的 x） abstract class Strategy { def maxIterations: Int } case object Once extends Strategy { val maxIterations = 1 } case class FixedPoint(maxIterations: Int) extends Strategy // 之前的 Batch 类出现在了这里 protected case class Batch(name: String, strategy: Strategy, rules: Rule[TreeType]*) // 由子类定义的需要执行的 Rule 们 protected val batches: Seq[Batch] // 在传入的 plan 上迭代地执行由子类定义的 batch def execute(plan: TreeType): TreeType = { var curPlan = plan batches.foreach { batch =&gt; val batchStartPlan = curPlan var iteration = 1 var lastPlan = curPlan var continue = true // Run until fix point (or the max number of iterations as specified in the strategy. while (continue) { // 对 curPlan 顺序执行一次当前 batch 的所有 rule curPlan = batch.rules.foldLeft(curPlan) { case (plan, rule) =&gt; val result = rule(plan) if (!result.fastEquals(plan)) { logTrace( s&quot;&quot;&quot; |=== Applying Rule ${rule.ruleName} === |${sideBySide(plan.treeString, result.treeString).mkString(&quot;\\n&quot;)} &quot;&quot;&quot;.stripMargin) } result } iteration += 1 // 根据最大迭代数或是否达到不动点来确定是否要继续迭代 if (iteration &gt; batch.strategy.maxIterations) { // Only log if this is a rule that is supposed to run more than once. if (iteration != 2) { logInfo(s&quot;Max iterations (${iteration - 1}) reached for batch ${batch.name}&quot;) } continue = false } if (curPlan.fastEquals(lastPlan)) { logTrace( s&quot;Fixed point reached for batch ${batch.name} after ${iteration - 1} iterations.&quot;) continue = false } lastPlan = curPlan // 进入下一轮迭代 } if (!batchStartPlan.fastEquals(curPlan)) { logDebug( s&quot;&quot;&quot; |=== Result of Batch ${batch.name} === |${sideBySide(plan.treeString, curPlan.treeString).mkString(&quot;\\n&quot;)} &quot;&quot;&quot;.stripMargin) } else { logTrace(s&quot;Batch ${batch.name} has no effect.&quot;) } // 进入下一个 batch } curPlan }} 所以，RuleExecutor 这个类的主要功能，在于 execute 函数可对传入的 plan 迭代地执行子类指定的 rule，不同组的 rule 通过分配在不同的 batch 中以及放置的位置来区分执行的先后次序。 我们再看回 Analyzer： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// SQLContext.scalaprotected[sql] lazy val analyzer: Analyzer = new Analyzer(catalog, functionRegistry, conf) { override val extendedResolutionRules = ExtractPythonUdfs :: sources.PreInsertCastAndRename :: Nil override val extendedCheckRules = Seq( sources.PreWriteCheck(catalog) ) }// Analyzer.scala // SQLContext 在创建时放入了 Analysis 过程需要的 Catalog 和 FunctionRegistryclass Analyzer( catalog: Catalog, registry: FunctionRegistry, conf: CatalystConf, maxIterations: Int = 100) // 最大迭代数取默认值 100 extends RuleExecutor[LogicalPlan] with HiveTypeCoercion with CheckAnalysis { def resolver: Resolver = { if (conf.caseSensitiveAnalysis) { caseSensitiveResolution } else { caseInsensitiveResolution } } // 生成 strategy val fixedPoint = FixedPoint(maxIterations) // 在 SQLContext 的匿名内部类中被重载，额外放入了两个 rule val extendedResolutionRules: Seq[Rule[LogicalPlan]] = Nil // 需要执行的 rule 们，同时在第二个 batch 中放入了在 SQLContext 的匿名内部类中指定的两个 rule lazy val batches: Seq[Batch] = Seq( Batch(&quot;Substitution&quot;, fixedPoint, CTESubstitution :: WindowsSubstitution :: Nil : _*), Batch(&quot;Resolution&quot;, fixedPoint, ResolveRelations :: ResolveReferences :: ResolveGroupingAnalytics :: ResolveSortReferences :: ResolveGenerate :: ResolveFunctions :: ExtractWindowExpressions :: GlobalAggregates :: UnresolvedHavingClauseAttributes :: TrimGroupingAliases :: typeCoercionRules ++ extendedResolutionRules : _*) ) // ...} 接下来，我们继续往下挖掘，看一下 Rule 类： 1234567891011abstract class Rule[TreeType &lt;: TreeNode[_]] extends Logging { // Rule 的名字。默认为类的类名 val ruleName: String = { val className = getClass.getName if (className endsWith &quot;$&quot;) className.dropRight(1) else className } // 子类通过重载 Rule 的 apply 函数来实现其逻辑 def apply(plan: TreeType): TreeType} 本文就不对每个 Rule 子类都进行讲解了，各位可以自行观看。你们只要知道真正起作用的是它的 apply 函数，我想看起来应该也是很轻松的事。 Optimizer 实际上，在学习过 Analyzer 的执行机制以后，Optimizer 就是水到渠成了。因为 Optimizer 同样继承了 RuleExecutor： 12345678910111213141516171819202122232425262728293031323334// SQLContext.scalaprotected[sql] lazy val optimizer: Optimizer = DefaultOptimizer// Optimizer.scalaabstract class Optimizer extends RuleExecutor[LogicalPlan]object DefaultOptimizer extends Optimizer { val batches = // SubQueries are only needed for analysis and can be removed before execution. Batch(&quot;Remove SubQueries&quot;, FixedPoint(100), EliminateSubQueries) :: Batch(&quot;Operator Reordering&quot;, FixedPoint(100), UnionPushdown, CombineFilters, PushPredicateThroughProject, PushPredicateThroughJoin, PushPredicateThroughGenerate, ColumnPruning, ProjectCollapsing, CombineLimits) :: Batch(&quot;ConstantFolding&quot;, FixedPoint(100), NullPropagation, OptimizeIn, ConstantFolding, LikeSimplification, BooleanSimplification, SimplifyFilters, SimplifyCasts, SimplifyCaseConversionExpressions) :: Batch(&quot;Decimal Optimizations&quot;, FixedPoint(100), DecimalAggregates) :: Batch(&quot;LocalRelation&quot;, FixedPoint(100), ConvertToLocalRelation) :: Nil} 完全同理，相比之下 Optimizor 执行器更加简单。 总结在本文中，我们学习了 Analyzer 和 Optimizer 的执行方式，了解到它们都利用了 RuleExecutor，区别仅在于在重载的过程中设定了不同的 Rule。可以说是用同样的逻辑完成了两件事。 在本文中我并未详细介绍 Rule 实现类，并不是因为它们不重要。实际上它们才是 Analysis 和 Optimization 过程的主角。在了解到 Rule 子类的执行入口是 apply 函数后，相信各位在阅读 Rule 实现类的过程中应该不会遇到太大的问题。 下一次，我们将继续大步向前，开始探究 SparkSQL 如何根据 Optimized Logical Plan 生成 Physical Plan。敬请期待。","link":"/sparksql_catalyst_source_4/"},{"title":"Spark Catalyst 源码解析：Planner 与 RDD","text":"在上一篇文章中，我们详细了解了 SparkSQL 如何利用 Analyzer 和 Optimizer，一步一步将 Unresolved Logical Plan 变为 Analyzed Logical Plan 再变为 Optimized Logical Plan。到了这一步，Logical Plan 的生命历程就走到了终点。 在这篇文章中，我将开始讲解 SparkSQL 如何通过 Planner 将 Optimized Logical Plan 变为 Physical Plan，再变为结果 RDD。 SparkPlanner到了这一步，我们就不能期待 Planner 和 Optimizer 他们一样继承自 RuleExecutor 了。为了了解这个过程的入口，我们先回到之前提到过的 SQLContext#QueryExecution： 12345678lazy val optimizedPlan: LogicalPlan = optimizer.execute(withCachedData)// 生成 PhysicalPlan// Optimized Logical Plan -&gt; Physical Planlazy val sparkPlan: SparkPlan = { SparkPlan.currentContext.set(self) planner.plan(optimizedPlan).next()} 总结下来就是这样的一个流程：optimizedPlan -&gt; planner.plan -&gt; sparkPlan。由此一来，我们首先锁定了入口方法 planner.plan： 12345678910111213141516171819202122232425262728// SQLContext.scalaprotected[sql] val planner = new SparkPlannerprotected[sql] class SparkPlanner extends SparkStrategies { // 从外部的 SQLContext 实例中导入相关设定参数 val sparkContext: SparkContext = self.sparkContext val sqlContext: SQLContext = self def codegenEnabled: Boolean = self.conf.codegenEnabled def unsafeEnabled: Boolean = self.conf.unsafeEnabled def numPartitions: Int = self.conf.numShufflePartitions def strategies: Seq[Strategy] = experimental.extraStrategies ++ ( DataSourceStrategy :: DDLStrategy :: TakeOrdered :: HashAggregation :: LeftSemiJoin :: HashJoin :: InMemoryScans :: ParquetOperations :: BasicOperators :: CartesianProduct :: BroadcastNestedLoopJoin :: Nil) // ...} 我们这次看到了一个 strategies 变量，其形式与之前在 Analyzer 和 Optimizer 里看到的 batches 变量十分相似。除此之外，我们并未看到 SparkPlanner 实现 plan 方法。这并不奇怪，毕竟 Analyzer 和 Optimizer 也没有实现 execute 方法。那我们先去看看 SparkPlanner 的父类 SparkStrategies： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private[sql] abstract class SparkStrategies extends QueryPlanner[SparkPlan] { self: SQLContext#SparkPlanner =&gt; object LeftSemiJoin extends Strategy with PredicateHelper { // ... } object HashJoin extends Strategy with PredicateHelper { // ... } object HashAggregation extends Strategy { // ... } object BroadcastNestedLoopJoin extends Strategy { // ... } object CartesianProduct extends Strategy { // ... } protected lazy val singleRowRdd = sparkContext.parallelize(Seq(new GenericRow(Array[Any]()): Row), 1) object TakeOrdered extends Strategy { // ... } object ParquetOperations extends Strategy { // ... } object InMemoryScans extends Strategy { // ... } object BasicOperators extends Strategy { // ... } object DDLStrategy extends Strategy { // ... }} 似乎 SparkStrategies 并未定义任何函数，倒是定义了大量的 Strategy 子类，这些子类都被应用在了 SQLContext#SparkPlanner 中。那么看来，这个类确实是名符其实的 SparkStrategies。我们继续去看它的父类吧！ 1234567891011121314151617181920// 可以看到 Strategy 与之前的 Rule 很类似，差别只在与 apply 函数返回的是 Seq[PhysicalPlan]abstract class GenericStrategy[PhysicalPlan &lt;: TreeNode[PhysicalPlan]] extends Logging { def apply(plan: LogicalPlan): Seq[PhysicalPlan]}// 相对的，QueryPlanner 也和 RuleExecutor 十分相似abstract class QueryPlanner[PhysicalPlan &lt;: TreeNode[PhysicalPlan]] { /** A list of execution strategies that can be used by the planner */ def strategies: Seq[GenericStrategy[PhysicalPlan]] // 返回一个占位符。该占位符将由 QueryPlanner 使用其它可用的 Strategy 替换掉 protected def planLater(plan: LogicalPlan) = this.plan(plan).next() def plan(plan: LogicalPlan): Iterator[PhysicalPlan] = { // Lazy 地在 LogicalPlan 上 apply 所有 Strategy val iter = strategies.view.flatMap(_(plan)).toIterator assert(iter.hasNext, s&quot;No plan for $plan&quot;) iter }} 从执行引擎这边能看到的似乎就只有这些了，我们甚至无法知道模板参数 PhysicalPlan 具体会是什么类型。通过查看之前出现过的 Strategy 类型，会在 sql 的包对象中发现这样一句： 12@DeveloperApitype Strategy = org.apache.spark.sql.catalyst.planning.GenericStrategy[SparkPlan] 至此我们就了解到，PhysicalPlan 树结点的类型为 SparkPlan。于是我们查看它的源代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105object SparkPlan { protected[sql] val currentContext = new ThreadLocal[SQLContext]()}// 与 LogicalPlan 相同，继承自 QueryPlanabstract class SparkPlan extends QueryPlan[SparkPlan] with Logging with Serializable { self: Product =&gt; @transient protected[spark] final val sqlContext = SparkPlan.currentContext.get() protected def sparkContext = sqlContext.sparkContext // sqlContext will be null when we are being deserialized on the slaves. In this instance // the value of codegenEnabled will be set by the desserializer after the constructor has run. val codegenEnabled: Boolean = if (sqlContext != null) { sqlContext.conf.codegenEnabled } else { false } /** Overridden make copy also propogates sqlContext to copied plan. */ override def makeCopy(newArgs: Array[AnyRef]): this.type = { SparkPlan.currentContext.set(sqlContext) super.makeCopy(newArgs) } // 定义计算结果在各个节点上的 partition 规则 def outputPartitioning: Partitioning = UnknownPartitioning(0) // TODO: WRONG WIDTH! // 定义输入数据的若干个节点分布要求 def requiredChildDistribution: Seq[Distribution] = Seq.fill(children.size)(UnspecifiedDistribution) // 定义计算结果在各个节点上的排序规则 def outputOrdering: Seq[SortOrder] = Nil // 定义输入数据的每个 partition 的若干个排序要求 def requiredChildOrdering: Seq[Seq[SortOrder]] = Seq.fill(children.size)(Nil) // 在 withScope 内调用 doExecute 方法来得出结果 final def execute(): RDD[Row] = { RDDOperationScope.withScope(sparkContext, nodeName, false, true) { doExecute() } } // 由子类重载该方法返回计算结果 protected def doExecute(): RDD[Row] // execute + collect def executeCollect(): Array[Row] = { execute().mapPartitions { iter =&gt; val converter = CatalystTypeConverters.createToScalaConverter(schema) iter.map(converter(_).asInstanceOf[Row]) }.collect() } // execute + take(n) def executeTake(n: Int): Array[Row] = { if (n == 0) { return new Array[Row](0) } // 先获得代表完整结果的 RDD val childRDD = execute().map(_.copy()) // result buffer val buf = new ArrayBuffer[Row] // partition 总数 val totalParts = childRDD.partitions.length // 已扫描的 partition 数 var partsScanned = 0 while (buf.size &lt; n &amp;&amp; partsScanned &lt; totalParts) { // 本次迭代尝试扫描的 partition 数 var numPartsToTry = 1 if (partsScanned &gt; 0) { // 从第二次迭代开始 if (buf.size == 0) { // 如果第一次迭代完全没有获取到结果，直接扫描剩下所有 partition numPartsToTry = totalParts - 1 } else { // 1.5 * n / (buf.size / partsScanned) numPartsToTry = (1.5 * n * partsScanned / buf.size).toInt } } numPartsToTry = math.max(0, numPartsToTry) // guard against negative num of partitions // 剩余所需结果数 val left = n - buf.size // 即将进行尝试的 partition 集 val p = partsScanned until math.min(partsScanned + numPartsToTry, totalParts) val sc = sqlContext.sparkContext val res = sc.runJob(childRDD, (it: Iterator[Row]) =&gt; it.take(left).toArray, p, allowLocal = false) // 将结果放入 buf res.foreach(buf ++= _.take(n - buf.size)) partsScanned += numPartsToTry } // 改变结果类型并返回。此步同 takeCollect val converter = CatalystTypeConverters.createToScalaConverter(schema) buf.toArray.map(converter(_).asInstanceOf[Row]) } // ... } 真是万万没想到，SparkPlan 与 LogicalPlan 同样继承自 QueryPlan，但仔细想想确实很合理。通过观察 SparkPlan 类便能发现，其实现类需要通过重载 doExecute 方法来定义自己的计算逻辑。在了解到这个主要入口以后，剩下的问题就变得轻松很多了。 但实际上，有一个难题我们并没有解决，有可能各位还没注意到这个问题。 12345678910111213abstract class QueryPlanner[PhysicalPlan &lt;: TreeNode[PhysicalPlan]] { // ... // 返回一个占位符。该占位符将由 QueryPlanner 使用其它可用的 Strategy 替换掉 protected def planLater(plan: LogicalPlan) = this.plan(plan).next() def plan(plan: LogicalPlan): Iterator[PhysicalPlan] = { // Lazy 地在 LogicalPlan 上 apply 所有 Strategy val iter = strategies.view.flatMap(_(plan)).toIterator assert(iter.hasNext, s&quot;No plan for $plan&quot;) iter }} plan 函数的 iter = strategies.view.flatMap(_(plan)).toIterator 这句是不是有点问题？为什么 planLater 那个实现返回的是一个占位符？这个问题我们先不着急回答，我们先看看 Strategy 实现类是怎么使用 planLater 的： 1234567891011121314151617181920212223242526// 笛卡尔积，由 SQL 语句的 JOIN 操作触发object CartesianProduct extends Strategy { def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match { case logical.Join(left, right, _, None) =&gt; execution.joins.CartesianProduct(planLater(left), planLater(right)) :: Nil case logical.Join(left, right, Inner, Some(condition)) =&gt; execution.Filter(condition, execution.joins.CartesianProduct(planLater(left), planLater(right))) :: Nil case _ =&gt; Nil }}// 注意：这个类属于 execution.joins 包，放在这里只是方便参考case class CartesianProduct(left: SparkPlan, right: SparkPlan) extends BinaryNode { override def output: Seq[Attribute] = left.output ++ right.output protected override def doExecute(): RDD[Row] = { val leftResults = left.execute().map(_.copy()) val rightResults = right.execute().map(_.copy()) leftResults.cartesian(rightResults).mapPartitions { iter =&gt; val joinedRow = new JoinedRow iter.map(r =&gt; joinedRow(r._1, r._2)) } }} 我们注意到，在 object CartesianProduct 的 apply 中，当遇到标记为 Join 的 Logical Plan 时，它的做法是先对左右子树分别调用 planLater 得到结果后，再构造 execution.joins.CartesianProduct。而 planLater 又会调用 plan，这意味着每一次调用 planLater 实际上都是一次递归，这是一个先序遍历。planLater 的实现是 this.plan(plan).next()，意味着即使 strategies 中可应用于传入子树的策略不止一个，返回的 Physical Plan 数也可能不止一个（注意 Strategy 的 apply 函数返回的是个 Seq），但 planLater 都只取第一个。 我们回到最初启动 plan 过程的入口： 1234lazy val sparkPlan: SparkPlan = { SparkPlan.currentContext.set(self) planner.plan(optimizedPlan).next()} 这里就是这个先序遍历开始的地方，同样使用了和 planLater 一样的调用方式，这就证明了我的猜想。这同时说明，尽管 Spark 可以为同一个 Logical Plan 生成多个 Physical Plan，但本该在这些 Physical Plan 中选出最低代价执行计划的功能并未实现。在 LogicalPlan 中我们有看到过疑似要用于 cost-based 优化的 Statistics 变量，但在 Physical Plan 这边实际上我们并未见到它的身影，而且 Statistics 类本身的设计也过于简单（它是一个只包含了一个 BigInt 变量的 case class，并未继承任何类），显得有些许儿戏。 但这毕竟是不能怪 SparkSQL 的，查询代价受环境的影响很大，比起 rule-based 优化来说，cost-based 太过不稳定，实现起来也复杂很多。不过不管怎么说，SparkSQL 仍然留下了可用于实现 cost-based 优化的接口，也许有朝一日这个功能真的会实现。 toRDD我们接着往下走： 1234lazy val executedPlan: SparkPlan = prepareForExecution.execute(sparkPlan)// 执行并返回结果lazy val toRdd: RDD[Row] = executedPlan.execute() 上文中出现的 prepareForExecution 实际上是一个 RuleExecutor 的子类，它唯一的 rule 是 EnsureRequirements，它会确保输入数据的 Partitioning 满足 SparkPlan 中规定的 childDistribution，如果不满足则会通过添加子结点等方式尝试修复。 最终，toRDD 通过调用 SparkPlan 的 execute 方法，获取到计算结果。 结语至此，我们大概了解了 SparkSQL 是如何处理用户的 SQL 语句，如何一步一步把它解析成 Logical Plan 再解析成 Physical Plan 再变成结果 RDD。如此粗略的介绍实在很难让你就此成为 SparkSQL 大师，因为 Catalyst 还有相当多的代码量用于定义优化规则即 Logical/Physical Plan 转换规则。以后我会考虑出一些进阶篇来讲讲这之中一些进阶级的细节实现，敬请期待咯。","link":"/sparksql_catalyst_source_5/"},{"title":"SparkSQL Hive ThriftServer 源码解析：SparkSQLCLIService","text":"此文接上文，继续讲解 SparkSQL Hive ThriftServer 源码。 上文提到，主类 HiveThriftServer2 在启动后便会启动 ThriftCLIService 和 SparkSQLCLIService，其中 ThriftCLIService 负责维护与客户端的连接并将客户端的请求转发至 SparkSQLCLIService，由 SparkSQLCLIService 执行运算并把结果返回给 ThriftCLIService，ThriftCLIService 再把结果以 ResultSet 的形式返回给客户端。两者之间的关系如下图所示： 但当下，我们并不清楚，两个 Service 之间以及 ThriftCLIService 与客户端之间是如何完成交互的。本文将先从 SparkSQLCLIService 开始，看看在这个方向上能不能找到点线索。 SparkSQLCLIService咱直接开始看代码吧！ 1234567891011121314151617181920212223242526272829303132private[hive] class SparkSQLCLIService(hiveContext: HiveContext) extends CLIService with ReflectedCompositeService { override def init(hiveConf: HiveConf) { // this.hiveConf = hiveConf setSuperField(this, &quot;hiveConf&quot;, hiveConf) // sessionManager = new SessionManager() val sparkSqlSessionManager = new SparkSQLSessionManager(hiveContext) setSuperField(this, &quot;sessionManager&quot;, sparkSqlSessionManager) // addService(sessionManager) addService(sparkSqlSessionManager) var sparkServiceUGI: UserGroupInformation = null if (ShimLoader.getHadoopShims.isSecurityEnabled) { try { HiveAuthFactory.loginFromKeytab(hiveConf) sparkServiceUGI = ShimLoader.getHadoopShims.getUGIForConf(hiveConf) HiveThriftServerShim.setServerUserName(sparkServiceUGI, this) } catch { case e @ (_: IOException | _: LoginException) =&gt; throw new ServiceException(&quot;Unable to login to kerberos with given principal/keytab&quot;, e) } } // super.init(hiveConf) initCompositeService(hiveConf) } // ...} 首先我们看到 SparkSQLCLISerivce 继承自 CLIService，同时混入了 ReflectedCompositeService 特质。由此可见，CompositeService 应该也是 SparkSQLCLIService 的父类之一。对比于 CLIService 的 init 方法（其部分源代码已以行注释的形式在上述对应代码中给出），SparkSQLCLIService 的 init 方法可以说完全是在做一模一样的事情，不同点仅在于 CLIService 启动一个 SessionManager，而 SparkSQLCLIService 启动了一个 SparkSQLSessionManager。我觉得光从名字上都能判断出来，SparkSQLSessionManager 一定继承自 SessionManager。 让我们继续一探究竟吧！ SparkSQLSessionManager1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556private[hive] class SparkSQLSessionManager(hiveContext: HiveContext) extends SessionManager with ReflectedCompositeService { private lazy val sparkSqlOperationManager = new SparkSQLOperationManager(hiveContext) override def init(hiveConf: HiveConf) { // this.hiveConf = hiveConf setSuperField(this, &quot;hiveConf&quot;, hiveConf) // int backgroundPoolSize = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_THREADS) val backgroundPoolSize = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_THREADS) // backgroundOperationPool = new ThreadPoolExecutor(backgroundPoolSize, backgroundPoolSize, // keepAliveTime, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(backgroundPoolQueueSize)) setSuperField(this, &quot;backgroundOperationPool&quot;, Executors.newFixedThreadPool(backgroundPoolSize)) getAncestorField[Log](this, 3, &quot;LOG&quot;).info( s&quot;HiveServer2: Async execution pool size $backgroundPoolSize&quot;) // addService(operationManager) setSuperField(this, &quot;operationManager&quot;, sparkSqlOperationManager) addService(sparkSqlOperationManager) // super.init(hiveConf) initCompositeService(hiveConf) } override def openSession( protocol: TProtocolVersion, username: String, passwd: String, sessionConf: java.util.Map[String, String], withImpersonation: Boolean, delegationToken: String): SessionHandle = { // 在 HiveContext 中创建 SQLSession hiveContext.openSession() // 利用 SessionManager 创建 HiveSession val sessionHandle = super.openSession( protocol, username, passwd, sessionConf, withImpersonation, delegationToken) val session = super.getSession(sessionHandle) // 通知 HiveThriftServer2Listener 有新的 HiveSession 被创建 HiveThriftServer2.listener.onSessionCreated( session.getIpAddress, sessionHandle.getSessionId.toString, session.getUsername) sessionHandle } override def closeSession(sessionHandle: SessionHandle) { // 通知 HiveThriftServer2Listener 有 HiveSession 被关闭 HiveThriftServer2.listener.onSessionClosed(sessionHandle.getSessionId.toString) // 利用 SessionManager 关闭 HiveSession super.closeSession(sessionHandle) sparkSqlOperationManager.sessionToActivePool -= sessionHandle // 在 HiveContext 中关闭 SQLSession hiveContext.detachSession() }} 果不其然，SparkSQLSessionManager 的 init 方法与 SessionManager 的 init 方法极为相似。从名字上看，Session Manager 当然是用来管理 Session 的了。SparkSQLSessionManager 的 openSession 和 closeSession 方法都有调用 SessionManager 的对应方法来管理 HiveSession，同时还管理了 HiveContext 内部的 SQLSession。简单的查看 HiveSession 和 SQLSession 的定义，可以得出结论，HiveSession 指的是 Hive ThriftServer 与 Client 之间的 Session，即通常意义上的网络 Session；而 SQLSession 指的是 SparkSQL 与 Hive ThriftServer 之间的 Session，但 SQLSession 实际存储的只是一系列与 SQL 查询有关的配置参数，和传统意义上的网络 Session 不同。 SparkSQLSessionManager 与 SessionManager 的不同点在于 SparkSQLSessionManager 启动了一个 SparkSQLOperationManager，而 SessionManager 启动的是 OperationManager。那么，其实也能猜到一些了。 SparkSQLOperationManager1234567891011121314151617181920212223/** * Executes queries using Spark SQL, and maintains a list of handles to active queries. */private[thriftserver] class SparkSQLOperationManager(hiveContext: HiveContext) extends OperationManager with Logging { val handleToOperation = ReflectionUtils.getSuperField[JMap[OperationHandle, Operation]](this, &quot;handleToOperation&quot;) val sessionToActivePool = Map[SessionHandle, String]() override def newExecuteStatementOperation( parentSession: HiveSession, statement: String, confOverlay: JMap[String, String], async: Boolean): ExecuteStatementOperation = synchronized { // 利用 session、statement、conf 相关信息创建一个 SparkExecuteStatementOperation val operation = new SparkExecuteStatementOperation(parentSession, statement, confOverlay)( hiveContext, sessionToActivePool) handleToOperation.put(operation.getHandle, operation) operation }} 简短，直白。很明显，newExecuteStatementOperation 方法会在客户端发送 JDBC 请求后被调用。方法创建了一个 SparkExecuteStatementOperation，并将其进行缓存管理。实际上，SparkSQLOperationManager 只复写了 OperationManager 的 newExecuteStatementOperation 方法，除此之外 OperationManager 还有 newGetSchemasOperation 等其他方法。这些方法从命名上判断，都是用户在查询表的元数据时才会触发的操作，比如 newGetSchemasOperation 应该是会在用户试图查询某张表的模式的时候才会触发的操作。SparkSQL 之所以要重载 newExecuteStatementOperation 的原因是显然的：Execute 意味着执行，SparkSQL Hive ThriftServer 通过重载该方法，把用户通过 execQuery 发送的执行请求转发至 SparkSQL。 那就直接看看 SparkExecuteStatementOperation 到底干了什么吧（如果你已经猜到了，我并不会觉得意外 ;-) ）。 SparkExecuteStatementOperation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687private[hive] class SparkExecuteStatementOperation( parentSession: HiveSession, statement: String, confOverlay: JMap[String, String], runInBackground: Boolean = true)( hiveContext: HiveContext, sessionToActivePool: SMap[SessionHandle, String]) // NOTE: `runInBackground` is set to `false` intentionally to disable asynchronous execution extends ExecuteStatementOperation(parentSession, statement, confOverlay, false) with Logging { /** 执行结果 */ private var result: DataFrame = _ private var iter: Iterator[SparkRow] = _ private var dataTypes: Array[DataType] = _ def close(): Unit = { // RDDs will be cleaned automatically upon garbage collection. logDebug(&quot;CLOSING&quot;) } // ... /** 获取 ResultSet 的下一行（注意该类有一个 iter 成员变量） */ def getNextRowSet(order: FetchOrientation, maxRowsL: Long): RowSet = { // ... } /** 获取 ResultSet 的模式（所包含域的名和类型） */ def getResultSetSchema: TableSchema = { // ... } /** 执行查询，结果放入 result 变量并生成对应 iter */ def run(): Unit = { val statementId = UUID.randomUUID().toString logInfo(s&quot;Running query '$statement'&quot;) setState(OperationState.RUNNING) // 通知 Server，即将开始进行运算 HiveThriftServer2.listener.onStatementStart( statementId, parentSession.getSessionHandle.getSessionId.toString, statement, statementId, parentSession.getUsername) hiveContext.sparkContext.setJobGroup(statementId, statement) sessionToActivePool.get(parentSession.getSessionHandle).foreach { pool =&gt; hiveContext.sparkContext.setLocalProperty(&quot;spark.scheduler.pool&quot;, pool) } try { // 噢吼 result = hiveContext.sql(statement) logDebug(result.queryExecution.toString()) result.queryExecution.logical match { case SetCommand(Some((SQLConf.THRIFTSERVER_POOL, Some(value))), _) =&gt; sessionToActivePool(parentSession.getSessionHandle) = value logInfo(s&quot;Setting spark.scheduler.pool=$value for future statements in this session.&quot;) case _ =&gt; } // 通知 Server，运算已完成 HiveThriftServer2.listener.onStatementParsed(statementId, result.queryExecution.toString()) // 提取结果 DataFrame 的 Iterator iter = { val useIncrementalCollect = hiveContext.getConf(&quot;spark.sql.thriftServer.incrementalCollect&quot;, &quot;false&quot;).toBoolean if (useIncrementalCollect) { result.rdd.toLocalIterator } else { result.collect().iterator } } dataTypes = result.queryExecution.analyzed.output.map(_.dataType).toArray setHasResultSet(true) } catch { // Actually do need to catch Throwable as some failures don't inherit from Exception and // HiveServer will silently swallow them. case e: Throwable =&gt; setState(OperationState.ERROR) HiveThriftServer2.listener.onStatementError( statementId, e.getMessage, e.getStackTraceString) logError(&quot;Error executing query:&quot;, e) throw new HiveSQLException(e.toString) } setState(OperationState.FINISHED) HiveThriftServer2.listener.onStatementFinish(statementId) }} 那其实就很一目了然了：用户通过 JDBC execQuery 发送的请求最终被原封不动地转发到了 HiveContext.sql 上进行运算，结果保存在 SparkExecuteStatementOperation 中，同时保存一个 Iterator，视客户端所需逐行逐行地以 ResultSet 的形式取出，并返回至客户端。 至此，SparkSQLCLIService 一侧的运作原理就基本探索完毕了。 总结在深入了解过 SparkSQLCLIService 一侧的原理以后，之前那张图大概就会变成下面这个样子： 总体而言，Spark Hive ThriftServer 确实是基于 Apache Hive 的基础之上通过少量的修改、继承甚至是利用 Java 反射机制来 hack Hive 原本的类来将 Hive 本该转发至 Hadoop MapReduce 的操作转发到了 SparkSQL 的 HiveContext.sql，因此在 JDBC 上调用 execQuery 和直接调用 HiveContext.sql 的效果是一致的。 除了 SparkSQLCLISerivce，ThriftCLIService 侧的代码其实都是 Apache Hive 本身的代码，Spark 未对其进行任何改写。Spark Hive ThriftServer 项目本身的所有代码仅包括 SparkSQLCLIService 这一侧的代码和 Spark SQL Shell 的代码。因此总体而言，在阅读完本篇文章后，你应该已经完全了解 Spark Hive ThriftServer 的工作原理了。Hive ThriftCLIService 一侧的代码很有可能我不会再去看了，因为那一侧的代码的功能已经十分明确，但由于涉及到网络通信，毫无疑问那一侧的代码量将会是这一侧的好几倍。因此如果你只是想了解 SparkSQL Hive Server 的运作原理，你的目的已经达到了。恭喜你！","link":"/sparksql_hive_thriftserver_source_2/"},{"title":"Spinnaker —— 使用 Paxos 构建分布式 KV 数据库","text":"在 MIT 6.824 的 Lecture 7 中，我们将阅读《Using Paxos to Build a Scalable, Consistent, and Highly Available Datastore》一文，看看 LinkedIn 的工程师是如何利用 Paxos 和 ZooKeeper 构建一个名为 Spinnaker 的 KV 数据库的。 这篇文章并不难，有好好理解上一课中关于 Raft 的内容的话就不会有太多的问题。诚如论文标题所属，这次的 Case Study 更多只是学习我们可以怎样利用 Raft/Paxos 来构建一个强一致性的分布式数据库，同时确保它的性能可以进行水平扩展。 点击这里可以阅读我之前的几篇 MIT 6.824 系列文章： Lecture 1：MapReduce Lecture 3：Google File System Lecture 4：Primary-Backup Replication Lecture 5 &amp; 6：Raft 背景对于现在的数据库而言，上层的网络应用开始对其提出越来越高的可扩展性的要求，其中一个比较合理的解决方案便是 Sharding。在以前，Sharding 通常由系统管理员手动完成，并维护数据分片的负载均衡，不过较新型的数据库也已开始支持基于 Key 的 Hash 或 Range 的自动数据分片及负载均衡。 除了性能可扩展以外，应用对于数据库可用性的要求也不低，为此数据库系统也需要提供一定的备份机制，以便在节点失效时仍能对外提供服务。一种比较常见的做法是使用一对相同的节点，以 Master-Slave 的形式进行同步数据备份，但这样的两路备份的解决方案仍然存在着问题（见论文 1.1 节）。 新型的数据库系统开始采用三路备份的结局方案来避免 Master-Slave 架构的问题，但也让节点间的备份协议变得更加复杂。Paxos 一直以来都被看做是这种场景下的唯一选择，但从未被用于进行数据库的备份，因为那会相对更加复杂一些，且较大的数据量也会使得备份速度更低。 由 LinkedIn 开发的 Spinnaker 是一款实验性质的 KV 数据库系统，采用类似 Paxos 的日志备份协议实现了三路数据备份，可基于 Key 的值域对数据进行分片，对外提供事务型的 Get/Put API。除外，Spinnaker 还允许应用程序在使用 Get API 时指定一致性模式，可选择强一致性或是时间轴一致性：使用时间轴一致性时，客户端可能会读取到过时的数据，但相对的也会换来更好的性能。 Why Spinnaker?MIT 6.824 安排在 Raft 之后学习 Spinnaker 主要有以下几点原因： 通过 Spinnaker 学习如何可以使用 Paxos 来构建一个 KV 数据库。在 MIT 6.824 的 Lab 3 中我们就会做一样的事情，只是会用 Raft 而不是 Paxos Spinnaker 是少数几个首先尝试使用 Paxos 来实现数据备份的产品，而不只是像 Chubby/ZooKeeper 那样做配置管理 Spinnaker 集群架构首先，Spinnaker 作为 KV 数据库系统，会把数据按照它们的 Key 进行基于值域的分片，每个分片独立进行备份，常规的 Spinnaker 部署配置通常会把数据分片备份 3 片以上。尽管可以设定更高的备份因子，后续讨论将假设只配置为备份 3 份。 每个数据分片都会有与其相关联的值域（如 $[0, 199]$），持有该数据分片的备份的节点共同组成一个 Cohort（备份组）：例如，上图中，节点 A、B、C 就共同组成了分片 $[0, 199]$ 的 Cohort。 每个 Cohort 都会有自己的 Leader，其他成员则作为 Follower。在存储数据时，Leader 首先会以先写日志的形式记录此次数据修改操作，并由 Paxos 对这部分日志备份至 Cohort 的其他成员。已完成提交的操作记录则会采用类似 Bigtable 的形式进行数据写入：数据首先会被写入到 Memtable，待 Memtable 的大小达到一定阈值后再被写出到 SSTable 中。 除外，Spinnaker 还使用了 ZooKeeper 来进行集群协调。ZooKeeper 为 Spinnaker 提供了存储元数据和检测节点失效的有效解决方案，也极大地简化了 Spinnaker 的设计。 Spinnaker 日志备份协议如上一节所属，在 Spinnaker 运行时，每一组 Cohort 都会有自己独立的 Leader，其他 Cohort 成员作为 Follower，客户端的写操作请求会被路由到 Cohort 的 Leader，由 Leader 与其他 Follower 完成日志备份共识后再写入数据变动并响应客户端。 数据写入在稳定状态下，Spinnaker 的一次数据写入的步骤如下： Spinnaker 将客户端发来的写入请求 $W$ 路由到受影响数据分片对应 Cohort 的 Leader 处 Leader 为写入请求 $W$ 生成对应的日志记录并追加到其日志中，而后并发地启动两个操作： 将写入请求 $W$ 的日志记录刷入到磁盘中 将该日志记录放入自己的提交队列，并开始向 Follower 进行备份 Follower 接收到 Leader 发来的日志后，也会将其刷入磁盘、放入自己的提交队列，然后响应 Leader Leader 收到大多数 Follower 的响应后，就会将写入请求 $W$ 应用到自己的 Memtable 上，并响应客户端 除外，Leader 也会周期地与 Follower 进行通信，告知 Follower 当前已提交的操作的序列号，Follower 便可得知先前的日志记录已完成提交，便可将其写入到自己的 Memtable 中。该通信周期被称为 Commit Period（提交周期）。 当客户端发起数据读取请求时，如果启用了强一致性模式，那么请求会被路由到 Leader 上完成，否则就可能会被路由到 Follower 上：正是由于 Commit Period 的存在，被路由到 Follower 上的时间轴一致请求有可能会读取到落后的数据，而落后的程序取决于 Commit Period 的长度。 Follower 恢复Follwer 的恢复过程可以被分为两个阶段：Local Recovery（本地恢复）和 Catch Up（追数据）。 假设 Follower 已知已提交的最后一条日志记录的序列号为 $f.cmt$。在 Local Recovery 阶段，Follwer 会先应用其本地存储的已知已提交的日志记录到 $f.cmt$，为其 Memtable 进行恢复。如果 Follower 的磁盘失效、所有数据都已丢失，那么 Follower 直接进入 Catch Up 阶段。 在 Catch Up 阶段，Follower 会向 Leader 告知其 $f.cmt$ 的值，Leader 便会向 Follower 发送 $f.cmt$ 后已完成提交的日志记录。 Leader 选举如上文所述，Spinnaker 主要借助 ZooKeeper 来侦测节点的失效事件。在一个 Cohort 的 Leader 失效时，其他 Follower 就会尝试成为 Leader。借助 ZooKeeper，Spinnaker 的 Leader 选举机制十分简单，只要确保新的 Leader 持有旧 Leader 所有已提交的日志记录即可。 这里我们先假设几个变量，以辅助后续的讨论： 令 $l.cmt$ 为 Leader 最后提交的日志的序列号 令 $l.lst$ 为 Leader 保存的最后一条日志的序列号 令 $f.cmt$ 为 Follwer $f$ 已知已提交的最后一条日志的序列号 令 $f.lst$ 为 Follwer $f$ 持有的最后一条日志的序列号 Spinnaker 的每个节点都会连接到 ZooKeeper 以进行 Leader 选举。假设目前正在进行 Leader 选举的 Cohort 对应的 Key 值域为 $r$，那么进行 Leader 选举所需的所有信息都会被保存到 ZooKeeper 的 /r 目录下。Leader 选举的过程如下： 其中一个节点删除 /r 目录下的旧数据，代表发起 Leader 选举 每个 Follower 节点将自己的 $f.lst$ 写入到 ZooKeeper Ephemeral Znode /r/candidates 之中 等待大多数节点完成上述步骤 $f.lst$ 值最大的 Follower 节点成为新的 Leader，并将自己的 Hostname 写入到 /r/leader 中 其他 Follower 通过读取 /r/leader 获知新 Leader 的身份 在选举出新 Leader 后，新 Leader 会进行以下操作： 为每个 Follwer，发送 $(f.cmt, l.cmt]$ 之间的日志，再发送 $l.cmt$ 日志已提交的信息 待有至少一个 Follwer 追上 $l.cmt$ 后，便开始按照常规的备份协议完成 $(l.cmt, l.lst]$ 的日志提交 日志压缩与合并在实现 Spinnaker 时的一个比较主要的 Engineering Chanllenge 在于如何避免因为过多的磁盘 IO 导致系统的性能底下。 从上文中所描述的 Spinnaker 集群架构可知，每个节点会持有不止一个数据分片，也就是说节点会同时属于多个 Cohort，但每个 Cohort 会独立地进行日志备份和写入。如果每个 Cohort 的日志都写入到独立的文件中，必然会引入大量的磁盘寻址消耗，所以 Spinnaker 会把同一个节点不同 Cohort 的日志写入到同一个文件中，在节点恢复时也可以在一次文件扫描中就完成所有 Cohort 的恢复，只需要在日志层面上区分其所属 Cohort 即可。 尽管如此，由于从 Follower 写入日志到日志实际提交中存在时间差，Follower 恢复时可能会观察到部分已写入磁盘的日志最终并未完成提交，需要将其从日志存储中移除，但考虑到不同 Cohort 的日志都存储在同一个文件中，这样的操作开销很大。为此，Spinnaker 引入了日志的 Logical Truncation（逻辑截断）机制：处于 Cohort 日志列表尾部需要移除的日志记录会被记录到一个 Skipped LSN 列表中，并写入到磁盘内，这样后续进行 Local Recovery 时，Follower 就会知道需要跳过这部分日志。 除外，使用固态硬盘也能很大程度上避免上述问题。如上文所述，不能在物理上区分不同 Cohort 日志的存储路径的原因在于机械磁盘的磁头寻址时间，而固态硬盘不存在这样的问题，因此在使用固态硬盘时，上述复杂的日志机制就变得没有意义了。在论文的附录 D.4 中讨论了使用固态硬盘的场景及相关的性能基准测试结果，可见使用固态硬盘时 Spinnaker 的写入性能有了 10 倍以上的提升。 结语如前文所述，这次 Spinnaker 的 Case Study 很好地讲述了如何可以利用 Raft/Paxos 来实现一个一致、高可用且可扩展的数据库系统。尽管 Spinnaker 很大程度上利用了 ZooKeeper 来简化自身的设计，但这部分内容也是可以通过 Raft/Paxos 这种分布式共识协议来实现的。 在完成本文的阅读后，我们实际上就可以开始 MIT 6.824 Lab 3 的实践了。后续我会以专门的系列文章来介绍 MIT 6.824 的 Lab 内容，敬请期待。","link":"/spinnaker/"},{"title":"解决 Windows10 笔记本关上后仍会掉电的问题","text":"一开始我只是想在谷歌上搜索一下，为啥我的 Win10 笔记本在盖上盖子以后放入背包中，每次拿出来之后电量都会有所下降，结果就一下子看到了很多很神奇的东西，包括 Win10 的几种节能状态，以及如何设置关闭盖子的行为。且听我一一道来。 Win10 节能状态总的来讲，自 Win7 到 Win8 到 Win10 以来，Windows 实际上一共有 3 种不同的节能状态，可参考该链接。链接中的文章很好地介绍了这 3 种状态，这里我就复制过来顺便简单翻译一下。 Sleep is a power-saving state that allows a computer to quickly resume full-power operation (typically within several seconds) when you want to start working again. Putting your computer into the sleep state is like pausing a DVD player; the computer immediately stops what it ’ s doing and is ready to start again when you want to resume working. 睡眠（Sleep）可以让计算机在你想要开始工作时迅速（通常在几秒钟内）恢复至全速运行状态。让你的计算机进入睡眠模式就像在 DVD 播放机上按下暂停按钮一样 —— 计算机会立刻停止它的当前任务并且随时准备好在你回来工作时再次启动。 Hibernation is a power-saving state designed primarily for laptops. While sleep puts your work and settings in memory and draws a small amount of power, hibernation puts your open documents and programs on your hard disk and then turns off your computer. Of all the power-saving states in Windows, hibernation uses the least amount of power. On a laptop, use hibernation when you know that you won ’ t use your laptop for an extended period and won ’ t have an opportunity to charge the battery during that time. 休眠（Hibernation）是一种主要为笔记本电脑设计的节能状态。睡眠实际上会把你当前的工作内容和设置放入到内存中，并且需要少量的电力来维持这些数据，而相比之下休眠则会把这些数据放入到磁盘中然后完全关闭你的计算机。在 Windows 中，休眠实际上是所需电力最少的节能状态。如果你使用的是笔记本电脑，那么如果你在一段较长的时间内都不会再使用你的电脑且这段时间也无法给它充电的话，你应该让它进入休眠状态。 Hybrid sleep is designed primarily for desktop computers. Hybrid sleep is a combination of sleep and hibernate; it puts any open documents and programs in memory and on your hard disk and then puts your computer into a low-power state so that you can quickly resume your work. That way, if a power failure occurs, Windows can restore your work from your hard disk. When hybrid sleep is turned on, putting your computer into sleep automatically puts your computer into hybrid sleep. Hybrid sleep is typically turned on by default on desktop computers and off by default on laptops. 混合睡眠（Hybrid Sleep）是一种主要为桌面电脑（台式机）设计的节能状态。混合睡眠实际上是睡眠与休眠的结合：它会把当前的工作内容继续保持在内存中，同时也把这些数据复制到磁盘中，然后再让你的计算机进入低耗能状态，如此一来你的计算机便可以快速地恢复当前状态，同时及时发生电力故障，Windows 仍然可以从磁盘中恢复当前的数据。在混合睡眠设置开启时，你令计算机进入睡眠模式时会让计算机自动进入混合睡眠模式。混合睡眠在桌面电脑上是默认开启的，而在笔记本电脑上则是默认关闭的。 设置 Win10 关闭盖子行为“关闭盖子” 这个词听着有点怪，而部分 Win10 用户实际上也应该在电源选项中见过这个名字： 实际上这个翻译也是挺奇怪的，而该选项的英文实际上是 “ Choose what closing the lid does ”，也就是配置电脑在盖上时应该做什么。 进入该页面，可看到设置如下： 这里可以选择在接通与未接通电源两种情况下，按下电源按钮和关闭盖子时计算机应该采取的行为。选项包括 “不采取任何操作”、“睡眠”、“休眠”、“关机”，在了解过 “睡眠” 和 “休眠” 的差异后，我想这四个选项的含义就不难理解了。我们只要将关闭盖子时的行为设置为休眠即可。 实际上，“关闭盖子时”的设置默认为“睡眠”，而该模式仍需要一定的电力维持内存中的数据，而且睡眠状态下的笔记本极易被唤醒（被鼠标、键盘、定时事件等唤醒），因此这也就是为何大多数 Win10 笔记本经常在盖子关上时自行启动了。可见这并不是一个 Bug，而只是单纯的设置不当。 不过值得注意的是，休眠后的计算机无法在短时间内恢复。进入睡眠模式的计算机在恢复时往往能在瞬间进入登录界面，而进入休眠模式的计算机由于需要从磁盘中读取数据，往往需要经历一段和开机相当的时间。当然，如果你的计算机使用的是 SSD，这个不足则微乎其微。 从上一节中对睡眠和休眠的描述来看，你可以在确定自己比较长时间内不会使用计算机时才选择让计算机进入休眠，如果离开的时间较短则可以选择让其进入睡眠。因此，可以在上述设置中选择在按下电源按钮时让计算机进入睡眠状态，这样如果你只是要短时间离开计算机（上厕所、倒杯水等），就可以考虑按下电源按钮进入睡眠，而不是直接盖上计算机。 除此之外，在使用休眠模式时要尤其小心：休眠模式在恢复时需要把磁盘上的数据重新读入到内存，考虑到 Win10 的高兼容性，实际上这个过程很容易发生错误导致部分软件无法恢复到原本的状态继续运行，因此恢复时很容易导致部分驱动程序崩溃，令计算机发生所谓的“蓝屏”现象。休眠模式的不稳定实际上自 Win7 以来便一直存在，Win10 也无法幸免，有较小几率仍会“蓝屏”，但相比 Win7 已有很大的改善。我想，这算是我们选择了 Win10 相比于 Mac OS 高得多的兼容性后所必须承受的代价吧。","link":"/win10_sleep_hibernate/"},{"title":"Hadoop Yarn 总结","text":"这篇文章的内容主要基于 Yarn 的论文总结而来。未来若有机会继续深入使用 Yarn，我会直接更新这篇文章的内容。 Yarn vs. Mesos由于先前已经读过 Mesos 的论文，在开始介绍 Yarn 前先简单讲解一下 Yarn 与 Mesos 的对比，以让大家更好地体会两个调度平台的差异。 总体而言，Yarn 在架构上有着和 Mesos 类似的组件角色，它们两者很多地方都是共通的，差别主要在于 Yarn 实现的是中心化的调度模型，而 Mesos 实现的是去中心化的分布式调度模型。由此带来的差别在于 Yarn 的 Master 角色（Resource Manager）在体量上会比 Mesos 的 Master 角色（Mesos Master）要更重，携带更多的集群信息，灾后恢复的过程将会更为复杂（在论文编写时，Yarn Resource Manager 仍无完整的灾后恢复机制）。而中心化调度的好处实际上在 Mesos 的论文中也有提到，就是 Master 更容易利用充足的信息做出最优的资源调度。除此以外，Yarn 和 Mesos 实际上大同小异。 在论文的行文上，Yarn 论文的篇幅比 Mesos 的论文要更长，花了 8 页纸的内容描述 Yarn 的背景、需求、基本组件及功能，而后者则只花了 5 页纸。实际上，Yarn 论文对 Yarn 各组件的介绍极为详细，且描述的多是每一个调度平台所应支持的功能，和 Yarn 本身关联性并不强，因此读者若有意自行实现一套新的集群资源调度平台，我很建议你去阅读 Yarn 论文的第 3 章，了解一个完整的资源调度平台除最核心的资源调度外应具备的辅助功能。可惜的是，Yarn 论文并未过多地描述 Yarn 实际可用的高可用架构，只提及了 Resource Manager 会利用一个“持久化存储”保存状态信息，而 Mesos 论文则详细地解释了 Mesos Master 如何利用 ZooKeeper 和 Mesos Slave 的信息进行有效恢复。 Yarn 的背景及需求在 Yarn 出现以前，Hadoop MapReduce 的资源管理是只为 Hadoop MapReduce 所设计的，设计之初只考虑了对常规 Hadoop MapReduce 作业的良好支持。然而，由于当时 Hadoop MapReduce 是少有的成熟分布式计算框架，许多开发者为了能够利用上大量的物理资源，不惜研究出各类奇技淫巧跳出 MapReduce 编程模型的限制，包括运行只包含 Map 任务的 MR 作业，甚至是利用 MR 作业部署长时间运行的服务。在这样的场景下，Hadoop MapReduce 的资源管理框架自然表现不佳，甚至出现了大量的论文以 Hadoop MapReduce 作为完全不相关测试环境的测试基准线。 总得来说，Hadoop 原有的资源管理模块有以下两个不足： 资源管理模块与 MapReduce 这个具体的编程模型之间存在紧密的耦合，导致开发者们不得不去滥用这个编程模型 对作业控制流和生命周期的中央式处理带来了各种各样的可扩展性问题 在这样的背景下， 通过将资源管理功能与具体编程模型分离，Yarn 得以将很多与作业调度相关的功能委托给了作业所提供的具体组件。在这样的语境下，MapReduce 实际上成为了运行在 Yarn 之上的一个应用。 除此以外，各种各样其他的公司内部需求延伸出了 Yahoo! 对 Yarn 的 10 个主要功能需求。这里只简要介绍这 10 个功能需求，关于它们产生的原因详见论文的第 2 章。 可扩展性（Scalability） 对多租户的支持（Multi-Tenancy） 服务性（Serviceability）：源自 Hadoop on Demand 已有的一个功能特性，指将资源管理框架与上层应用框架各自的版本升级进行解耦的能力 本地性感知（Locality Awareness）：HoD 无法做到的功能，指对 HDFS 上存储数据的位置的感知，将任务尽量分配到相近的结点上，提高数据本地性 高集群利用率（High Cluster Utilization） 可用性（Availability） 安全且可审计的操作（Secure and Auditable Operation）：对多租户的支持所延伸出的安全需求 支持不同的编程模型（Support for Programming Model Diversity） 弹性的资源模型（Flexible Resource Model） 前向兼容（Backward Compatability） Yarn 资源分配流程Yarn 的语境包含两类角色： 平台（Platform）：负责集群的资源管理，如 Yarn 框架（Framework）：负责逻辑执行计划的协调，如 MR 为了实现集群资源管理，Yarn 采用的也是 Master - Slave 架构。Yarn 的架构示意图如下： Yarn 架构中主要包含以下 3 种角色： Resource Manager：Yarn 集群的 Master，负责追踪集群的资源使用以及结点存活信息并进行资源调度 Application Master：负责为某个作业协调逻辑执行计划、向 Resource Manager 申请资源并生成物理执行计划，并在考虑错误可能发生的情况下协调物理计划的执行 Node Manager：运行在集群的各个结点上，负责管理结点的资源和健康信息，并与 Resource Manager 进行通信 要通过 Yarn 启动一次作业，大致需要经历如下几个步骤： 客户端向 Resource Manager 提交作业，获取执行权限与资源 当有足够的可用资源后，Resource Manager 会负责在集群的一个结点上启动一个容器并启动作业的 Application Master Application Master 会开始向 Resource Manager 发起容器请求，包括每个容器的资源需求以及所需的容器个数，同时还包含数据本地性偏好等信息 Resource Manager 会根据自己的资源分配策略尽量满足每一个应用的需求。响应时，Resource Manager 会把可用的资源绑定到一个 Token 上并返回给 Application Master 当 Application Master 获得 Token 后便会利用该 Token 向可用的 Node Manager 发出容器启动请求 Yarn 机制详解本节将基于上述 Yarn 资源分配的基本过程，详细讲解每一个 Yarn 集群资源的职责和功能。 Resource Manager作为 Yarn 集群的 Master 角色，Resource Manager 通常会作为守护进程运行在专属的机器上，利用其对全集群资源的使用情况的了解执行各种各样的调度策略。为了获知全集群的真实情况，Resource Manager 会基于心跳机制与 Node Manager 进行通信，通过 Node Manager 汇报的结点状态快照信息构建出全集群的状态。 当接收到来自客户端或 Application Master 的资源请求后，Resource Manager 会对资源进行调度，并在有足够的可用资源时以容器（Container）的形式返回资源。实际上，容器只是一个表示某个结点上指定量资源的逻辑结构。返回的内容中还包括了 Application Master 从 Node Manager 处使用这些资源时所需要的 Token。除此以外，Resource Manager 还会把在各个 Node Manager 上运行的应用任务的退出状态发送给 Application Master。 为了更有效地利用集群，Resource Manager 还可以向 Application Master 撤回已分配的资源，并在 Application Master 过长时间不作出响应时直接通知 Node Manager 杀死正在运行的任务。 值得注意的是，Resource Manager 作为独立于应用的 Yarn 组件是不会帮助应用做具体的处理的，这些逻辑将交由应用的 Application Master 或其他进程完成。类似的事情包括： Resource Manager 不会为正在运行的应用提供状态和性能状况等度量信息。这个实际上是 Application Master 的职责 Resource Manager 不会为框架向外提供已完成作业的报告信息。这个实际上应由框架自己的某个守护进程完成 可用性方面，Resource Manager 作为集群的 Master 存在单点故障的问题。 Resource Manager 在恢复时首先会从一个持久化存储中恢复状态信息。恢复完毕后，Resource Manager 会杀死当前集群上的所有容器，包括 Application Master。然后它会再为每个 Application Master 进行重新启动。 这样的恢复无疑是十分粗糙的，极大地依赖了上层应用的可用性，因此 Yarn 也在不断改进，思考如何能在保留容器的情况下完成 Resource Manager 恢复。 Application MasterApplication Master 实际上类似于 Mesos 中的 Framework Scheduler，是有上层框架所提供的组件，负责为自己所负责的作业从 Resource Manager 处获取资源并运行各个子任务。Application Master 负责在集群中协调应用的执行，但其自身实际上也像其他容器一样运行在集群中。 Application Master 会周期地与 Resource Manager 通信以汇报自己的存活以及资源需求。Application Master 所能发出的资源请求（ResourceRequest）里的信息主要包括了其对每个容器的资源需求（CPU、内存）、容器的个数以及本地性偏好和优先级等。 来自 Resource Manager 的资源响应中包含了所提供的容器以及访问这些资源所需要的 Token。基于其从 Resource Manager 接收到的资源，Application Master 可以动态地调整自己的物理执行计划，并再向 Resource Manager 更新自己的资源需求。 由于 Application Master 本身也运行在某个 Node Manager 的一个容器里，Application Master 本身也需要具备高可用性，实现针对框架自身的灾后恢复。 当 Resource Manager 应集群资源紧缺发来资源撤回的请求时，Application Master 可以选择对部分容器进行主动关闭，并在关闭前完成数据保存的操作。相比于让 Resource Manager 通过 Node Manager 直接杀死容器，由 Application Master 主动退出更有利于容器后续的恢复。 Application Master 的失效实际上不是 Yarn 所关心的：Yarn 在 Application Master 失效后只会帮其进行重新启动，Application Master 的状态恢复需要由它自己完成。 Node Manager作为 Yarn 集群的 Worker 角色，Node Manager 作为守护进程运行在集群的每一个机器上。Node Manager 会验证来自 Application Master 的容器启动请求，并基于心跳信息与 Resource Manager 通信。Node Manager 主要负责追踪结点的资源使用情况和健康状况、管理容器的生命周期、向 Resource Manager 汇报错误等。 包括 Application Master 在内的所有容器都会采用一种叫做容器启动上下文（Container Launch Context，CLC）的数据结构所表述，其中包含启动一个容器所需的环境变量、对远程存储的依赖以及启动命令等信息。 在启动容器前，Node Manager 首先会对 Application Master 发来的 Token 进行验证。验证通过后，Node Manager 会先配置容器的启动环境，包括设置环境变量、初始化监控模块、拷贝依赖文件至本地存储等。拷贝到本地存储的依赖会被维持在本地，由不同的容器所共享。最终，Node Manager 会在某个依赖确定不再被使用时将其从本地存储上移除。 Node Manager 还会负责处理来自 Resource Manager 和 Application Master 的容器终止信号。发出这样的信号有可能是由于集群资源不足，也有可能是由于容器所属的应用已执行完毕，容器负责的任务已不再被需要。当应用结束后，其在所有结点上的容器所占用的资源都会被占用。总之，无论如何， Node Manager 在容器退出后都会负责清理容器的工作目录。 除此以外，Node Manager 也会为应用任务的执行提供一些便利服务。例如，Node Manager 会在任务完成后对任务的 stdout 和 stderr 输出进行日志聚合（Log Aggregation），上传到 HDFS 中进行持久化。除此以外，Node Manager 还提供了文件驻留的功能，允许任务把部分的输出文件进行保留而不被容器退出时的清理工作所删除。 由于 Node Manager 和 Resource Manager 中一直保持着心跳连接，Resource Manager 很容易通过超时发现某个 Node Manager 失效。此时 Resource Manager 就会认定该 Node Manager 上的所有容器已被杀死，并把执行失败的信息回报给 Application Master。Node Manager 在恢复时会重新与 Resource Manager 进行同步，并清理自己的本地状态。","link":"/yarn_summary/"},{"title":"ZooKeeper —— 分布式系统协调服务","text":"来到 MIT 6.824 的 Lecture 8，我们终于要开始读大名鼎鼎的 ZooKeeper 的论文了。Introducing… 《ZooKeeper: Wait-free Coordination for Internet-scale Systems》. 在 Lecture 7 中，我们通过阅读 Spinnaker 的论文了解了如何使用 Paxos 来构建一个数据存储，其论文提到 Spinnaker 使用了 ZooKeeper 来进行集群协调以简化自身的实现，那这次我们就可以来了解一下 ZooKeeper 都提供了哪些功能了。 点击这里可以阅读我之前的几篇 MIT 6.824 系列文章： Lecture 1：MapReduce Lecture 3：Google File System Lecture 4：Primary-Backup Replication Lecture 5 &amp; 6：Raft Lecture 7：Spinnaker ZabZooKeeper 本身的难点并不多，因此在开始介绍 ZooKeeper 之前，我们不妨先来了解下 ZooKeeper 所用到的共识算法，Zab。 Zab，即 ZooKeeper Atomic Broadcast，在定位上和 Raft 比较接近，但考虑到其是 ZooKeeper 专用的公式算法，功能上相对而言会更加轻量一些。实际上，Zab 也仅仅是把自己定位为“原子广播协议”，而不是 Raft 的“分布式共识算法”。在学习过 Raft 以后，想必 Zab 提供的功能对大家而言应该不会太陌生，因此这里只对 Zab 进行简单的介绍，详情可参考论文《A Simple Totally Ordered Broadcast Protocol》。 和 Raft 类似，Zab 集群由一个 Leader 节点和若干个 Follower 节点构成，整个系统的运行同样需要大多数的 Follower 节点在线。 当需要对数据存储进行变更时，整个流程基本可以视为一个简单的 2PC 流程： Propose：Leader 向其他所有节点广播即将进行的操作；接收到的节点将该操作写入先写日志 Leader 等待，直到接收到大多数节点返回的 ACK Commit：Leader 对自己保存的数据存储备份进行变更，并通知其他节点完成相同的操作 作为一个原子广播协议，Zab 提供以下可靠性和消息顺序保证： Reliable Delivery（可靠消息投递）：如果消息 $m$ 被投递到了一个节点上，那么它最终也会被投递到其他所有节点上（注：原文使用了 Delivery 一词，有消息投递的意思，但结合上下文来看更多指节点接收到消息并写入到先写日志的过程。这里暂译作“投递”） Total Order（全序）：如果消息 $a$ 在消息 $b$ 之前被投递到一个节点上，那么所有投递了 $a$ 和 $b$ 的节点都会先投递 $a$ 再投递 $b$ Causal Order（因果序）：如果消息 $a$ Causally Precede（因果地领先于）消息 $b$，那么 $a$ 必须排在 $b$ 之前 对于上述第三项功能的 Causally Precede，文中也给出了详细解释，具体包含两种情况： 如果消息 $a$ 和 $b$ 发自同一个节点，且 $a$ 在 $b$ 之前 Propose，那么 $a$ Causally Precede $b$ 如果发生了 Leader 切换，那么上一任 Leader Propose 的消息全部 Causally Precede 新 Leader Propose 的消息 在 Leader 失效进行切换时，Zab 也会进入恢复模式，等待出现新的 Leader 并完成与大多数 Follower 节点的同步。大致上与 Raft 相同，可惜的是论文中没有详细说明 Zab 的 Leader 选举流程。 ZooKeeper在了解过 Zab 以后，我们再来看看 ZooKeeper。 按照论文的描述，Apache ZooKeeper 的定位是 Coordination Kernel，即通过提供一些基本的原语 API 来辅助上层分布式应用实现进程间的协调。从功能上看，ZooKeeper 提供了一个基于目录树结构的内存型 KV 存储：数据统一以 ZNode 的形式保存在各个 ZooKeeper 节点的内存中，数据的变更由 Leader 节点通过 Zab 协议同步给所有的 Follower 节点。 要使用 ZooKeeper 服务时，客户端只需要与 ZooKeeper 集群的任一节点建立连接即可，客户端的所有读写请求都会由该节点来负责处理。如果是读请求，节点将会使用自身保存的数据直接返回结果；写请求则会被转发给 Leader 节点进行处理。 比起仅提供同步写入的接口，ZooKeeper 也提供了异步写入的接口：客户端可以同时发起多个写请求，无须顺序等待其结果返回。这样的 API 使得客户端可以以更高的吞吐量完成数据变更，而对于来自同一个客户端的并发请求，ZooKeeper 也提供了 FIFO 的顺序保证。 除外，ZooKeeper 还支持 Watch 功能。客户端在对 ZNode 发起读请求时，可以通过额外的参数设定监听该 ZNode 未来的变更事件。当后续该 ZNode 中保存的内容发生了变化后，与客户端连接的 ZooKeeper 节点就会为客户端发来通知消息。 写操作流程这里我们来详细说说 ZooKeeper 处理一次写操作的具体流程。大体来看，ZooKeeper 对一次写操作的处理可以分为以下几步： Leader 对写操作进行预处理，转换为等价 ZooKeeper 事务 Leader 通过 Zab 协议向所有 Follower 节点 Propose 该事务 Leader 收到大多数 Follower 节点的 ACK 信息，对事务进行 Commit，应用变更到数据存储 在预处理阶段，Leader 首先会将客户端发来的写请求转换为等价的幂等 ZooKeeper 事务。每个事务都明确的表明了其执行前的期望状态和执行完成后的结果状态。考虑到 Leader 永远持有最新的数据，Leader 是最适合使用自身保存的数据来计算对应的 ZooKeeper 事务的。 这里 ZooKeeper 事务的幂等性还为 Zab 的实现带来的便利，使得 Zab 无论是在正常的数据传递还是节点恢复时都不需要保证消息传递的 exactly-once 语义，只需要保证消息传递的顺序以及 at-least-once 投递即可。 ZooKeeper 还会为每一个 ZooKeeper 事务赋予名为 ZXID 的 64 位唯一 ID 进行标识，其中低 32 位为该事务在此次任期中的序列号，高 32 位为当前 Leader 所属任期的 epoch 值，用于区分不同 Leader 发来的消息。 ZXID 中保存的信息和 Raft 的日志 ID 基本相同，差别在于 Raft 使用了两个 int64 位来保存这两个信息，而 ZooKeeper 则等价于使用了两个 int32 位来进行保存，位数上的差距使得 ZooKeeper 对应字段所能存储的值域明显地受限。在实际使用的过程中，ZooKeeper 也会发生因低 32 位空间被耗尽而导致 ZooKeeper 必须主动进行 Leader 切换的问题。 在完成 ZooKeeper 事务转换后，Leader 便会使用 Zab 协议完成该事务的广播，并最终将数据变更写入到数据存储。 快照与数据恢复为了应对节点失效的情景，ZooKeeper 也会对所保存的数据周期地保存到磁盘中，生成快照，以在节点失效重启后能够快速地从最近的快照中恢复数据状态。 ZooKeeper 在生成数据快照的过程中不需要锁死整个服务，快照生成与客户端请求处理会并发进行。这就导致了，最终 ZooKeeper 生成的数据快照可能不会对应 ZooKeeper 在任意一个时间点上的实际状态。 要进行数据恢复也很简单：考虑到 ZooKeeper 事务幂等的特性，快照只需要一并记录其开始生成时对应的 TXID，恢复时直接从该事务开始恢复即可。 结语类似于 Spinnaker，此次 ZooKeeper 也是使用 Paxos/Raft 库构建分布式备份服务的一次很好的 Case Study，主要在于两点： 作为“协调内核”服务，ZooKeeper 对外提供的 API 设计及顺序和一致性保证 ZooKeeper 针对大型分布式应用场景所做出的性能优化设计 在定位上，ZooKeeper 和 etcd 也比较相似，本质上都是基于主从备份的强一致性 KV 数据存储，但在 API 设计上还是存在一定的差异，底层设计上则更是千差万别。感兴趣的同学也建议同样去了解一下 etcd 的设计，以综合对比不同的分布式系统协调服务。 ZooKeeper 的论文中也描述了很多如何利用 ZooKeeper 提供的基本 API 实现某些常见的分布式系统协调模式，感兴趣的同学也可自行阅读论文的第 3 章。","link":"/zookeeper/"},{"title":"并发、Goroutine 与 GOMAXPROCS","text":"每当有新人加入 Go-Miami 小组的时候，他们总会提到他们有多想学习更多有关 Go 并发模型的东西。似乎并发就像这个语言的大新闻一样。不过，在我第一次听说 Go 时确实如此 – 实际上正是 Rob Pike 的 Go 并发模式这个视频让我确信我需要去学这门语言。 要想理解为什么用 Go 编写并发程序会更加容易而且更难出错，我们首先得了解一个并发程序是什么样的，以及它可能会出现哪些问题。我不会在这篇文章中讨论 CSP（Communicating Sequential Processes，通信顺序进程），尽管它确实是 Go 的 Channel 实现的基础。这篇文章主要讲述一个并发程序会是什么样的、Goroutine 在这之中起着什么样的作用、以及 GOMAXPROCS 环境变量和运行时函数会如何影响 Go 运行时和我们编写的程序的行为。 进程与线程 在我们启动一个应用程序，例如我现在正在用来写这篇文章的浏览器时，操作系统会为应有程序创建一个进程。进程的作用就像一个容器，装着应用程序在运行的过程中会使用并且维护的资源，包括内存地址空间、指向文件或设备的句柄以及线程。 一条线程是一个由操作系统负责调度的执行路径，负责在一个处理器上执行我们在函数中编写的代码。一个进程在开始时只有一条线程，即主线程。当主线程结束时，进程也随之终止，因为主线程是整个应用程序的起点。主线程可以启动新线程，而这些新线程也可以启动更多的新线程。 操作系统负责调度线程到可用的处理器上执行，且不会考虑该线程属于哪个进程。每个操作系统都会有它自己的调度算法，因此对我们来说最好还是不要编写依赖于某种调度算法的并发程序。再说了，这些调度算法在每次操作系统发布新版本时都可能会变化。 Goroutine 与并行 Go 的任何函数和方法都可以被创建为一个 Goroutine。我们可以认为 main 函数就作为一个 Goroutine 在执行，尽管 Go 运行时并没有启动这个 Goroutine。Goroutine 是轻量级的，因为它们通常只会占用很少的内存和资源，以及它们的初始栈空间很小。在 1.2 版之前的初始栈空间为 4K 而在 1.4 版之后初始栈空间为 8K。Goroutine 的栈还可以按需增长。 操作系统负责将线程调度到可用的处理器上执行，而 Go 运行时则将 Goroutine 调度到与单个操作系统线程相绑定的逻辑处理器上执行。在默认情况下，Go 运行时会使用一个逻辑处理器来运行我们程序创建的所有 Goroutine。不过，即使只用这唯一一个逻辑处理器和操作系统线程，成千上万个 Goroutine 仍然可以以惊人的效率和性能并发执行。尽管并不推荐你添加更多的逻辑处理器，但如果你想要并行地运行 Goroutine，Go 也允许你通过 GOMAXPROCS 环境变量和运行时函数来添加逻辑处理器。 并发（Concurrency）并不是并行（Parallelism）。并行是指多个线程在多个处理器上同时执行代码。如果你通过配置让运行时使用多个逻辑处理器，调度器就会将 Goroutine 分配到这些逻辑处理器上，如此一来这些 Goroutine 便会运行在不同的操作系统线程中。然而，要想真正实现并行，你需要将你的程序运行在一个拥有多个物理处理器的机器上。否则，这些 Goroutine 只会在一个物理处理器上并发执行，即使 Go 运行时在使用多个逻辑处理器。 并发案例 接下来我们来创建一个小程序来看看 Go 是如何并发运行 Goroutine 的。在执行这个案例时我们会使用一个逻辑处理器： 123456789101112131415161718192021222324252627282930313233343536package mainimport ( &quot;fmt&quot; &quot;runtime&quot; &quot;sync&quot;)func main() { runtime.GOMAXPROCS(1) // &lt;----- var wg sync.WaitGroup wg.Add(2) fmt.Println(&quot;Starting Go Routines&quot;) go func() { defer wg.Done() for char := 'a'; char &lt; 'a'+26; char++ { fmt.Printf(&quot;%c &quot;, char) } }() go func() { defer wg.Done() for number := 1; number &lt; 27; number++ { fmt.Printf(&quot;%d &quot;, number) } }() fmt.Println(&quot;Waiting To Finish&quot;) wg.Wait() fmt.Println(&quot;\\nTerminating Program&quot;)} 这个程序使用 go 关键字和两个匿名函数启动了两个 Goroutine。第一个 Goroutine 负责以小写字母打印英文字母表，第二个 Goroutine 则打印数字 1 到数字 26。如果我们运行该程序我们将得到如下输出： 12345Starting Go RoutinesWaiting To Finisha b c d e f g h i j k l m n o p q r s t u v w x y z 1 2 3 4 5 6 7 8 9 10 1112 13 14 15 16 17 18 19 20 21 22 23 24 25 26Terminating Program 从输出来看，我们可以看到代码是并发执行的。在两个 Goroutine 启动后，主 Goroutine 开始等待这两个 Goroutine 完成。这么做的原因是一旦主 Goroutine 结束后，应用程序就终止了。使用 WaitGroup 即可很好地让 Goroutine 相互知会它们何时结束执行。 我们可以看到，第一个 Goroutine 在完成打印所有 26 个字母后才轮到第二个 Goroutine 打印它需要打印的 26 个数字。因为第一个 Goroutine 在一微秒之内就完成了它的工作，因此我们没能看到调度器在第一个 Goroutine 执行完成前中断它。我们可以通过在第一个 Goroutine 中调用 Sleep 函数来让调度器切换 Goroutine： 1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( &quot;fmt&quot; &quot;runtime&quot; &quot;sync&quot; &quot;time&quot;)func main() { runtime.GOMAXPROCS(1) var wg sync.WaitGroup wg.Add(2) fmt.Println(&quot;Starting Go Routines&quot;) go func() { defer wg.Done() time.Sleep(1 * time.Microsecond) // &lt;--------- for char := 'a'; char &lt; 'a'+26; char++ { fmt.Printf(&quot;%c &quot;, char) } }() go func() { defer wg.Done() for number := 1; number &lt; 27; number++ { fmt.Printf(&quot;%d &quot;, number) } }() fmt.Println(&quot;Waiting To Finish&quot;) wg.Wait() fmt.Println(&quot;\\nTerminating Program&quot;)} 这次我们在第一个 Goroutine 启动时就调用了 Sleep 函数，这使得调度器切换了两个 Goroutine： 12345Starting Go RoutinesWaiting To Finish1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ab c d e f g h i j k l m n o p q r s t u v w x y zTerminating Program 并行案例 在我们上面的两个示例中，Goroutine 都是在并发执行而不是并行执行。接下来我们对代码做些修改来让 Goroutine 并行执行。我们只需要添加第二个逻辑处理器来让调度器使用两条线程即可： 123456789101112131415161718192021222324252627282930313233343536package mainimport ( &quot;fmt&quot; &quot;runtime&quot; &quot;sync&quot;)func main() { runtime.GOMAXPROCS(2) // &lt;------------ var wg sync.WaitGroup wg.Add(2) fmt.Println(&quot;Starting Go Routines&quot;) go func() { defer wg.Done() for char := 'a'; char &lt; 'a'+26; char++ { fmt.Printf(&quot;%c &quot;, char) } }() go func() { defer wg.Done() for number := 1; number &lt; 27; number++ { fmt.Printf(&quot;%d &quot;, number) } }() fmt.Println(&quot;Waiting To Finish&quot;) wg.Wait() fmt.Println(&quot;\\nTerminating Program&quot;)} 程序输出如下： 12345Starting Go RoutinesWaiting To Finisha b 1 2 3 4 c d e f 5 g h 6 i 7 j 8 k 9 10 11 12 l m n o p q 13 r s 14t 15 u v 16 w 17 x y 18 z 19 20 21 22 23 24 25 26Terminating Program 我们每次运行该程序时都会得到不一样的结果。调度器的行为在每次程序执行时都不尽相同。我们可以看到 Goroutine 是真的在并行执行。两个 Goroutine 都同时开始运行，而且你能看到它们都在争夺标准输出来输出它们的结果。 结语 我们可以为调度器添加更多的逻辑处理器，但这并不意味着我们应该这么做。Go 开发团队如此设计运行时的默认设置是有原因的，由其是默认只使用一个逻辑处理器的设置。你要记住，随意地添加逻辑处理器以并行地执行 Goroutine 并不一定能为你的程序带来更高的性能。永远要记得为你的程序进行基准测试和性能分析并在绝对必要时才去修改 Go 的运行时配置。 为我们的程序引入并发的问题在于，我们的 Goroutine 最终都会开始尝试访问同一个资源，有可能还是同时尝试。对共享资源执行的读写操作必须是原子的。也就是说，同一时间只能有一个 Goroutine 进行读写，否则我们的程序就会出现竞态条件。要想了解更多有关竞态条件的事可以阅读我的另一篇文章。 Channel 便是我们在 Go 中编写安全而优雅的并发应用程序的方法，使用它可以很好地消除竞态条件并让编写并发程序变得有趣起来。既然现在我们知道 Goroutine 是如何工作、如何被调度以及如何能并行执行，Channel 就是下一个我们需要学习的东西了。","link":"/concurrency_goroutine_gomaxprocs/"},{"title":"Groovy 教程 - 与 Java 的差异","text":"这是一篇译文，读者可前往 Groovy Getting Started - Differences with Java 阅读原文。 Groovy 语言在设计时便考虑到要尽可能让语言本身令 Java 程序员感到自然。如此，我们在设计 Groovy 时则尽可能让其少出现出人意料的地方，尤其是对于那些有着 Java 背景的开发者。 在这篇文章中我们将列举几处 Java 和 Groovy 的显著差异。 1 默认引入如下的这些包和类都会被默认引入 —— 也就是说，你不需要显式的 import 语句即可使用它们： java.io.* java.lang.* java.math.BigDecimal java.math.BigInteger java.net.* java.util.* groovy.lang.* groovy.util.* 2 多方法 在 Groovy 中，方法会在运行时被选择并调用。这种机制被称为运行时分发（Runtime Dispatch）或多方法（Multi-methods）。这意味着具体被调用的方法会在运行时根据实参的类型被挑选。在 Java 中则是截然相反：具体被调用的方法会在编译期根据实参的声明类型被挑选。 如下 Java 代码可以同时在 Java 环境和 Groovy 环境中编译运行，但却会有不同的行为： 12345678int method(String arg) { return 1;}int method(Object arg) { return 2;}Object o = &quot;Object&quot;;int result = method(o); 在 Java 中，你会有： 1assertEquals(2, result); 而在 Groovy 中则会有： 1assertEquals(1, result); 这是因为 Java 会利用静态信息类型（变量 o 被声明为 Object）来挑选被调用的方法，而 Groovy 则会在方法被确实调用的运行时才进行选择。由于调用时所使用的实参是一个 String，那么 String 版本的方法就被调用了。 3 数组初始化语句 在 Groovy 中，{...} 块被保留用作定义闭包。也就是说，你不能像如下语句这样来创建数组字面量： 1int[] array = { 1, 2, 3 }; 你需要这样： 1int[] array = [1, 2, 3] 4 包可见性在 Groovy 中，不给出任何修饰符并不会使得一个类的域像 Java 那样仅在该包内可见： 123class Person { String name} 在 Groovy 中这样会创建出一个属性（Property），也就是一个 private 域和对应的 Getter 和 Setter 方法。 通过为域添加上 @PackageScope 注解即可将其声明为包内可见： 123class Person { @PackageScope String name} 5 ARM 块 Groovy 不支持 Java7 的自动资源管理（Automatic Resource Management, ARM）代码块，而是提供了各种不同的利用了闭包的方法，使得我们可以使用更简洁的写法来达成同样的效果。例如： 1234567891011Path file = Paths.get(&quot;/path/to/file&quot;);Charset charset = Charset.forName(&quot;UTF-8&quot;);try (BufferedReader reader = Files.newBufferedReader(file, charset)) { String line; while ((line = reader.readLine()) != null) { System.out.println(line); }} catch (IOException e) { e.printStackTrace();} 可被写作： 123new File('/path/to/file').eachLine('UTF-8') { println it} 或者，如果你想让它看起来更像 Java 的话，也可以这样写： 12345new File('/path/to/file').withReader('UTF-8') { reader -&gt; reader.eachLine { println it }} 6 内部类 Groovy 的匿名内部类和嵌套类在某种程度上以 Java 为指导，但你不需要再翻阅 Java 语言规范并苦想二者之间的差异。实际的实现实际上与 groovy.lang.Closure 很接近，只是还多了一点其他的不同，例如无法访问私有的域或方法，但局部变量则不需要被声明为 final 了。 6.1 静态内部类如下为静态内部类的案例： 12345class A { static class B {}}new A.B() 实际上，Groovy 对静态内部类的支持是最好的，因此如果你确实需要一个内部类的话，你应该将其声明为静态的。 6.2 匿名内部类12345678910111213import java.util.concurrent.CountDownLatchimport java.util.concurrent.TimeUnitCountDownLatch called = new CountDownLatch(1)Timer timer = new Timer()timer.schedule(new TimerTask() { void run() { called.countDown() }}, 0)assert called.await(10, TimeUnit.SECONDS) 6.3 创建非静态内部类的实例在 Java 中，你可以这样： 123456789public class Y { public class X {} public X foo() { return new X(); } public static X createX(Y y) { return y.new X(); }} Groovy 并不支持像 y.new X() 这样的语法。你应该像如下代码那样，写成 new X(y)： 123456789public class Y { public class X {} public X foo() { return new X() } public static X createX(Y y) { return new X(y) }} 值得注意的是，Groovy 允许你在调用只有一个参数的方法时不给出任何实参。如此一来参数值会被设置为 null。对构造器的调用同样遵循此规则。因此你有可能会写成 new X() 而不是 new X(this)。由于这样做在某种情况下也有可能是合理的，因此我们还没有找出一个很好的办法来避免这样的问题。 7 Lambda 表达式Java8 支持 Lambda 表达式和方法引用： 12Runnable run = () -&gt; System.out.println(&quot;Run&quot;);list.forEach(System.out::println); Java8 的 Lambda 表达式在某种程度上可以被看作是匿名内部类。Groovy 不支持这样的语法，但支持闭包： 12Runnable run = { println 'run' }list.each { println it } // or list.each(this.&amp;println) 8 GString 由于带双引号的字符串字面量会被解析为 GString 对象，如果一个类包含一个 String 字面量其中包含了美金符号，Groovy 可能会无法编译或是给出与 Java 编译器所给出的大相径庭的代码。 尽管 Groovy 能够根据 API 声明的参数类型来对 GString对象和 String 对象家进行自动转换，你仍然需要注意那些将参数类型声明为 Object 但在方法体内对实参类型进行判断的 Java API。 9 String 和 Character 字面量 在 Groovy 中，带单引号的字符串字面量被用作 String 对象的创建，而带双引号的字符串字面量则会创建出 GString 或 String 对象，取决于字面两种是否包含插值占位符。 123assert 'c'.getClass()==Stringassert &quot;c&quot;.getClass()==Stringassert &quot;c${1}&quot;.getClass() in GString 只有当赋值给一个类型为 char 的变量时，Groovy 才会自动地将一个只包含一个字符的 String 转换为 char 类型。当你想调用一个参数类型为 char 的方法时，你需要显式地对类型进行转换或者预先进行类型转换。 123456789char a='a'assert Character.digit(a, 16)==10 : 'But Groovy does boxing'assert Character.digit((char) 'a', 16)==10try { assert Character.digit('a', 16)==10 assert false: 'Need explicit cast'} catch(MissingMethodException e) {} Groovy 支持两种不同的类型转换语法，而当转换包含多个字符的字符串至 char 时，两种语法会有不同的表现。Groovy 风格的类型转换会更为智能，只以字符串的第一个字符作为转换结果，而 C 风格的强制类型转换则会直接抛出异常。 123456789101112// for single char strings, both are the sameassert ((char) &quot;c&quot;).class==Characterassert (&quot;c&quot; as char).class==Character// for multi char strings they are nottry { ((char) 'cx') == 'c' assert false: 'will fail - not castable'} catch(GroovyCastException e) {}assert ('cx' as char) == 'c'assert 'cx'.asType(char) == 'c' 10 基本数据类型和包装类 由于在 Groovy 中所有东西都是对象，Groovy 会对对基本数据类型的引用进行自动包装。鉴于此，Groovy 不会像 Java 那样让类型扩充享有比装箱更高的优先级。例如： 12345678910int im(i)void m(long l) { // 注1 println &quot;in m(long)&quot;}void m(Integer i) { // 注2 println &quot;in m(Integer)&quot;} 1 如果是 Java 的话就会调用这个方法，因为类型扩充比装拆箱享有更高的优先级 2 Groovy 则会调用这个方法，因为所有对基本数据类型变量的引用的类型实际上都是为其对应的包装类 11 == 的行为 在 Java 中，== 用于检验基本数据类型的相等性和引用的一致性。而在 Groovy 中，对于 Comparable 类，== 会被理解为 a.compareTo(b) == 0，否则理解为 a.equals(b)。要检测引用的一致性，需要这样写：a.is(b)。 12 转换 Java 会自动进行类型扩充或类型收窄的转换。 转换至 转换自 boolean byte short char int long float double boolean - N N N N N N N byte N - Y C Y Y Y Y short N C - C Y Y Y Y char N C C - Y Y Y Y int N C C C - Y T Y long N C C C C - T T float N C C C C C - Y double N C C C C C C - * 'Y' 即指 Java 可以自动执行该转换，'C' 即指 Java 可在显式声明了强制类型转换时执行该转换，'T' 即指 Java 可执行该转换但会导致有效数据被删节，'N' 即指 Java 无法执行该转换。 Groovy 则大大扩充了这些转换规则。 转换至 转换自 boolean Boolean byte Byte short Short char Character int Integer long Long BigInteger float Float double Double BigDecimal boolean - B N N N N N N N N N N N N N N N N Boolean B - N N N N N N N N N N N N N N N N byte T T - B Y Y Y D Y Y Y Y Y Y Y Y Y Y Byte T T B - Y Y Y D Y Y Y Y Y Y Y Y Y Y short T T D D - B Y D Y Y Y Y Y Y Y Y Y Y Short T T D T B - Y D Y Y Y Y Y Y Y Y Y Y char T T Y D Y D - D Y D Y D D Y D Y D D Character T T D D D D D - D D D D D D D D D D int T T D D D D Y D - B Y Y Y Y Y Y Y Y Integer T T D D D D Y D B - Y Y Y Y Y Y Y Y long T T D D D D Y D D D - B Y T T T T Y Long T T D D D T Y D D T B - Y T T T T Y BigInteger T T D D D D D D D D D D - D D D D T float T T D D D D T D D D D D D - B Y Y Y Float T T D T D T T D D T D T D B - Y Y Y double T T D D D D T D D D D D D D D - B Y Double T T D T D T T D D T D T D D T B - Y BigDecimal T T D D D D D D D D D D D T D T D - * 'Y' 即指 Groovy 可以执行该转换，'D' 即指 Groovy 进行动态编译或遇到显式类型转换语句时可执行该转换，'T' 即指 Groovy 可以执行该转换但有效数据会被删节，'B' 即指该转换为装箱/拆箱操作，'N' 即指 Groovy 不能执行该转换。 当转换至 boolean/Boolean 时，Groovy 会使用 Groovy 真值；从数字到字符的转换实为从 Number.intvalue() 到 char 的强制转换；当转换至 BigInteger 或 BigDecimal 时，如果源类型为 Float 或 Double，Groovy 会使用 Number.doubleValue() 来构建结果，否则则使用 toString() 的 结果来构建。其他类型转换的行为均如 java.lang.Number 类所定义。 13 新增的关键词 Groovy 比起 Java 新增了如下几个关键词。不要将它们用作变量名等标识符： as def in trait","link":"/groovy-differences-with-java/"},{"title":"Groovy 教程 - 整合 Groovy 至应用程序","text":"这是一篇译文，读者可前往 Groovy Getting Started - Integrating Groovy into applications 阅读原文。 1 Groovy 整合机制 Groovy 语言提供了多种在运行时将其整合至（Java 甚至 Groovy）应用程序中的方法，包括了从最简单代码的执行到完整的应用程序整合缓存和编译器定制化。 本章中所有的示例都使用 Groovy 编写而成，但这些整合机制同样可用于 Java。 1.1 Eval 在运行时动态执行 Groovy 代码最简单的方式莫过于使用 groovy.util.Eval 类了，我们只需调用该类的 me 方法即可： 1234import groovy.util.Evalassert Eval.me('33*3') == 99assert Eval.me('&quot;foo&quot;.toUpperCase()') == 'FOO' Eval 类还提供了许多其他方法来允许用户传入参数进行简单的运算： 1234assert Eval.x(4, '2*x') == 8 // 注1assert Eval.me('k', 4, '2*k') == 8 // 注2assert Eval.xy(4, 5, 'x*y') == 20 // 注3assert Eval.xyz(4, 5, 6, 'x*y+z') == 26 // 注4 包含一个名为 x 的参数的简单运算 包含一个自定义的名为 k 的参数的简单运算 包含两个分别名为 x 和 y 的参数的简单运算 包含三个分别名为 x、y 和 z 的参数的简单运算 尽管 Eval 类使得我们可以很方便地运行简单的脚本，但它并不具备很好的横向扩展性：它不会对脚本进行任何缓存，也并不是设计来用于执行长度超过一行的脚本的。 1.2 GroovyShell1.2.1 多种代码来源 比起 Eval，groovy.lang.GroovyShell 类提供了更好的执行脚本的方式，同时还提供了对脚本实例运行结果进行缓存的支持。比起像 Eval 一般运行脚本并返回结果，GroovyShell 类还提供了更多的做法： 1234567def shell = new GroovyShell() // 注1def result = shell.evaluate '3*5' // 注2def result2 = shell.evaluate(new StringReader('3*5')) // 注3assert result == result2def script = shell.parse '3*5' // 注4assert script instanceof groovy.lang.Scriptassert script.run() == 15 // 注5 创建了一个 GroovyShell 实例 可以像 Eval 那样直接执行脚本代码 也可以从多种不同的来源中读取代码（String、Reader、File、InputStream） parse 方法返回一个 Script 实例，可以此延迟脚本的执行 Script 类提供了 run 方法 1.2.2 在脚本与应用程序间共享数据 我们可以通过 groovy.lang.Binding 类来实现脚本与应用程序间的数据共享： 123456789def sharedData = new Binding() // 注1def shell = new GroovyShell(sharedData) // 注2def now = new Date()sharedData.setProperty('text', 'I am shared data!') // 注3sharedData.setProperty('date', now) // 注4String result = shell.evaluate('&quot;At $date, $text&quot;') // 注5assert result == &quot;At $now, I am shared data!&quot; 创建 Binding 实例用于存储共享数据 创建即将使用这些共享数据的 GroovyShell 实例 将一个 String 添加到了 Binding 中 讲一个 Date 添加到了 Binding 中（你可以放入除基本类型外的其他类型的数据 执行脚本 值得注意的是我们还可以在脚本中向 Binding 写入数据： 123456def sharedData = new Binding() // 注1def shell = new GroovyShell(sharedData) // 注2shell.evaluate('foo=123') // 注3assert sharedData.getProperty('foo') == 123 // 注4 创建 Binding 实例 创建即将使用这些共享数据的 GroovyShell 实例 通过使用一个未声明的变量来讲数据存储到 Binding 中 从应用程序中获取数据 值得注意的是，如果你想要将数据写入到 Binding 中，你需要使用未声明的变量。像下面的例子那样使用 def 或 explicit 类型是不会将数据写入到 Binding 中的，因为这样做实际上是创建了一个局部变量： 12345678910def sharedData = new Binding()def shell = new GroovyShell(sharedData)shell.evaluate('int foo=123')try { assert sharedData.getProperty('foo')} catch (MissingPropertyException e) { println &quot;foo is defined as a local variable&quot;} 当你想要在多线程环境中使用共享数据时必须提高警惕：你所传递给 GroovyShell 的 Binding 实例不是线程安全的，而且它被所有脚本所共享。 我们倒是可以通过利用由 parse 方法返回的 Script 实例来绕过共享的 Binding 实例： 123456789101112def shell = new GroovyShell()def b1 = new Binding(x:3) // 注1def b2 = new Binding(x:4) // 注2def script = shell.parse('x = 2*x')script.binding = b1script.run()script.binding = b2script.run()assert b1.getProperty('x') == 6assert b2.getProperty('x') == 8assert b1 != b2 将变量 x = 3 保存到 b1 中 将变量 x = 4 保存到 b2 中 然而，你仍该意识到，这样做的时候你则是在共享同一个 Script 实例的使用，因此如果你想要让两个线程同时使用同样的脚本的话，这样的做法并不合适。在这种情况下，你应创建两个不同的 Script 实例： 123456789101112131415def shell = new GroovyShell()def b1 = new Binding(x:3)def b2 = new Binding(x:4)def script1 = shell.parse('x = 2*x') // 注1def script2 = shell.parse('x = 2*x') // 注2assert script1 != script2script1.binding = b1 // 注3script2.binding = b2 // 注4def t1 = Thread.start { script1.run() } // 注5def t2 = Thread.start { script2.run() } // 注6[t1,t2]*.join() // 注7assert b1.getProperty('x') == 6assert b2.getProperty('x') == 8assert b1 != b2 创建用于 1 号线程的 Script 实例 创建用于 2 号线程的 Script 实例 将第一个 Binding 赋予第一个 Script 将第二个 Binding 赋予第二个 Script 在一个独立的线程中启动第一个 Script 在一个独立的线程中启动第二个 Script 等待运行结束 除非你需要像上述案例那样的线程安全性，否则我们更推荐你直接使用 GroovyClassLoader。 1.2.3 自定义脚本类 我们了解到 parse 方法可以返回 groovy.lang.Script 实例，但它同样可以返回自定义的类，只要该类扩展了 Script 类。这么做能像下述的案例那样让 Script 实例支持更多的操作： 1234567abstract class MyScript extends Script { String name String greet() { &quot;Hello, $name!&quot; }} 这个自定义类定义了一个叫做 name 的属性以及一个叫做 greet 的新方法。通过一些自定义设置，我们可以使用这个类作为脚本的基类： 12345678910import org.codehaus.groovy.control.CompilerConfigurationdef config = new CompilerConfiguration() // 注1config.scriptBaseClass = 'MyScript' // 注2def shell = new GroovyShell(this.class.classLoader, new Binding(), config) // 注3def script = shell.parse('greet()') // 注4assert script instanceof MyScriptscript.setName('Michel')assert script.run() == 'Hello, Michel!' 创建 CompilerConfiguration 实例 令其使用 MyScript 类作为脚本基类 然后在创建 GroovyShell 时使用该 CompilerConfiguration 实例 现在返回的脚本可以访问新方法 greet 了 你可以进行的设置当然不止 scriptBaseClass。你可以使用任意 CompilerConfiguration 设置，包括编译定制器。 1.3 GroovyClassLoader 在之前的章节中，我们看到 GroovyShell 可以很方便地执行脚本，但它并不适合用于编译除脚本以外的东西。在 GroovyShell 内部它实际上使用了 groovy.lang.GroovyClassLoader，而后者则是运行时编译并载入类的核心所在。 通过使用 GroovyClassLoader，你可以载入类而不是脚本实例： 1234567import groovy.lang.GroovyClassLoaderdef gcl = new GroovyClassLoader() // 注1def clazz = gcl.parseClass('class Foo { void doIt() { println &quot;ok&quot; } }') // 注2assert clazz.name == 'Foo' // 注3def o = clazz.newInstance() // 注4o.doIt() // 注5 创建一个 GroovyClassLoader 实例 parseClass 方法会返回一个 Class 实例 可以看到返回的类确实是在脚本中定义的类 你也可以创建一个该类的实例，可见返回的确实是类而不是脚本 你也可以调用所创建实例的方法 GroovyClassLoader 会维持对所有由其所创建的类的引用，而这很容易导致内存泄漏。具体来说，如果你使用一个 String 对象来让 GroovyClassLoader 对同样的脚本进行两次处理，你实际上会获得两个不同的类！ 12345678import groovy.lang.GroovyClassLoaderdef gcl = new GroovyClassLoader()def clazz1 = gcl.parseClass('class Foo { }') // 注1def clazz2 = gcl.parseClass('class Foo { }') // 注2assert clazz1.name == 'Foo' // 注3assert clazz2.name == 'Foo'assert clazz1 != clazz2 // 注4 动态创建一个名为 Foo 的类 使用第二次 parseClass 方法调用创建一个一模一样的类 两个类拥有相同的名称 但它们是两个不同的类！ 原因在于 GroovyClassLoader 不会记录源代码文本。如果你希望它返回相同的 Class 实例，你则必须像下面的示例那样使用文件作为代码来源： 123456def gcl = new GroovyClassLoader()def clazz1 = gcl.parseClass(file) // 注1def clazz2 = gcl.parseClass(new File(file.absolutePath)) // 注2assert clazz1.name == 'Foo' // 注3assert clazz2.name == 'Foo'assert clazz1 == clazz2 // 注4 从一个 File 中解析类 使用不同的 File 实例进行类解析，但两个 File 在物理上指向同一个文件 两个类有相同的名称 现在，它们确实是相同的 Class 实例了 使用 File 作为输入时，GroovyClassLoader 能够对生成的类文件进行缓存，这就避免了在运行时对同样的代码生成多个不同的类了。 1.4 GroovyScriptEngine 对于那些需要处理脚本重载与脚本依赖的应用程序来说，groovy.util.GroovyScriptEngine 提供了扩展性强的良好基础。前面我们看到，GroovyShell 专注于处理各个独立的 Script 对象，GroovyClassLoader 负责处理任意 Groovy 类的动态编译与载入，而接下来你将看到，GroovyScriptEngine 是在 GroovyClassLoader 之上添加了新的一层封装，可用于处理脚本的依赖与重载。 为此，我们会在接下来的案例中先创建一个 GroovyScriptEngine 并在一个无限循环中运行它。首先，你需要创建一个文件夹并在里面放入如下脚本文件： ReloadingTest.groovy 12345678class Greeter { String sayHello() { def greet = &quot;Hello, world!&quot; greet }}new Greeter() 然后你就能用 GroovyScriptEngine 运行这个代码了： 1234567def binding = new Binding()def engine = new GroovyScriptEngine([tmpDir.toURI().toURL()] as URL[]) // 注1while (true) { def greeter = engine.run('ReloadingTest.groovy', binding) // 注2 println greeter.sayHello() // 注3 Thread.sleep(1000)} 创建一个 GroovyScriptEngine 并指定其在我们的源文件夹中寻找源文件 运行脚本，返回一个 Greeter 实例 打印信息 这样，每秒你都会看到其打印一行信息： 123Hello, world!Hello, world!... 不要 中断脚本的执行，现在我们将 ReloadingTest.groovy 文件的内容修改至如下： 12345678class Greeter { String sayHello() { def greet = &quot;Hello, Groovy!&quot; greet }}new Greeter() 你应该能看到打印的信息发生了如下的改变： 12345Hello, world!...Hello, Groovy!Hello, Groovy!... 我们还能依赖另一个脚本。为此，我们先不要中断刚才正在执行的脚本，并在刚刚的文件夹中创建文件如下： Dependency.groovy 123class Dependency { String message = 'Hello, dependency 1'} 然后更新 ReloadingTest.groovy 脚本如下： 12345678910import Dependencyclass Greeter { String sayHello() { def greet = new Dependency().message greet }}new Greeter() 这次，你会看到打印信息变成了这样： 12345Hello, Groovy!...Hello, dependency 1!Hello, dependency 1!... 最后，你还能在不修改 ReloadingTest.groovy 文件的情况下对 Dependency.groovy 文件进行修改： 123class Dependency { String message = 'Hello, dependency 2'} 之后你应该能观察到依赖文件被重新载入了： 1234Hello, dependency 1!...Hello, dependency 2!Hello, dependency 2! 1.5 CompilationUnit 最后，我们还可以通过直接使用 org.codehaus.groovy.control.CompilationUnit 类来进行更多的操作。该类负责确定编译各个步骤的具体行为，还能让你在编译中加入新的步骤甚至在指定的步骤中停止编译。 然而，我们不推荐你重载 CompilationUnit，除非其他标准的做法都无法满足你的需求。 2 Bean 脚本框架 Bean 脚本框架（Bean Scripting Framework，BSF）尝试为 Java 创建一套 API 用以调用脚本语言。可惜的是，它已经被最新的 JSR-223 API 所替代而且很长一段时间没有更新了。 Groovy 的 BSF 引擎由 org.codehaus.groovy.bsf.GroovyEngine 类所实现。然而，BSF 的 API 通常会将这个细节所遮蔽。你只需要在 BSF API 中像处理其他脚本语言那样使用 Groovy 即可。 由于 Groovy 本身有对与 Java 应用程序整合的原生支持，大多数情况下你不需要为 BSF 操心太多，除非你还想要调用如 JRuby 等其他语言，或者你希望你的应用程序与你所使用的脚本语言之间保持极度松耦合的关系。 2.1 热身入门 假设你已经把 Groovy 和 BSF 的 JAR 包放到了类路径中，你可以使用如下 Java 代码来运行一段 Groovy 脚本样例了： 1234String myScript = &quot;println('Hello World')\\n return [1, 2, 3]&quot;;BSFManager manager = new BSFManager();List answer = (List) manager.eval(&quot;groovy&quot;, &quot;myScript.groovy&quot;, 0, 0, myScript);assertEquals(3, answer.size()); 2.2 传递参数 BSF 还允许你在 Java 应用程序和脚本语言之间传递 Bean 对象。你可以通过注册/注销 Bean 类的方式使 BSF 得知其存在。之后你可以通过 BSF 提供的方法来对 Bean 类进行检索。除此之外，你还可以声明/反声明 Bean 类，如此一来便能注册该 Bean 类且使得脚本语言也能直接使用它们。当我们使用 Groovy 时通常会使用第二种方法，示例如下： 1234BSFManager manager = new BSFManager();manager.declareBean(&quot;xyz&quot;, 4, Integer.class);Object answer = manager.eval(&quot;groovy&quot;, &quot;test.groovy&quot;, 0, 0, &quot;xyz + 1&quot;);assertEquals(5, answer); 2.3 其他调用选项 上面的案例中均使用了 eval 方法。除此之外 BSF 还提供了很多其他方法功能使用（详情可参阅 BSF 文档）。其中包括 apply 方法，其允许你在脚本语言中定义一个匿名函数并将其应用于给定的参数。Groovy 则通过闭包来支持该功能。示例如下： 123456789BSFManager manager = new BSFManager();Vector&lt;String&gt; ignoreParamNames = null;Vector&lt;Integer&gt; args = new Vector&lt;Integer&gt;();args.add(2);args.add(5);args.add(1);Integer actual = (Integer) manager.apply(&quot;groovy&quot;, &quot;applyTest&quot;, 0, 0, &quot;def summer = { a, b, c -&gt; a * 100 + b * 10 + c }&quot;, ignoreParamNames, args);assertEquals(251, actual.intValue()); 2.4 访问脚本引擎 尽管在一般情况下你不会用到，但 BSF 提供了一些方法使你可以直接访问脚本引擎。脚本引擎的其中一个功能为对给定的对象调用方法。示例如下： 123456BSFManager manager = new BSFManager();BSFEngine bsfEngine = manager.loadScriptingEngine(&quot;groovy&quot;);manager.declareBean(&quot;myvar&quot;, &quot;hello&quot;, String.class);Object myvar = manager.lookupBean(&quot;myvar&quot;);String result = (String) bsfEngine.call(myvar, &quot;reverse&quot;, new Object[0]);assertEquals(&quot;olleh&quot;, result); 3 JSR-223 javax.script API JSR-223 为一套从 Java 中调用脚本框架的标准 API。它从 Java6 开始加入 Java 平台，并企图向开发者提供一套从 Java 中调用多种语言的通用框架。Groovy 本身就提供了功能丰富的整合机制，所以如果你并不打算在同一个应用程序中使用其他脚本语言，我们更推荐你使用 Groovy 的整合机制而不是功能有限的 JSR-223 API。 你需要通过如下代码来初始化 JSR-223 引擎使其能从 Java 访问 Groovy： 123456import javax.script.ScriptEngine;import javax.script.ScriptEngineManager;import javax.script.ScriptException;ScriptEngineManager factory = new ScriptEngineManager();ScriptEngine engine = factory.getEngineByName(&quot;groovy&quot;); 然后你就能很轻松地运行 Groovy 脚本了： 12Integer sum = (Integer) engine.eval(&quot;(1..10).sum()&quot;);assertEquals(new Integer(55), sum); 你还能在 Java 和 Groovy 间共享变量： 1234engine.put(&quot;first&quot;, &quot;HELLO&quot;);engine.put(&quot;second&quot;, &quot;world&quot;);String result = (String) engine.eval(&quot;first.toLowerCase() + ' ' + second.toUpperCase()&quot;);assertEquals(&quot;hello WORLD&quot;, result); 如下示例展示了如何调用一个可调用函数： 12345678910import javax.script.Invocable;ScriptEngineManager factory = new ScriptEngineManager();ScriptEngine engine = factory.getEngineByName(&quot;groovy&quot;);String fact = &quot;def factorial(n) { n == 1 ? 1 : n * factorial(n - 1) }&quot;;engine.eval(fact);Invocable inv = (Invocable) engine;Object[] params = {5};Object result = inv.invokeFunction(&quot;factorial&quot;, params);assertEquals(new Integer(120), result); 默认情况下脚本引擎会对脚本函数维持强引用。你可以通过将一个名为 #jsr223.groovy.engine.keep.globals 的引擎属性设置到脚本上下文中来改变此行为。将该变量设置为 phantom 来使用虚引用、设置为 weak 来使用弱引用、设置为 soft 来使用软引用。该变量值不区分大小写，但设置为任何其他 String 值都会使引擎继续使用强引用。","link":"/groovy-integrating/"},{"title":"Java8 时间 API","text":"Java8 中最为人津津乐道的新改变恐怕当属函数式 API 的加入。但实际上，Java8 所加入的新功能远不止这个。本文将基于《Java SE8 for the Really Impatient》的第 5 章，归纳一下 Java8 加入的位于 java.time 包下的日期和时间 API。 时间点与时间间隔在我们常说的四维空间体系中，时间轴往往作为除长宽高三轴以外的第四轴。时间轴由无穷多个时间点组成，而两个时间点之间的距离组成一个时间间隔。相较于我们常说的日期、时间，时间点本身所携带的信息是很少的，不会携带如时区等冗余的信息。作为时间轴上的一个点，我们可以将它称为绝对时间。 Java8 引入了 Instant 类（瞬时）来表示时间轴上的一个点。Instant 的构造方法是 private 的，我们只能通过调用它的静态工厂方法来产生一个 Instant 实例。其中最常用的是 Instant.now() 方法，返回当前的时间点。Instant 类也实现了 comparesTo 和 equals 方法来对比两个瞬时点。 通过调用 Duration.between() 方法我们便可以计算两个时间点之间的时间间隔： 12345678Instant start = Instant.now();runAlgorithm();Instant end = Instant.now();Duration timeElapsed = Duration.between(start, end);long millis = timeElapsed.toMillis(); between 方法返回一个 Duration 实例。Duration 内部以 long 成员来存储时间间隔信息，最小单位可去到纳秒，同时提供了如 toMillis 、 toSeconds 等方法。 Instant 和 Duration 类常用的方法包括如下： 方法 描述 plus、minus 对当前 Instant 或 Duration 增加或减少一段时间 plusNanos、plusMillis、plusSeconds、plusMinutes、plusHours、plusDays 根据指定的时间单位，对当前 Instant 或者 Duration 添加一段时间 minusNanos、minusMillis、minusSeconds、minusMinutes、minusHours、minusDays 根据指定的时间单位，对当前 Instant 或者 Duration 减少一段时间 multipliedBy、dividedBy、negated 返回当前 Duration 与指定 long 值相乘或相除得到的时间间隔 isZero、isNegative 检查 Duration 是否为 0 或负数 注意：Instant 类和 Duration 类都是不可变的，上述方法都会返回一个新的实例。 本地日期在新的时间 API 中，Java 提供了两种时间格式：不带时区信息的本地时间和带时区的时间。本地日期表示一个日期，而本地时间还包含（一天中的）时间，但它们都不包含任何有关时区的信息。例如，June 14, 1903 就是一个本地日期。由于日期不含一天中的时间，也不含时区信息，所以它无法与一个准确的瞬时点对应。相反，July 16, 1969, 09:32:00 EDT 就是一个带时区的时间，它表示了时间轴上准确的一点。但有很多计算是不需要考虑时区的，在某些情况下考虑时区甚至可能导致错误的结果。出于此原因，API 设计者们更推荐使用不带时区的时间，除非你真的需要这个时区信息。 LocalDate 就是一个不带时区的本地日期：它只带有年份、月份和当月的天数。你可以通过 LocalDate 的静态工厂方法 now 或 of 来创建一个实例： 12LocalDate alonzosBirthday = LocalDate.of(1903, 6, 14);alonzosBirthday = LocalDate.of(1903, Month.JUNE, 14); 这里我们看到，静态工厂方法中指示月份的数字是以 1 开始的，因此 6 就代表着六月。如果你实在是太喜欢以 0 开始，无法接受这种设定，你也可以使用枚举类型 Month 来指定月份。 下表中列出了 LocalDate 对象的一些常用方法。详细的方法说明请参考 LocalDate 的 JavaDoc。 方法 描述 now、of 静态工厂方法，可以根据当前时间或指定的年月日来创建一个 LocalDate 对象 plusDays、plusWeeks、plusMonths、plusYears 返回在当前 LocalDate 的基础上加上几天、几周、几个月或者几年后的新的 LocalDate 对象，原有的 LocalDate 对象保持不变 minusDays、minusWeeks、minusMonths、minusYears 返回在当前 LocalDate 的基础上减去几天、几周、几个月或者几年后的新的 LocalDate 对象，原有的 LocalDate 对象保持不变 plus、minus 返回在当前 LocalDate 的基础上加上或减去一个 Duration 或者 Period 的新的 LocalDate 对象，原有的 LocalDate 对象保持不变 withDayOfMonth、withDayOfYear、withMonth、withYear 返回一个月份天数、年份天数、月份、年份修改为指定的值的新的 LocalDate 对象，原有的 LocalDate 对象保持不变 getDayOfMonth 获取月份天数（在 $[1,31]$ 之间） getDayOfYear 获取年份天数（在 $[1,366]$ 之间） getDayOfWeek 获取星期几（返回一个 DayOfWeek 枚举值） getMonth、getMonthValue 获取月份，返回一个 Month 枚举的值，或者是 $[1,12]$ 之间的一个数字 getYear 获取年份，在 $[-999999999,999999999]$ 之间 until 获取两个日期之间的 Period 对象，或者以指定 ChronoUnits 为单位的数值 isBefore、isAfter 比较两个 LocalDate isLeapYear 是否为闰年 注意：LocalDate 类是不可变的，上述方法都会返回一个新的实例。 在上一节中我们提到，两个瞬时点 Instant 之间的是一个持续时间 Duration。对于本地时间，对应的对象就是时段 Period，它表示一段逝去的年月日。 本地时间LocalTime 代表一天中的某个时间，例如下午 3 点 30 分。同样，你可以通过 LocalTime 的静态工厂方法 now 和 of 来创建一个实例。 12LocalTime rightNow = LocalTime.now();LocalTime bedtime= LocalTime.of(22, 30) 下表中列出了 LocalTime 对象的一些常用方法。详细的方法说明请参考 LocalTime 的 JavaDoc。 方法 描述 now、of 静态工厂方法，可以根据当前时间或指定的时分秒来创建一个 LocalTime 对象 plusHours、plusMinutes、plusSeconds、plusNanos 返回在当前 LocalTime 的基础上加上几小时、几分钟、几秒或者几纳秒后的新的 LocalTime 对象，原有的 LocalTime 对象保持不变 minusHours、minusMinutes、minusSeconds、minusNanos 返回在当前 LocalTime 的基础上减去几小时、几分钟、几秒或者几纳秒后的新的 LocalTime 对象，原有的 LocalTime 对象保持不变 plus、minus 返回在当前 LocalTime 的基础上加上或减去一个 Duration 的新的 LocalTime 对象，原有的 LocalTime 对象保持不变 withHour、withMinute、withSecond、withNano 返回一个小时数、分钟数、秒数、纳秒数修改为指定的值的新的 LocalTime 对象，原有的 LocalTime 对象保持不变 getHour、getMinute、getSecond、getNano 返回该 LocalTime 的小时、分钟、秒钟及纳秒值 isBefore 、 isAfter 比较两个 LocalTime 注意：LocalTime 类是不可变的，上述方法都会返回一个新的实例。 LocalDateTime 类则可看作是 LocalDate 和 LocalTime 的结合。它用于存储本地时区中的某个时间点，包含当前的年月日等日期信息，同时也包含了时钟、分钟、秒钟等时间信息。同样，LocalDateTime 也是不可变的。 详细的方法说明请参考 LocalDateTime 的 JavaDoc。 带时区的时间Java8 的时间 API 当然也加入了对时区的支持。分别对应着 LocalDate、LocalTime 和 LocalDateTime，带时区的时间类为 ZonedDate、ZonedTime、ZonedDateTime。 Java 中的时区信息来自于 IANA（Internet Assigned Numbers Authority）的数据库，其中每个时区都有着对应的 ID，例如 America/New_York 或者 Europe/Berlin。调用 ZoneId.getAvailableIds 方法即可获取所有可用的时区信息。 你还可以使用 ZoneId.of(id) 方法，用指定的时区 ID 来获取对应的 ZoneId 对象。通过调用 local.atZone(zoneId) 方法，你可以将一个 LocalDateTime 转换成一个 ZonedDateTime 对象，或者通过调用静态方法 ZonedDateTime.of 来创建一个对象。 ZonedDateTime 的许多方法都与 LocalDateTime 一致。下表中列出了 ZonedDateTime 特有的常用方法，详细的方法说明请参考 ZonedDateTime 的 JavaDoc。 方法 描述 now、of、ofInstant 根据当前时间或指定的年月日时分秒、纳秒和 ZoneId，或者一个 Instant 和一个 ZoneId 来创建一个 ZonedDateTime 对象 withZoneSameInstant、withZoneSameLocal 返回时区失去中的一个新的 ZonedDateTime 对象，它表示相同的瞬时点或本地时间 getOffset 获得与 UTC 之间的时差，返回一个 ZoneOffset 对象 toLocalDate、toLocalTime、toInstant 返回对应的本地日期、本地时间或瞬时点 除此之外，Java8 还提供了一个 OffsetDateTime 类，用来表示带有（与 UTC 相比的）偏移量的时间。这个类专门用于一些不需要时区规则的业务场景，比如某些网络协议。对于人类可读的时间，ZonedDateTime 是更好的选择。 详情请查阅 OffsetDateTime 的 JavaDoc。 日期校正器有些时候，我们可以能需要得到类似“每月的第一个星期二”这样的日期。Java8 提供了 TemporalAdjuster 接口，用以实现自定义的日期校正逻辑。通过将创建好的 TemporalAdjuster 传递给日期时间类的 with 方法便可在原有日期时间对象的基础上产生出一个符合要求的日期时间。例如，你可以通过如下代码来计算下一个星期二： 1234567891011TemporalAdjuster NEXT_TUESDAY = (Temporal temporal) -&gt; { int dowValue = DayOfWeek.TUESDAY.getValue(); int calDow = temporal.get(ChronoField.DAY_OF_WEEK); if (calDow == dowValue) { return temporal; } int daysDiff = calDow - dowValue; return temporal.plus(daysDiff &gt;= 0 ? 7 - daysDiff : -daysDiff, DAYS);};LocalDate nextTuesDay = today.with(NEXT_TUESDAY); 此处利用 Lambda 表达式快速实现了一个匿名的 TemporalAdjuster 对象。注意 Lambda 表达式的参数类型为 Temporal，某些 LocalDate 或者 LocalDateTime 之类的类特有的方法将不可用，在使用前必须进行强制转换。你可以通过 ofDateAdjuster 方法和一个 UnaryOperator&lt;LocalDate&gt; 来避免强制转换： 123456789101112TermporalAdjuster NEXT_WORKDAY = TemporalAdjusters.ofDateAdjuster((LocalDate w) -&gt; { LocalDate result; DayOfWeek dow = w.getDayOfWeek(); if (dow == DayOfWeek.FRIDAY) result = w.plusDays(3); else if (dow == DayOfWeek.SATURDAY) result = w.plusDays(2); else result = w.plusDays(1); return result;}); 上述代码中使用的 ofDateAdjuster 方法来自类 TemporalAdjusters。实际上这个类通过静态方法提供了大量的常用 TemporalAdjuster 实现。比如，我们也可以通过如下代码来计算下一个星期二： 123LocalDate nextTuesDay = LocalDate.now().with( TemporalAdjusters.nextOrSame(DayOfWeek.TUESDAY)); 下表中列出了 TemporalAdjusters 的一些常用方法。详细的方法说明请参考 TemporalAdjusters 的 JavaDoc。 方法 描述 previous(dayOfWeek)、next(dayOfWeek) 返回被校正日期之后或之前最近的指定星期几 previoursOrSame(dayOfWeek)、nextOrSame(dayOfWeek) 返回从被校正日期开始，之前或之后的指定星期几。如果被校正日期已吻合条件，被校正的日期实例将被直接返回 dayOfWeekInMonth(n, dayOfWeek) 返回该月中指定的第几个星期几 firstInMonth(dayOfWeek)、lastInMonth(dayOfWeek) 返回该月第一个或最后一个星期几 firstDayOfMonth()、firstDayOfNextMonth()、firstDayOfNextYear()、lastDayOfMonth()、lastDayOfPreviousMonth()、lastDayOfYear() 返回方法名所描述的日期 格式化和解析除了日期校正，日期与字符串之间的相互转换也是十分常见的操作。对于原有的 java.util.Date 等类，我们使用 java.text.DateFormat 来对日期进行格式化和解析。对于 Java8 新引入的日期时间类，我们使用 java.time.format.DateTimeFormatter 类。 DateTimeFormatter 类提供了三种格式化方法来打印日期时间： 预定义的标准格式 语言环境相关的格式 自定义的格式 下表中列出了所有预定义的 DateTimeFormatter。详细说明可参考 DateTimeFormatter 的 JavaDoc。 格式 示例 BASIC_ISO_DATE 20111203 ISO_LOCAL_DATEISO_LOCAL_TIMEISO_LOCAL_DATE_TIME 2011-12-0310:15:302011-12-03T10:15:30 ISO_OFFSET_DATEISO_OFFSET_TIMEISO_OFFSET_DATE_TIME 2011-12-03+01:0010:15:30+01:002011-12-03T10:15:30+01:00 ISO_ZONED_DATE_TIME 2011-12-03T10:15:30+01:00[Europe/Paris] ISO_INSTANT 2011-12-03T10:15:30Z ISO_ORDINAL_DATE 2012-337 ISO_WEEK_DATE 2012-W48-6 ISO_DATEISO_TIMEISO_DATE_TIME 2011-12-03+01:00; 2011-12-0310:15:30+01:00; 10:15:302011-12-03T10:15:30+01:00[Europe/Paris] RFC_1123_DATE_TIME Tue, 3 Jun 2008 11:05:30 GMT 通过调用 DateTimeFormatter 类的 format 方法即可对日期进行格式化： 12String formatted = DateTimeFormatter.ISO_DATE_TIME.format(apollolllaunch); // 1969-07-16T09:32:00-0500[America/New_York] 标准格式主要用于机器可读的时间戳。为了产生人类可读的日期和时间，你需要使用语言环境相关的格式。下表中列出了 Java8 提供的 4 种风格： 风格 日期 时间 SHORT 7/16/69 9:32 AM MEDIUM Jul 16, 1969 9:32:00 AM LONG July 16, 1969 9:32:00 AM EDT FULL Wednesday, July 16, 1969 9:32:00 AM EDT 你可以通过静态方法 ofLocalizedDate 、 ofLocalizedTime 和 ofLocalizedDateTime 来创建这些格式： 12345DateTimeFormatter formatter = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.LONG);String formatted = formatter.format(apollolllaunch); // July 16, 1969 9:32:00 AM EDT 这些方法使用的都是默认的语言环境。通过使用 withLocale 方法可以更改为其他语言环境： 12String formatted = formatter.withLocale(Locale.FRENCH).format(apollolllaunch); // 16 juillet 1969 09:32:00 EDT 你可以通过调用 formatter.toFormat() 方法来获取一个等效的 java.util.DateFormat 对象。 最后，你可以通过指定的模式来自定义日期的格式。例如： 1formatter = DateTimeFormatter.ofPattern(&quot;E yyyy-MM-dd HH:mm&quot;); 其中不同的符号对应着不同的含义。下表中列出了不同符号的具体含义和实例，详情可查阅 DateTimeFormatter 的 JavaDoc。 含义 符号 示例 纪元 GGGGG GGGGG ADAnno DominiA 年份 yyyyyy 691969 月份 MMMMMMMMMMMMMMM 707JulJulyJ 日份 ddd 606 星期几 eEEEEEEEEEE 3WedWednesdayW 24 小时制时钟（$[0,23]$） HHH 909 12 小时制时钟（$[0,11]$） KKK 909 AM/PM a AM 分钟 mm 02 秒钟 ss 00 时区 ID VV America/New_York 时区名称 zzzzz EDTEastern Daylight Time 时差 xxxxxxXXX -04-0400-04:00-Z4:ZZ 本地化的时差 OOOOO GMT-4GMT-04:00 要从一个字符串中解析出日期时间，可以使用静态方法 parse 的各个重载方法。例如： 1234LocalDate churchsBirthday = LocalDate.parse(&quot;1903-06-14&quot;);ZonedDateTime apollolllaunch = ZonedDateTime.parse(&quot;1969-07-16 03:32:00-0400&quot;, DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ssxx&quot;)); 与遗留代码互操作尽管使用全新的 API 可以获得更好的开发体验，但兼容遗留代码总是不可避免的。因此，熟知新的日期时间类和旧的日期时间类之间的转换方法也是我们必须学习的。 总体来讲，转换规则可以归纳为下表： 类 To 遗留类 From 遗留类 java.time.Instantjava.util.Date Date.from(instant) date.toInstant() java.time.Instantjava.sql.Timestamp Timestamp.from(instant) timestamp.toInstant() java.time.Instantjava.nio.file.attribute.FileTime FileTime.from(instant) fileTime.toInstant() java.time.ZonedDateTimejava.util.GregorianCalendar GregorianCalendar.from(zonedDateTime) cal.toZonedDateTime() java.time.LocalDatejava.sql.Time Date.valueOf(localDate) date.toLocalDate() java.time.LocalTimejava.sql.Time Date.valueOf(localDate) date.toLocalTime() java.time.LocalDateTimejava.sql.Timestamp Timestamp.valueOf(localDateTime) timestamp.toLocalDateTime() java.time.ZoneIdjava.util.TimeZone Timezone.getTimeZone(id) timeZone.toZoneId() java.time.format.DateTimeFormatterjava.text.DateFormat formatter.toFormat() 无","link":"/java_8_time/"},{"title":"Java 内存布局与垃圾回收归纳","text":"本文基于由周志明所著的《深入理解 Java 虚拟机》一书的第二部分的内容，同时加入了 JVM 规范以及 Oracle 官方 GC 性能调优指南中的内容，旨在能让读者更好地理解这部分知识。目前本文只会包含 JVM 内存布局与垃圾回收相关的归纳内容，如果以后有机会我会继续更新 JVM GC 调优相关的内容。如果读者对本文的内容组织有更好的建议，欢迎在下方评论处提出。 Java 内存布局JVM 在执行 Java 程序的过程中会把它所管理的内存划分为若干个不同的数据区域。有的区域依赖线程的启动和结束而建立和销毁，这样的区域可看作是线程私有的区域（Per-Thread Data Area）；其他的区域则随着虚拟机进程的启动而存在，可以看做是线程间共享的区域。 具体而言，JVM 的运行时数据区域包括如下： 程序计数器（Program Counter Register）：线程私有的数据区域，保存当前正在执行的虚拟机指令的地址； Java 虚拟机栈（Java Virtual Machine Stack）：线程私有的数据区域，每个栈帧中存放当前执行方法的本地变量及返回地址； Java 堆（Heap）：线程共享的数据区域，对象及数组的分配空间； 方法区（Method Area）：线程共享的数据区域，存放由类加载器加载的类型信息、常量及静态变量； 本地方法栈（Native Method Stack）：用于 Native 方法的方法栈 本章将分别讲述JVM的内存区域划分以及它们各自可能导致的内存溢出异常。 详见《JVMS》第 2.5 节。 程序计数器程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器，而每个线程都有其独立的程序计数器。如果线程正在执行的不是 native 方法，这个计数器记录的是正在被执行的虚拟机指令的地址；如果正在执行的是一个 native 方法，则该计数器的内容不确定（undefined）。 JVM的程序计数器区域应足够大以容纳方法返回地址 returnAddress 或具体平台特有的 native 指针，因此此内存区域是唯一一个在《JVMS》中没有规定任何OutOfMemoryError 情况的区域。 详见《JVMS》第 2.5.1 节。 Java 虚拟机栈Java 虚拟机栈（Java Virtual Machine Stack）是线程私有的数据区域。每个 Java 方法被调用的时候都会创建一个栈帧（Frame）放入到 Java 虚拟机栈中，其中存储了局部变量表、操作数栈、动态链接、方法出口等信息。方法从调用直至执行完成的过程就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 《JVMS》允许将栈帧直接于堆空间上进行分配，即栈帧所处的内存空间并不需要是连续的。《JVMS》还允许具体的 JVM 实现定义虚拟机栈的大小是静态的还是动态的，JVM 随时可以按需地扩大或压缩虚拟机栈的大小。静态虚拟机栈的大小则可以在虚拟机栈创建时确定。 如果线程请求的栈深度超出 JVM 所允许的深度，JVM 将抛出 StackOverflowError 异常；如果 JVM 的虚拟机栈是可以动态扩展的，但在扩展式无法申请到足够的内存，那么 JVM 将抛出 OutOfMemoryError 异常。 详见《JVMS》第 2.5.2 节。有关栈帧的细节详见《JVMS》第 2.6 节。 堆堆（Heap）是线程间共享的数据区域，于 JVM 启动时被创建，所有的类实例以及数组都要在堆上分配。除此之外，堆是 JVM 垃圾收集器主要管理的区域。 根据《JVMS》的规定，堆的内存空间也不需要是连续的，其大小可以是静态的也可以是动态的。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，JVM 将抛出 OutOfMemoryError 异常。 详见《JVMS》第 2.5.3 节。 方法区方法区（Method Area）是线程间共享的数据区域，用于存储已被虚拟机加载的类信息、常量、静态变量以及 JIT 编译后的代码等数据。 方法区在逻辑上属于堆的一部分，因此方法区可以和堆一样不需要连续的内存地址，也可以选择固定大小或可扩展。除此之外，JVM也可以选择不对方法区进行垃圾回收。该须臾的内存回收目标主要是针对常量池的回收和对类的卸载，因此一般来说，这个区域的回收“成绩”比较难以令人满意。 当方法区无法满足内存分配需求时，JVM 将抛出 OutOfMemoryError 异常。 详见《JVMS》第 2.5.4 节。 本地方法栈本地方法栈（Native Method Stack）与虚拟机栈类似，是用于服务 Native 方法的方法栈。在于虚拟机栈相同的情形下，本地方法栈也能抛出 StackOverflowError 和 OutOfMemoryError 异常。 对象存活判断算法在了解过JVM的总体内存布局后，我们便开始讨论JVM堆的垃圾回收机制。首先，JVM会将所有的对象和数组存放在堆中。垃圾收集器在对堆进行回收前，要做的第一件事就是要先确定这些对象之中哪些还“存活”着，哪些已经“死去”（即不可能再被任何途径使用的对象）。 本章首先介绍两种常见的用于判断对象是否应被回收的算法。具体包括如下： 引用计数算法（Reference Counting） 可达性分析算法（Reachability Analysis） 引用计数算法引用计数算法（Reference Counting）的机制十分简单：给对象添加一个引用计数器，每当有一个地方引用它时，计数器值就加 1；当引用失效时，计数器值就减 1；计数器为 0 的对象就不可能再被使用了。 引用计数算法实现简单，判定效率高，部分语言的运行时选用了这种算法，但主流的JVM没有使用这种算法来管理内存，其中最主要的原因是它难以解决对象之间的相互循环引用问题。 可达性分析算法可达性分析算法（Reachability Analysis）很好地解决了对象间循环引用的问题。该算法首先会使用一系列被称为 “GC Root” 的对象作为判定的起始点，从这些结点开始不断地向下搜索。搜索时所走过的路径则代表着从 GC Root 到当前结点的引用链（Reference Chain），当内存中的某个对象不存在从 GC Root 到该对象的引用链时，此对象即为不可用的对象。如果从图论的角度来思考，将对象作为图的结点，将对象间的引用作为图的有向边，那么这个问题实际上就可以变为从 GC Root 结点到其他所有结点的多源可达性问题。 对于 Java 而言，可作为 GC Root 的对象包括如下四种： 虚拟机栈（栈帧中的本地变量表）中引用的对象（即方法的本地变量） 方法区中类静态属性引用的对象（即静态成员变量） 方法区中常量引用的对象（即基本数据类型常量） 本地方法栈中 JNI（即一般说的 native 方法）引用的对象 内存回收算法在解决了对象存活性的判断问题后，接下来就可以开始讨论如何回收已经死亡的对象了。 本章将介绍三种内存回收算法。具体包括如下： 标记 – 清除算法（Mark - Sweep） 复制收集算法（Copying） 标记 – 整理算法（Mark - Compact） 标记 – 清除算法标记 – 清除算法（Mark - Sweep）分为了“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。 标记 – 清除算法是最基础的收集算法，但其主要的不足有两点： 效率：标记和清除这两个过程的时间效率并不高 空间：标记 – 清除算法对内存进行直接回收，会导致回收后产生大量不连续的内存碎片。过多的空间碎片会导致以后程序在运行过程中需要为一个较大的对象分配内存空间时无法找到足够大的连续内存空间，而不得不因此提前触发另一次垃圾收集动作 我们很快就能看到，标记 – 清除算法作为最基础的收集算法，后续的两种算法实际上都是基于其原本的思路并对其不足进行改进而得出的。 复制收集算法为了解决标记 – 清除算法的时间效率问题，人们发明了复制收集算法（Copying）。 复制收集算法将可用的内存划分为了大小相等的两块，每次只使用其中一个块。当当前内存块用完时，复制收集算法就会将还存活着的对象复制到另一块内存上，再把已经使用过的内存块一次性清理掉。 复制收集算法将标记 – 清除算法的清除操作从对若干个分散的小内存区域的回收变为了对一块大内存区域的一次性回收，有效提高了回收的时间效率。除此之外，每次复制发生时都会把存活对象连续地复制到另一块内存中，因此内存分配时也就不需要考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配即可。可惜，这种算法会使得总体可用的内存缩小为原本的一半，成本也十分巨大。 标记 – 整理算法实际上前文所提及的复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。 标记 – 整理算法（Mark - Compact）在某种程度上与标记 – 清除算法类似，但它在标记完成后不会直接对可回收对象进行清理，而是将所有存活的对象移动到内存的一端，然后直接清理掉边界以外的所有内存。 HotSpot JVM 分代收集机制与收集器在讲述完常见的对象存活判定算法和垃圾收集算法后，我们接下来将以 Oracle HotSpot JVM 为例来巩固大家对这些算法的运用的理解。 本章将首要介绍 Oracle HotSpot JVM 所使用的内存分代收集机制，继而详细介绍 HotSpot JVM 所实现的所有垃圾收集器。 分代收集机制在上一章中，我们讨论了复制收集算法和标记 – 整理算法。其中，复制收集算法有着极高的时间效率，通过将内存空间划分为两块区域有效地避免了内存碎片的问题。不过，当对象存活率较高时，复制收集算法的回收率下降，自然就会导致复制操作频繁发生，继而降低效率。标记 – 整理算法则是在标记 – 回收算法的基础上改进了回收操作，使得内存碎片的问题得以避免，无奈其相比于复制收集算法仍然显得更为低效，不过它的执行效率并不会因对象存活率的升高而降低。 目前，主流的商业 JVM 所采用的都是分代收集机制（Generational Collection）。这种机制并没有什么新的思想，只是根据对象存活周期的长短将内存划分为了新生代（Young Generation）和老年代（Tenured Generation），并根据各个分代的特点采用最适当的收集算法。 对于新生代内存空间而言，所存储对象的存活周期较短，每次垃圾收集时都可以发现较大量的无用对象，因此在该空间上可以采用更为高效的复制收集算法；而老年代因为对象存活率高，则应使用标记 – 清理算法或标记 – 整理算法来进行回收。 当新生代内存空间渐满时便会触发针对新生代的垃圾回收，该操作被称为 Minor GC（Minor Collection）。这个过程中部分处于新生代的对象可能会被移入老年代。当老年代渐满时，JVM 就会触发 Major GC（Major Collection，也被称为 Full GC），对整个堆空间（包括新生代空间）进行回收。因此，通常来讲 Major GC 持续的时间会比 Minor GC 长很多。 上图即为 JVM 的分代内存空间布局。可以看到，除了两个年代空间各有的 Virtual 空间（代表预留但未真正占用的内存空间）以外，新生代内存空间包含一块 Eden 区和两块 Survivor 区。新创建的对象都会被分配到 Eden 区中，每次新生代内存空间只有 Eden 区和其中一块 Survivor 区被占用，而当 Minor GC 发生时，存活的对象就会被复制到另一块未被占用的 Survivor 区中，部分升级为老年代的对象也会被移入到老年代内存空间中。 可达性分析与 Stop the World在开始讨论 Oracle HotSpot JVM 所实现的垃圾收集器前，我们先简单介绍一下 HotSpot JVM 所实现的垃圾收集算法的特点，方便大家对后续内容的理解。 在前文中，我们提到主流的 JVM 均采用可达性分析算法来判断对象的存活。可达性分析算法的第一步是枚举 GC Root 结点。可作为 GC Root 的结点主要包括全局性引用（如常量和静态成员变量）以及执行上下文（如栈帧中的本地变量表）。第二步则是根据当前对象间的引用关系来分析所有对象的可达性。 实际上，可达性分析算法对执行时间是敏感的，因为这项分析工作必须在一个能确保一致性的快照中进行：在整个分析期间，整个执行系统应看起来像是被冻结在某个时间点上一般，不可以出现分析过程中对象引用关系还在不断变化的情况，否则分析结果的准确性就无法得到保证。因此，当 JVM 进行内存回收时需要停顿所有的执行线程，也就是所谓的 Stop the World。 垃圾收集器接下来我们将开始详细讲述 Oracle HotSpot JVM 所支持的垃圾收集器。我们将会看到其中不乏可完全避免 Stop the World 现象发生的垃圾收集器。 上图展示了 Oracle HotSpot JVM 中的 7 种作用于不同分代的收集器。收集器之间的连线代表着这两种收集器可以搭配使用，而它们所处的区域则表示了它们是属于新生代收集器还是老年代收集器。这些收集器采用不同的机制实现了不同的垃圾收集算法，它们的侧重点也各有不同，因此并不存在一种在任何场景下都适用的完美收集器。根据实际需要选取最佳的垃圾收集器也是 JVM 调优师的必备技能之一。 Serial 与 Serial Old 收集器Serial 收集器是最基本最古老的收集器，曾是 JVM 新生代收集的唯一选择。这个收集器是一个单线程的新生代收集器，采用复制收集算法，必须在进行垃圾收集时暂停其他所有工作线程，直至收集结束。 Serial Old 收集器则是 Serial 收集器的老年代版本，同样也是一个单线程收集器，采用标记 – 整理算法进行回收，同样会触发 Stop the World。 相比于后来出现的各种新式收集器，Serial 与 Serial Old 收集器的功能显得比较鸡肋，但仍然有着它们独有的优势：实现简单，在单 CPU 的环境下更为高效。由于 Serial 与 Serial Old 收集器采用单线程进行垃圾收集，在单 CPU 环境下不会引入线程切换的开销，因此要更为高效。 实际上，Serial收集器是 Client 模式下的默认新生代收集器，因为在桌面应用场景中，JVM 所管理的内存往往不会很大，因此停顿的时间完全可以被控制在 100 毫秒以内。 ParNew收集器ParNew 收集器其实就是 Serial 收集器的多线程版本，使用多条线程进行垃圾收集。 相比于 Serial 收集器，ParNew 收集器适用于多 CPU 环境，其多线程的特性能更好地利用多个 CPU 来加速垃圾收集操作。不过在单 CPU 环境下，由于 ParNew 收集器引入了线程切换的开销，其性能要低于 Serial 收集器。 作为除 Serial 收集器外唯一一个能与 CMS 收集器配合工作的新生代收集器，ParNew 收集器是 Server 模式虚拟机首选的新生代收集器。 Parallel Scavenge 与 Parallel Old 收集器与 ParNew 收集器类似，Parallel Scavenge 收集器也是使用复制算法的多线程新生代收集器。不同于 ParNew 收集器和 Serial 收集器，Parallel Scavenge 收集器专注于提高程序的吞吐量（即没有用于进行 GC 的 CPU 时间比例）而不是降低停顿时间。 Parallel Old 收集器则为 Parallel Scavenge 收集器的老年代版本，采用标记 - 整理算法。注意到 Parallel Scavenge 收集器不能和 CMS 收集器搭配工作，在 Parallel Old 收集器出现前 Parallel Scavenge 收集器只能与 Serial Old 收集器搭配，因此 Parallel Old 收集器的出现实际上使得吞吐量优先的程序终于有了完整的收集器组合。 因此，在注重吞吐量及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 和 Parallel Old 收集器。 CMS 收集器CMS 收集器（Concurrent Mark-Sweep Collector）是一种专注降低回收停顿时间的老年代收集器，使用的是标记 – 清除算法。 CMS 收集器的执行过程分为 4 个步骤： 初始标记（CMS Initial Mark）：暂停所有用户线程（STW），标记所有与 GC Root 直接关联的对象； 并发标记（CMS Concurrent Mark）：恢复用户线程的执行，使用独立的 GC 线程标记所有可达对象 重新标记（CMS Remark）：暂停所有用户线程（STW），使用多条 GC 线程修正因用户程序继续执行导致可达性发生变化的对象的标记 并发清除（CMS Concurrent Sweep）：恢复用户线程的执行，使用独立的 GC 线程回收死亡对象 可见，CMS 收集器进行垃圾回收时，大多数时候用户线程都仍在执行，有效降低了程序因垃圾收集所导致的停顿时间。 CMS 收集器的主要缺点包括以下 4 点： 对 CPU 资源敏感 存在浮动垃圾 Concurrent Mode Failure 存在空间碎片 首先，CMS 收集器作为一种并发收集器对 CPU 资源是敏感的，可用 CPU 资源的下降会严重影响 CMS 的回收效率。GC 线程对 CPU 资源的占用也会导致用户程序的执行速度有所下降。 而后，CMS 收集器在并发标记阶段难以避免会因用户程序的持续执行导致可达性分析不完全正确，部分对象会在被标记后变为可回收对象。CMS 收集器无法在当次收集中处理这样的对象，这样的对象就成为了“浮动垃圾”，需要等待下一次收集才能被清理。 除此之外，由于标记阶段用户程序的持续执行，新对象需要申请更多的内存空间，因此 CMS 收集器不能在老年代空间即将填满时才开始进行收集，而是需要预留一部分空间给用户程序。我们可以通过 -XX:CMSInitiatingOccupancyFraction 参数来设置触发 CMS GC 的老年代空间使用百分比。 当 CMS 运行期间预留的内存无法满足程序需要，就会出现一次 Concurrent Mode Failure。此时 JVM 就会启动后备预案，启用 Serial Old 收集器重新进行老年代的垃圾收集（Full GC），这样就导致 GC 停顿时间变长了。过高的 -XX:CMSInitiatingOccupancyFraction 参数容易导致 Concurrent Mode Failure 频繁发生，反而降低性能。 最后，CMS 收集器采用的是标记 – 清除算法，这样的算法在收集后会产生大量的空间碎片，后续出现的大对象可能难以找到可用的连续空间，导致提前触发 Major GC。值得注意的是，CMS 收集器还提供了 -XX:+UseCMSCompactAtFullCollection 和 -XX:CMSFullGCsBeforeCompaction 参数来在满足一定条件时进行内存整理。 G1 收集器G1 收集器（Garbage-First Collector）和 CMS 类似，专注于减少 GC 停顿时间，是一种能在多数时候与用户程序并发的垃圾收集器。不同的是，G1 收集器可以管理整个 JVM 堆，有着比 CMS 更为优秀的特性，并且能建立 GC 停顿时间的预测模型，根据使用者的需要将 GC 停顿时间控制在指定的范围内。实际上，G1 收集器的出现本身就是为了替换 CMS 收集器的。 G1 收集器会把整个 JVM 堆分割为若干个大小相同的区域（Region），并将这些区域划分给新生代和老年代。此时新生代和老年代便不再是物理隔离的了，它们都是一部分 Region（不需要连续）的集合。 G1 收集器会根据以往进行回收所获得的空间大小以及回收所需时间的经验值来跟踪每个 Region 里面的垃圾堆积的价值大小，优先回收价值最大的 Region，也因此得名Garbage-First。 G1 收集器的收集过程同样分为 4 个步骤： 初始标记：停止所有用户线程，标记与 GC Root 直接关联的对象，并指定新的 Region，使用户线程恢复运行后便开始使用新的 Region 并发标记：恢复用户线程，并发地从 GC Root 开始对堆中对象进行可达性分析 最终标记：停止所有用户线程，修正在并发标记期间因用户程序继续运作而导致标记发生变化的那一部分对象标记 筛选回收：对各个 Region 和回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来回收某些 Region。回收时，多个 Region 中的存活对象会被复制到某个新的 Region 中 实际上，G1 收集器仍然有着和 CMS 收集器类似的缺点： 浮动垃圾仍然存在 JVM 仍有可能在收集过程中耗尽剩余空间，此时 G1 收集器仍然需要触发一次 STW 来完成回收。 不过，G1 收集器不会产生内存碎片，因为从 Region 的角度上看，G1 收集器所采用的算法类似于复制回收算法，从整个堆的角度上看又类似于标记 – 整理算法。无论怎样，G1 收集器都不会产生内存碎片。","link":"/jvm_gc_summary/"},{"title":"Linux Bash 参考指南","text":"我写这篇文章主要是用来作为我的 Linux Bash 工具书的，希望这篇文章对你也能起到同样的效果。随着我学习到更多有关 Linux Bash 的知识，我会不断地更新这篇文章。 本文的内容假设你对 Linux 和 Linux 命令行有基本的了解。如果你没有的话，你可以去看看这篇教程。 本文的内容分为三个主要部分： Linux Shell 脚本编程 Linux Bash 内置命令 常见的轻量级 Linux 命令工具 让我们开始吧 第一部分：脚本编程在这一部分中，我会介绍 Linux Shell 脚本编程的基本语法，并在其中穿插一些对日常工作有所帮助的代码示例。这些代码示例能在大部分 POSIX Shell 解释器上运行，但如果这些代码对于某些如 Bash 的特殊 Shell 解释器有更好的写法的话，文中也会一并给出。 1.1 基本使用在 Shell 解释器执行脚本文件时，它实际上会考虑脚本文件中给出的控制流语句，并一行一行地从脚本文件中读入命令并执行。如此，如果你知道怎么使用 Shell 命令行的话，一个 Shell 脚本文件的基本组成对你来说实际上是很直观的。 如下是一段简单的 Hello World 示例： 1234#!/bin/sh# My first scriptecho 'Hello, world!' 上述脚本文件的第一行被称为 She-Bang 注释，以 #! 开头。这行注释能告诉 Shell 该用哪个解释器程序来解释这个脚本文件。在上面的示例中，我们使用了 /bin/sh 来解释这个文件。如果你想要用除了 Shell 以外的解释器来执行文件的话，你也可以在这行注释中指定别的解释器程序，例如 #!/bin/env python 可以用来执行 Python 文件。 第二行则是一般的行注释，以 # 开头。行注释是 Shell 支持的唯一一种注释形式，也就是说 Shell 不支持块注释。Shell 解释器在运行时会忽略注释，注释里的内容通常都是写给其他人类读者的。 除了注释，一个 Shell 脚本文件由若干个命令组成，正是这些命令告诉解释器接下来该做什么。在 Shell 脚本文件中使用的命令和你平常在命令行中使用的命令完全一致。如果你不知道你可以使用哪些命令以及如何使用这些命令，你可以去看看本文的后面几个部分的内容，或者直接使用 man 命令来查看相关文档。 1.2 标准输入和标准输出标准输入和标准输出是你的脚本与用户进行交互的主要方式。 正如你在上面的示例中看到的那样，echo 命令可以用来产生标准输出，而从标准输入中读取内容则可以通过 read 命令完成： 123echo -n &quot;Enter some text &gt; &quot;read textecho &quot;You entered: $text&quot; read 命令接受一个变量名作为参数。在运行时，它会扫描标准输入中的内容并将其保存到指定名称的变量中。你可以在后续的代码中使用 $ 记号读取该变量的值。 有关 echo 和 read 命令更多高级的用法，详见 help echo 和 help read。 1.3 命令行参数除了标准输入和标准输出以外，用户还可以在运行你的脚本文件时指定命令行参数，而命令行参数通常会被用于进行简单的配置。 在你的脚本代码中，你可以： 使用变量 $# 获取用户给定命令行参数的数量 使用从 $1 到 $9 逐个获取用户给定的命令行参数，或者 使用 $@ 变量将用户输入的所有命令行参数读取为一个数组 值得一提的是，你还可以使用一个特殊的 $0 参数，它包含了正在运行的脚本文件在系统中的绝对路径，而且它的值不会被包含在 $@ 变量中。 1.4 函数就像其他编程语言一样，你还可以在 Shell 脚本中定义函数以减少代码冗余。 Shell 函数有两种定义方式： 12345678910function print { echo &quot;Received $@&quot;}another_print() { echo &quot;Received $@&quot;}print Helloanother_print hello 两种方式的效果完全一致。 调用函数的方式和你调用其他的命令的方式完全一致。在函数中你同样可以使用 $1 到 $9 、 $@ 、 $# 等变量获取函数调用时给定的参数。 1.5 退出状态码有时，你的脚本文件可能会因为一些原因而不能成功执行，这时你可能会想要告诉你的用户具体发生了什么错误。除了通过 echo 命令将错误信息打印到错误输出以外，你还可以使用 exit 指令来以非零的退出状态码结束你的脚本： 123456789PROGNAME=$(basename $0)error_exit() { echo &quot;${PROGNAME}: ${1:-&quot;Unknown Error&quot;}&quot; 1&gt;&amp;2 exit 1}echo &quot;Example of error with line number and message&quot;error_exit &quot;$LINENO: An error has occurred.&quot; 函数则可以通过使用 return 命令来返回指定的值并结束自身的运行，而不至于结束整个脚本。 你在任何使用都可以通过访问变量 $? 来获取上一个指定的命令的退出状态码或上一个指定的函数的返回值。 请通过查阅 help exit 和 help return 来了解更多有关内容。 1.6 控制流语句除了那些你能在命令行中使用的命令以外，你还可以在脚本中使用控制流语句来实现更加复杂的功能。Shell 所支持的控制流语句和你在其他编程语言中用的很相似，包括 if 、 case 、 for 、 while 和 until。 1.6.1 Ifif 语句可以让你的脚本文件只在满足某个指定条件时才执行某段代码： 1234567if [ &quot;$#&quot; -e 0 ]; then echo &quot;Received nothing&quot;elif [ &quot;$#&quot; -e 1 ]; then echo &quot;Received one argument: $1&quot;else echo &quot;Received arguments: $@&quot;fi 你可以使用任意命令作为 if 的条件，如果命令的退出码为零那么 if 就会认为条件满足。请查阅 help if 了解更多有关内容。 通常，我们会使用类似 [ &quot;$#&quot; -e 1 ] 这样的简写方式来使用 test 命令，声明数值比较或是其他在 Linux 中常见的条件，例如指定路径指向的是一个文件或是一个目录。该简写等价于 test &quot;$#&quot; -e 1。请查阅 help test 来了解除 -e 以外的条件运算。 在某些 Shell 上（包括 Bash、Zsh 和 Ksh），你还可以以双中括号的形式声明条件，如 [[ a != b ]]。这种写法并非由 POSIX 标准给出，且只被一部分 Shell 支持。这样的写法比起原生的 test 命令更加安全，原因在于它不会在执行前对给定的参数进行展开，但这样的写法无疑会导致你的脚本的可移植性下降。请查阅 help [[ 和这篇文章了解更多有关内容。 你还可以使用 &amp;&amp; 和 || 来对多个 test 命令的结果进行组合。这些运算符同样支持在其他编程语言中常见的短路功能，而这样的功能可以被用来将某些简单的条件检查语句变得更加简短。例如，[ 2 -gt 1 ] &amp;&amp; echo 'ok' 等价于如下代码片段： 123if [ 2 -gt 1 ]; then echo 'ok'fi 除了中括号以外，大括号也可以被用来声明条件语句。请查阅这篇文章了解更多有关内容。 1.6.2 Casecase 语句的功能和其他编程语言中的 switch 语句大致相同。它会用给定的字符串与若干个 Glob 模式进行匹配，并在满足匹配时执行对应的命令。一个简单的 case 语句示例如下： 12345case $character in [[:lower:]] | [[:upper:]] ) echo &quot;You typed the letter $character&quot;;; [0-9] ) echo &quot;You typed the digit $character&quot;;; * ) echo &quot;You did not type a letter or a digit&quot;esac 值得注意的是，我们需要使用一个右括号（)）来分隔 Glob 模式和对应的命令，并在除了最后一个分支以外的其他每以个 case 分支的最后一个命令的末尾加上两个分号（;;）。 1.6.3 Whilewhile 语句可以被用来循环执行指定的命令，直到给定的条件不满足： 12345number=0while [ &quot;$number&quot; -lt 10 ]; do echo &quot;Number = $number&quot; number=$((number + 1))done 1.6.4 Untiluntil 语句的作用和 while 语句十分相似，但它会在给定的条件满足时结束循环： 12345number=0until [ &quot;$number&quot; -ge 10 ]; do echo &quot;Number = $number&quot; number=$((number + 1))done 1.6.5 Forfor 语句可以用来遍历给定的数组： 123for argument in &quot;$@&quot;; do echo &quot;Received $argument&quot;done 第二部分：Bash Shell 内置命令本部分将主要描述 Bash Shell 各个内置命令（Built-in Command）的作用及用法。要初步了解这些命令，我们可以首先使用 help 命令来查看 Bash Shell 支持的所有内置命令和简单的文档。 dirs、pushd、popdBash 提供了一套以栈/链表管理你曾经进入的路径的方式，方便你在不同的路径间跳转，相关的管理命令包括 dirs、pushd、popd。 首先，Bash 使用了链表来实现这个先入先出的栈行为，而 dirs 命令能把该链表的内容完整地打印出来： 12345678910111213141516171819202122232425$ help dirsdirs: dirs [-clpv] [+N] [-N] Display directory stack. Display the list of currently remembered directories. Directories find their way onto the list with the `pushd' command; you can get back up through the list with the `popd' command. Options: -c clear the directory stack by deleting all of the elements -l do not print tilde-prefixed versions of directories relative to your home directory -p print the directory stack with one entry per line -v print the directory stack with one entry per line prefixed with its position in the stack Arguments: +N Displays the Nth entry counting from the left of the list shown by dirs when invoked without options, starting with zero. -N Displays the Nth entry counting from the right of the list shown by dirs when invoked without options, starting with zero. Exit Status: Returns success unless an invalid option is supplied or an error occurs. 简单来讲，用户在使用 dirs 查看链表内容的同时，可以通过 -p、-v 选项调整 dirs 的输出样式，通过 +N、-N 参数指定 dirs 只输出链表某部分的内容，甚至使用 -c 选项清空链表。 值得注意的是，dirs -c 以及后面会提到的 popd 命令并不能真正地清空链表内容：当链表内容变为空后，Bash 会自动将当前目录放入到链表中，成为链表中仅有的元素。 dirs 的帮助信息中也提到，用户可使用 pushd 和 popd 管理链表中的元素。首先先来看 pushd： 1234567891011121314151617181920212223242526272829$ help pushdpushd: pushd [-n] [+N | -N | dir] Add directories to stack. Adds a directory to the top of the directory stack, or rotates the stack, making the new top of the stack the current working directory. With no arguments, exchanges the top two directories. Options: -n Suppresses the normal change of directory when adding directories to the stack, so only the stack is manipulated. Arguments: +N Rotates the stack so that the Nth directory (counting from the left of the list shown by `dirs', starting with zero) is at the top. -N Rotates the stack so that the Nth directory (counting from the right of the list shown by `dirs', starting with zero) is at the top. dir Adds DIR to the directory stack at the top, making it the new current working directory. The `dirs' builtin displays the directory stack. Exit Status: Returns success unless an invalid argument is supplied or the directory change fails. 简单来讲，pushd 会对链表进行一定的操作，然后将当前目录改为目前位于链表左端（栈顶部）的目录。根据用户给定参数的不同，pushd 有三种运行模式： 用户未给定参数，那么 pushd 交换链表最左侧的两个元素 用户给定了一个路径，那么 pushd 将该路径放入到链表最左侧 用户给定 +N 或 -N，实际上意味着用户指定了链表中的某一个元素（N 为从 0 开始的索引值；+N 意味着从左端数起，-N 则从右端数起），然后 pushd 会对链表进行旋转（最右端的元素离开链表，再将其放入到链表的左端），直到用户指定的元素到达链表最左侧 用户可以通过 -n 选项让 pushd 在对链表进行操作后不改变当前目录。 最后再来看看 popd 12345678910111213141516171819202122232425$ help popdpopd: popd [-n] [+N | -N] Remove directories from stack. Removes entries from the directory stack. With no arguments, removes the top directory from the stack, and changes to the new top directory. Options: -n Suppresses the normal change of directory when removing directories from the stack, so only the stack is manipulated. Arguments: +N Removes the Nth entry counting from the left of the list shown by `dirs', starting with zero. For example: `popd +0' removes the first directory, `popd +1' the second. -N Removes the Nth entry counting from the right of the list shown by `dirs', starting with zero. For example: `popd -0' removes the last directory, `popd -1' the next to last. The `dirs' builtin displays the directory stack. Exit Status: Returns success unless an invalid argument is supplied or the directory change fails. 类似，popd 会根据用户指定的参数对链表中的元素进行删除，然后把当前目录改为链表最左侧的元素。根据用户指定参数的不同，popd 也有三种运行模式： 若用户没有指定参数，那么 popd 移除链表最左侧的元素 若用户指定参数 +N，那么 popd 移除链表最左侧的第 N+1 个元素 若用户指定参数 -N，那么 popd 移除链表最右侧的第 N+1 个元素 同样，popd 也支持 -n 参数，使得其在对链表完成操作后不改变当前目录。 exec我们先来看 help exec 给出的信息： 1234567891011121314151617exec: exec [-cl] [-a name] [command [arguments ...]] [redirection ...] Replace the shell with the given command. Execute COMMAND, replacing this shell with the specified program. ARGUMENTS become the arguments to COMMAND. If COMMAND is not specified, any redirections take effect in the current shell. Options: -a name pass NAME as the zeroth argument to COMMAND -c execute COMMAND with an empty environment -l place a dash in the zeroth argument to COMMAND If the command cannot be executed, a non-interactive shell exits, unless the shell option `execfail' is set. Exit Status: Returns success unless COMMAND is not found or a redirection error occurs. 文档中实际上给出了颇为完整的信息：exec 命令可以直接将当前进程运行的 Shell 程序替换为由给定命令指定的程序，新的程序则可以访问当前 Shell 已经设定好的环境变量。该命令仅提供了 -a 、 -c 、 -l 三个选项，其中 -a 和 -l 选项均用于对指定程序的 $0 参数进行改动，而 -c 选项则可以屏蔽所有环境变量，让指定程序在一个干净的环境中运行。 除此以外，exec 命令还允许你在不指定具体命令的情况下指定重定向，这种情况下重定向操作将直接作用于当前 Shell。很多时候，你可以在你的脚本文件中使用 exec 1&gt; out 2&gt;&amp;1 命令，这样该脚本文件后续的输出就会被导向到指定的文件中。 警告：尝试在当前 Shell 中使用 exec 对标准输出进行重定向时一定要小心，这有可能导致很多程序无法正常运行。建议在执行前先使用如 exec 3&gt;&amp;1 命令将标准输出保存到其他文件描述符中，以便后续恢复。 export我们先来看看 help export 给出的信息： 12345678910111213141516$ help exportexport: export [-fn] [name[=value] ...] or export -p Set export attribute for shell variables. Marks each NAME for automatic export to the environment of subsequently executed commands. If VALUE is supplied, assign VALUE before exporting. Options: -f refer to shell functions -n remove the export property from each NAME -p display a list of all exported variables and functions An argument of `--' disables further option processing. Exit Status: Returns success unless an invalid option is given or NAME is invalid. 对于常用的变量设置而言，相比起一般的 FOO=bar 的变量设置方式，export FOO=bar 会使得当前 Shell 的子进程（包括该 Shell 调用的其他程序/文件）也可以访问该变量，因此设置环境变量时我们都会使用 export 指令来进行设置。 相比于常见的 export FOO=bar 用法，export 实际上可以接受若干对由空格分隔的变量键值对；若不给定变量的值，export 会把当前 Shell 已给该变量设定的值（默认为空）进行导出；-n 选项可以移除指定的 export 变量；export -p 能够打印当前已经 export 的变量；若使用 -f 选项，export 指令会把指定的 name 当做 Shell 函数而不是变量进行导出。 set我们先来看看 help set 给出的信息： 1234567891011121314151617set: set [-abefhkmnptuvxBCHP] [-o option-name] [--] [arg ...] Set or unset values of shell options and positional parameters. Change the value of shell attributes and positional parameters, or display the names and values of shell variables. Options: ... Using + rather than - causes these flags to be turned off. The flags can also be used upon invocation of the shell. The current set of flags may be found in $-. The remaining n ARGs are positional parameters and are assigned, in order, to $1, $2, .. $n. If no ARGs are given, all shell variables are printed. Exit Status: Returns success unless an invalid option is given. 此处仅截取了完整 help 信息首位两段的内容。可见，set 命令可以修改当前 Shell 程序的启动选项和位置参数。我们可以通过 $- 变量访问当前 Shell 的启动选项，同样也可以通过 $1 等变量访问当前位置参数。 set 命令允许我们配置的这些选项多数都能用于开关 Shell 的某些功能，比如我们常用的 -x 能够在命令执行时将命令打印到标准输出，-v 则会回显输入的文本。在选项打开后我们还可以使用对应的 + 写法取消这些选项，例如用 set +x 取消 set -x 的效果。这些开关选项同样可以作为 Shell 的启动选项进行配置，例如我们可以将脚本文件的 She-Bang 注释写为 #!/bin/sh -x 来达到同样的效果。 我们回忆一下 set 命令的标准格式： 1set [-abefhkmnptuvxBCHP] [-o option-name] [--] [arg ...] 可见 set 支持很多的选项，其中包括一个特殊的 -o 选项、 -- 以及剩下的参数。接下来我会先简单说说除 -o 外每一个选项的作用： 选项名 作用 a 创建和修改变量时会同样将其 export 为环境变量 b 在任务终止时立刻打印通知 e Shell 在其中一个命令返回非零退出状态码时立刻退出。用于脚本文件时能让你的脚本在其中一个命令发生错误时立刻结束执行 f 关闭 Glob 模式展开 h 在查找到命令后记住它们所在的位置 k 所有以复制形式指定的参数都将作为命令的环境变量，而不仅仅是位于命令前面的那些 m 开启作业管理功能 n 读取命令但不执行。适用于对脚本文件进行语法检查。对交互式 Shell 不起作用 p 该选项会在当前用户 ID 与实际用户 ID 不同时被开启，开启后会不再处理 $ENV 文件，也不会导入 Shell 函数。关闭该选项会使得当前用户 ID 和组 ID 被设置为实际用户 ID 和组 ID t 在读取并执行一个命令后退出 Shell u 将尝试读取未设定的变量的行为视为错误。在当前 Shell 设定该选项可能导致 Tab 键命令自动补全功能发生错误 v 在读取到一行 Shell 输入时将其重新打印到错误输出。作用于 Shell 脚本时，解释器会把读入的每一行命令打印到错误输出 x 在运行命令前将命令和其参数打印到标准输出。Shell 在执行命令前可能会考虑 alias 并对你输入的文本进行展开，启动该选项后所打印的命令将是 Shell 在完成上述预处理后实际执行的命令 B 启动 Shell 的大括号展开功能 C 禁止使用输出重定向覆写已存在的文件 E Shell 函数将继承 ERR 陷入 H 开启 ! 式的历史命令展开功能。该选项在互动式 Shell 下默认开启 P 启动后不会在执行命令时解析符号链接 T Shell 函数会继承 DEBUG 陷入 除了这些选项以外，我们还可以通过 -o 选项来指定选项。实际上，上述选项一一对应着 -o 所支持的选项，-o 所支持的多数选项可被视为上述选项的全称。我们通过 -o &lt;name&gt; 来开启对应名称的选项，并通过 +o &lt;name&gt; 关闭。支持的选项名称包括如下： 选项名称 作用 allexport 等同于 -a braceexpand 等同于 -B emacs 使用 Emacs 风格的行编辑命令行接口 errexit 等同于 -e errtrace 等同于 -E functrace 等同于 -T hashall 等同于 -h histexpand 等同于 -H history 启动历史命令功能 ignoreeof 当前 Shell 在读取到 EOF 后不会退出 interactive-comments 允许在互动式命令中输入注释 keyword 等同于 -k monitor 等同于 -m noclobber 等同于 -C noexec 等同于 -n noglob 等同于 -f nolog 无作用 notify 等同于 -b nounset 等同于 -u onecmd 等同于 -t physical 等同于 -P pipefail 管道中最后一个返回非零退出状态码的命令的退出状态码将作为该管道命令的返回值，若所有命令的退出状态码都为零则返回零 posix 改变 Bash 与 POSIX 标准不同的行为以严格遵循 POSIX 标准 privileged 等同于 -p verbose 等同于 -v vi 使用 Vi 风格的行编辑命令行接口 xtrace 等同于 -x 除了上述选项，set 命令还允许在选项后接上 -- 或 - 以及其余参数作为 Shell 的当前位置参数，其中： 使用 -- 时，若后续未给出参数，则当前位置参数会被清空 使用 - 时，-x 和 -v 选项会被关闭 source我们先来看看 help source 给出的信息： 123456789101112$ help sourcesource: source filename [arguments] Execute commands from a file in the current shell. Read and execute commands from FILENAME in the current shell. The entries in $PATH are used to find the directory containing FILENAME. If any ARGUMENTS are supplied, they become the positional parameters when FILENAME is executed. Exit Status: Returns the status of the last command executed in FILENAME; fails if FILENAME cannot be read. 可见，相比起一般的文件运行，source 会把指定文件中的指令读入到当前的 Shell 中并执行。这种运行方式与在子进程/子 Shell 中运行的方式相比，在部分行为上会有所不同，例如若文件中使用了 exit 指令，当前的 Shell 也会被退出，导致后面的指令无法被执行；有时为了令某个文件中的 export 指令在当前 Shell 中生效，也会需要使用 source 命令来运行该文件，这种用法常见于 .bashrc、/etc/profile 等会为 Bash 设置环境变量的文件。 除外，source 命令还有一个简写形式，为 .，即 . some_file 等价于 source some_file。 第三部分：小型命令行工具freefree 命令可用于查看系统当前的内存使用情况。典型的 free 输出如下： 1234$ free total used free shared buff/cache availableMem: 8312948 5037412 3039060 17720 236476 3134680Swap: 25165824 58460 25107364 通过 man free 可以查看 free 的详细介绍。首先： 123456789NAME free - Display amount of free and used memory in the systemSYNOPSIS free [options]DESCRIPTION free displays the total amount of free and used physical and swap memory in the system, as well as the buffers and caches used by the kernel. The information is gathered by parsing /proc/meminfo. ... 这里提到，free 输出的结果主要是通过解析 /proc/meminfo 文件的内容得出的。而后介绍了 free 输出各行的含义： 123456789101112131415161718192021222324DESCRIPTION ... The displayed columns are: total Total installed memory (MemTotal and SwapTotal in /proc/meminfo) used Used memory (calculated as total - free - buffers - cache) free Unused memory (MemFree and SwapFree in /proc/meminfo) shared Memory used (mostly) by tmpfs (Shmem in /proc/meminfo, available on kernels 2.6.32, displayed as zero if not available) buffers Memory used by kernel buffers (Buffers in /proc/meminfo) cache Memory used by the page cache and slabs (Cached and Slab in /proc/meminfo) buff/cache Sum of buffers and cache available Estimation of how much memory is available for starting new applications, without swapping. Unlike the data provided by the cache or free fields, this field takes into account page cache and also that not all reclaimable memory slabs will be reclaimed due to items being in use (MemAvailable in /proc/meminfo, available on kernels 3.14, emulated on kernels 2.6.27+, otherwise the same as free) 后续便是各个参数的介绍。首先是可以调整显示数字单位的参数： 123456789101112131415161718192021222324252627OPTIONS -b, --bytes Display the amount of memory in bytes. -k, --kilo Display the amount of memory in kilobytes. This is the default. -m, --mega Display the amount of memory in megabytes. -g, --giga Display the amount of memory in gigabytes. --tera Display the amount of memory in terabytes. -h, --human Show all output fields automatically scaled to shortest three digit unit and display the units of print out. Following units are used. B = bytes K = kilos M = megas G = gigas T = teras If unit is missing, and you have petabyte of RAM or swap, the number is in terabytes and columns might not be aligned with header. 使用的效果大致如下： 1234$ free -h total used free shared buff/cache availableMem: 7.9G 4.7G 3.0G 17M 230M 3.1GSwap: 24G 59M 23G 除外，还有 --si 参数可以让 free 输出的单位按照 1000 进制进行计算而不是 1024 进制： 1--si Use power of 1000 not 1024. 1234$ free -h --si total used free shared buff/cache availableMem: 8.3G 4.9G 3.2G 17M 236M 3.3GSwap: 25G 61M 25G 接下来是可拆分显示 buff、cache 数值的 -w 参数： 123-w, --wide Switch to the wide mode. The wide mode produces lines longer than 80 characters. In this mode buffers and cache are reported in two separate columns. 1234$ free -wh total used free shared buffers cache availableMem: 7.9G 4.8G 2.9G 17M 33M 197M 3.0GSwap: 24G 59M 23G 还有可以显示额外行信息的 -l、-t 参数： 1234-l, --lohi Show detailed low and high memory statistics.-t, --total Display a line showing the column totals. 1234567$ free -hlt total used free shared buff/cache availableMem: 7.9G 4.8G 2.9G 17M 230M 3.0GLow: 7.9G 5.0G 2.9GHigh: 0B 0B 0BSwap: 24G 59M 23GTotal: 31G 4.9G 26G 最后，free 指令还允许用户指定按周期进行持续汇报。用户可通过 -s 和 -c 参数调整 free 汇报的周期时长和汇报次数： 12345-s, --seconds seconds Continuously display the result delay seconds apart. You may actually specify any floating point number for delay, usleep(3) is used for microsecond resolution delay times.-c, --count count Display the result count times. Requires the -s option. rename相较于 mv、rename 可用于对文件进行批量重命名。我们通过 man rename 可以查看 rename 的简单描述： 123456789101112131415NAME rename - renames multiple filesSYNOPSIS rename [ -v ] [ -n ] [ -f ] perlexpr [ files ]OPTIONS -v, --verbose Verbose: print names of files successfully renamed. -n, --no-act No Action: show what files would have been renamed. -f, --force Force: overwrite existing files. 由此可见，rename 命令接收两类位置参数：第一个位置参数为 Perl 正则表达式，用于表达文件名变换规则；第二个参数为若干个文件名，在 Bash Shell 下我们可以利用 Blob 表达式的展开来方便地指定我们需要重命名的文件。 支持的选项主要包括如下三个： -v、--verbose：打印成功重命名的文件名称 -n、--no-act：不执行重命名操作，只打印会被重命名的文件 -f、--force：重命名时覆盖已存在的文件 总体而言是很好理解的三个选项。这个命令唯一的疑点就在于 Perl 表达式的语法了。 在这个页面中有对 rename 可用的 Perl 正则表达式语法有简单的介绍。 简单而言，Perl 正则表达式主要用于进行字符串替换，具体的组成如下： 1&lt;s|y&gt;/&lt;match_expr&gt;/&lt;sub_expr&gt;/[g][i] 其中第一个匹配表达式用于匹配给定字符串中的某个部分，而第二个转换表达式则表示如何转换匹配到的子字符串。两个表达式都支持各种常见的正则表达式元素，同时加入了一些简单的语法来更好地完成替换动作。 Perl 正则表达式包含替换（Substitution）和转译（Translation）两种执行模式，分别对应首字母 s 和 y。除此以外，通过结尾的修饰符 g 和 i 也可以选择全局匹配（正则表达式默认只匹配第一个匹配的子字符串）和大小写不敏感匹配。","link":"/linux_bash_reference/"},{"title":"MIT 6.824 Lab 1 - 实现 MapReduce","text":"在这篇文章中，我们将按照 MIT-6.824 2021 Spring 的安排，完成 Lab 1，用 Golang 实现 MapReduce 分布式计算框架。 完整的 Lab 说明可参阅链接 http://nil.csail.mit.edu/6.824/2021/labs/lab-mr.html。 不了解 MapReduce 原理的读者，也可以先阅读我先前的文章《Google MapReduce 总结》。 牛刀小试首先，我们通过 Git 获取 Lab 的初始代码： 1git clone git://g.csail.mit.edu/6.824-golabs-2021 6.824 初始代码中默认已经提供了 单进程串行 的 MapReduce 参考实现，在 main/mrsequential.go 中。我们可以通过以下命令来试玩一下： 123456789101112cd 6.824cd src/main# 构建 MR APP 的动态链接库go build -race -buildmode=plugin ../mrapps/wc.go# 运行 MRrm mr-out*go run -race mrsequential.go wc.so pg*.txt# 查看结果more mr-out-0 除了 mrapps/wc.go，初始代码在 mrapps 中还提供了其他 MR APP 实现，也可以参照着替换上述命令中的参数来试玩一下。 这里使用了 Golang 的 Plugin 来构建 MR APP，使得 MR 框架的代码可以和 MR APP 的代码分开编译，而后 MR 框架再通过动态链接的方式载入指定的 MR APP 运行。 任务分析如上文所述，在 main/mrsequential.go 中我们可以找到初始代码预先提供的 单进程串行 的 MapReduce 参考实现，而我们的任务是实现一个 单机多进程并行 的版本。 通过阅读 Lab 文档 http://nil.csail.mit.edu/6.824/2021/labs/lab-mr.html 以及初始代码，可知信息如下： 整个 MR 框架由一个 Coordinator 进程及若干个 Worker 进程构成 Coordinator 进程与 Worker 进程间通过本地 Socket 进行 Golang RPC 通信 由 Coordinator 协调整个 MR 计算的推进，并分配 Task 到 Worker 上运行 在启动 Coordinator 进程时指定 输入文件名 及 Reduce Task 数量 在启动 Worker 进程时指定所用的 MR APP 动态链接库文件 Coordinator 需要留意 Worker 可能无法在合理时间内完成收到的任务（Worker 卡死或宕机），在遇到此类问题时需要重新派发任务 Coordinator 进程的入口文件为 main/mrcoordinator.go Worker 进程的入口文件为 main/mrworker.go 我们需要补充实现 mr/coordinator.go、mr/worker.go、mr/rpc.go 这三个文件 基于此，我们不难设计出，Coordinator 需要有以下功能： 在启动时根据指定的输入文件数及 Reduce Task 数，生成 Map Task 及 Reduce Task 响应 Worker 的 Task 申请 RPC 请求，分配可用的 Task 给到 Worker 处理 追踪 Task 的完成情况，在所有 Map Task 完成后进入 Reduce 阶段，开始派发 Reduce Task；在所有 Reduce Task 完成后标记作业已完成并退出 而 Worker 的功能则相对简单，只需要保证在空闲时通过 RPC 向 Coordinator 申请 Task 并运行，再不断重复该过程即可。 此外 Lab 要求我们考虑 Worker 的 Failover，即 Worker 获取到 Task 后可能出现宕机和卡死等情况。这两种情况在 Coordinator 的视角中都是相同的，就是该 Worker 长时间不与 Coordinator 通信了。为了简化任务，Lab 说明中明确指定了，设定该超时阈值为 10s 即可。为了支持这一点，我们的实现需要支持到： Coordinator 追踪已分配 Task 的运行情况，在 Task 超出 10s 仍未完成时，将该 Task 重新分配给其他 Worker 重试 考虑 Task 上一次分配的 Worker 可能仍在运行，重新分配后会出现两个 Worker 同时运行同一个 Task 的情况。要确保只有一个 Worker 能够完成结果数据的最终写出，以免出现冲突，导致下游观察到重复或缺失的结果数据 第一点比较简单，而第二点会相对复杂些，不过在 Lab 文档中也给出了提示 —— 实际上也是参考了 Google MapReduce 的做法，Worker 在写出数据时可以先写出到临时文件，最终确认没有问题后再将其重命名为正式结果文件，区分开了 Write 和 Commit 的过程。Commit 的过程可以是 Coordinator 来执行，也可以是 Worker 来执行： Coordinator Commit：Worker 向 Coordinator 汇报 Task 完成，Coordinator 确认该 Task 是否仍属于该 Worker，是则进行结果文件 Commit，否则直接忽略 Worker Commit：Worker 向 Coordinator 汇报 Task 完成，Coordinator 确认该 Task 是否仍属于该 Worker 并响应 Worker，是则 Worker 进行结果文件 Commit，再向 Coordinator 汇报 Commit 完成 这里两种方案都是可行的，各有利弊。我在我的实现中选择了 Coordinator Commit，因为它可以少一次 RPC 调用，在编码实现上会更简单，但缺点是所有 Task 的最终 Commit 都由 Coordinator 完成，在极端场景下会让 Coordinator 变成整个 MR 过程的性能瓶颈。 代码设计与实现代码的设计及实现主要是三个部分： Coordinator 与 Worker 间的 RPC 通信，对应 mr/rpc.go 文件 Coordinator 调度逻辑，对应 mr/coordinator.go 文件 Worker 计算逻辑，对应 mr/worker.go 文件 RPC 通信Coordinator 与 Worker 间的需要进行的通信主要有两块： Worker 在空闲时向 Coordinator 发起 Task 请求，Coordinator 响应一个分配给该 Worker 的 Task Worker 在上一个 Task 运行完成后向 Coordinator 汇报 考虑到上述两个过程总是交替进行的，且 Worker 在上一个 Task 运行完成后总是立刻会需要申请一个新的 Task，在实现上这里我把它们合并为了一个 RPC 调用： ApplyForTask RPC： 由 Worker 向 Coordinator 发起，申请一个新的 Task，同时汇报上一个运行完成的 Task（如有） Coordinator 接收到 RPC 请求后将同步阻塞，直到有可用的 Task 分配给该 Worker 或整个 MR 作业已运行完成 参数： Worker ID 上一个完成的 Task 的类型及 Index。可能为空 响应： 新 Task 的类型及 Index。若为空则代表 MR 作业已完成，Worker 可退出 运行新 Task 所需的其他信息，包括： 如果是 MAP Task，需要 对应的输入文件名 总 REDUCE Task 数量，用于生成中间结果文件 如果是 REDUCE Task，需要总 MAP Task 数量，用于生成对应中间结果文件的文件名 可点击链接 https://github.com/Mr-Dai/MIT-6.824/blob/master/src/mr/rpc.go 查看我的完整实现。 Coordinator由于涉及整个 MR 作业的运行过程调度以及 Worker Failover 的处理，Coordinator 组件的逻辑会相对复杂。 首先，Coordinator 需要维护以下状态信息： 基础配置信息，包括 总 MAP Task 数量、总 Reduce Task 数量 调度所需信息，包括 当前所处阶段，是 MAP 还是 REDUCE 所有仍未完成的 Task 及其所属的 Worker 和 Deadline（若有），使用 Golang Map 结构实现 所有仍未分配的 Task 池，用于响应 Worker 的申请及 Failover 时的重新分配，使用 Golang Channel 实现 123456789type Coordinator struct { lock sync.Mutex // 保护共享信息，避免并发冲突 stage string // 当前作业阶段，MAP or REDUCE。为空代表已完成可退出 nMap int nReduce int tasks map[string]Task availableTasks chan Task} 然后，Coordinator 需要实现以下几个过程： 在启动时，基于指定的输入文件生成 MAP Task 到可用 Task 池中 处理 Worker 的 Task 申请 RPC，从池中分配一个可用的 Task 给 Worker 并响应 处理 Worker 的 Task 完成通知，完成 Task 最终的结果数据 Commit 在 MAP Task 全部完成后，转移至 REDUCE 阶段，生成 REDUCE Task 到可用 Task 池 在 REDUCE Task 全部完成后，标记 MR 作业已完成，退出 周期巡检正在运行的 Task，发现 Task 运行时长超出 10s 后重新分配其到新的 Worker 上运行 这里我们一个个来。先看 Coordinator 启动时的 MAP Task 生成： 1234567891011121314151617181920212223242526272829func MakeCoordinator(files []string, nReduce int) *Coordinator { c := Coordinator{ stage: MAP, nMap: len(files), nReduce: nReduce, tasks: make(map[string]Task), availableTasks: make(chan Task, int(math.Max(float64(len(files)), float64(nReduce)))), } // 每个输入文件生成一个 MAP Task for i, file := range files { task := Task{ Type: MAP, Index: i, MapInputFile: file, } c.tasks[GenTaskID(task.Type, task.Index)] = task c.availableTasks &lt;- task } // 启动 Coordinator，开始响应 Worker 请求 log.Printf(&quot;Coordinator start\\n&quot;) c.server() // 启动 Task 自动回收过程 // ... return &amp;c} 然后我们再来看 可用 Task 获取与分配： 1234567891011121314151617181920212223242526272829303132// 基于 Task 的类型和 Index 值生成唯一 IDfunc GenTaskID(t string, index int) string { return fmt.Sprintf(&quot;%s-%d&quot;, t, index)}// ApplyForTask RPC 的处理入口，由 Worker 调用func (c *Coordinator) ApplyForTask(args *ApplyForTaskArgs, reply *ApplyForTaskReply) error { if args.LastTaskType != &quot;&quot; { // 记录 Worker 的上一个 Task 已经运行完成 // ... } // 获取一个可用 Task 并返回 task, ok := &lt;- c.availableTasks if !ok { // Channel 关闭，代表整个 MR 作业已完成，通知 Worker 退出 return nil } c.lock.Lock() defer c.lock.Unlock() log.Printf(&quot;Assign %s task %d to worker %s\\n&quot;, task.Type, task.Index, args.WorkerID) task.WorkerID = args.WorkerID task.Deadline = time.Now().Add(10 * time.Second) c.tasks[GenTaskID(task.Type, task.Index)] = task // 记录 Task 分配的 Worker ID 及 Deadline reply.TaskType = task.Type reply.TaskIndex = task.Index reply.MapInputFile = task.MapInputFile reply.MapNum = c.nMap reply.ReduceNum = c.nReduce return nil} 然后是 Worker Task 已完成的处理： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// ApplyForTask RPC 的处理入口，由 Worker 调用func (c *Coordinator) ApplyForTask(args *ApplyForTaskArgs, reply *ApplyForTaskReply) error { if args.LastTaskType != &quot;&quot; { // 记录 Worker 的上一个 Task 已经运行完成 c.lock.Lock() lastTaskID := GenTaskID(args.LastTaskType, args.LastTaskIndex) // 判断该 Task 是否仍属于该 Worker，如果已经被重新分配则直接忽略，进入后续的新 Task 分配过程 if task, exists := c.tasks[lastTaskID]; exists &amp;&amp; task.WorkerID == args.WorkerID { log.Printf( &quot;Mark %s task %d as finished on worker %s\\n&quot;, task.Type, task.Index, args.WorkerID) // 将该 Worker 的临时产出文件标记为最终产出文件 if args.LastTaskType == MAP { for ri := 0; ri &lt; c.nReduce; ri++ { err := os.Rename( tmpMapOutFile(args.WorkerID, args.LastTaskIndex, ri), finalMapOutFile(args.LastTaskIndex, ri)) if err != nil { log.Fatalf( &quot;Failed to mark map output file `%s` as final: %e&quot;, tmpMapOutFile(args.WorkerID, args.LastTaskIndex, ri), err) } } } else if args.LastTaskType == REDUCE { err := os.Rename( tmpReduceOutFile(args.WorkerID, args.LastTaskIndex), finalReduceOutFile(args.LastTaskIndex)) if err != nil { log.Fatalf( &quot;Failed to mark reduce output file `%s` as final: %e&quot;, tmpReduceOutFile(args.WorkerID, args.LastTaskIndex), err) } } // 当前阶段所有 Task 已完成，进入下一阶段 delete(c.tasks, lastTaskID) if len(c.tasks) == 0 { c.transit() } } c.lock.Unlock() } // 获取一个可用 Task 并返回 // ...} 然后我们来看 作业运行阶段的切换： 12345678910111213141516171819202122func (c *Coordinator) transit() { if c.stage == MAP { // MAP Task 已全部完成，进入 REDUCE 阶段 log.Printf(&quot;All MAP tasks finished. Transit to REDUCE stage\\n&quot;) c.stage = REDUCE // 生成 Reduce Task for i := 0; i &lt; c.nReduce; i++ { task := Task{ Type: REDUCE, Index: i, } c.tasks[GenTaskID(task.Type, task.Index)] = task c.availableTasks &lt;- task } } else if c.stage == REDUCE { // REDUCE Task 已全部完成，MR 作业已完成，准备退出 log.Printf(&quot;All REDUCE tasks finished. Prepare to exit\\n&quot;) close(c.availableTasks) // 关闭 Channel，响应所有正在同步等待的 RPC 调用 c.stage = &quot;&quot; // 使用空字符串标记作业完成 }} 最后我们再来看 过期 Task 的回收。考虑到该过程需要对已分配的 Task 进行周期巡检，我们直接在 Coordinator 启动时启动一个 Goroutine 来实现： 1234567891011121314151617181920212223242526272829func MakeCoordinator(files []string, nReduce int) *Coordinator { // ... // 启动 Coordinator，开始响应 Worker 请求 log.Printf(&quot;Coordinator start\\n&quot;) c.server() // 启动 Task 自动回收过程 go func() { for { time.Sleep(500 * time.Millisecond) c.lock.Lock() for _, task := range c.tasks { if task.WorkerID != &quot;&quot; &amp;&amp; time.Now().After(task.Deadline) { // 回收并重新分配 log.Printf( &quot;Found timed-out %s task %d previously running on worker %s. Prepare to re-assign&quot;, task.Type, task.Index, task.WorkerID) task.WorkerID = &quot;&quot; c.availableTasks &lt;- task } } c.lock.Unlock() } }() return &amp;c} 可点击链接 https://github.com/Mr-Dai/MIT-6.824/blob/master/src/mr/coordinator.go 查看我的完整实现。 WorkerWorker 的核心逻辑比较简单，主要是一个死循环，不断地向 Coordinator 调用 ApplyForTask RPC： Coordinator 返回空响应，代表 MR 作业已完成，则退出循环，结束 Worker 进程 Coordinator 返回 MAP Task，则 读取对应输入文件的内容 传递至 MR APP 指定的 Map 函数，得到对应的中间结果 按中间结果 Key 的 Hash 值进行分桶，保存至中间结果文件 Coordinator 返回 REDUCE Task，则 读取所有属于该 REDUCE Task 的中间结果文件数据 对所有中间结果进行排序，并按 Key 值进行归并 传递归并后的数据至 MR APP 指定的 REDUCE 函数，得到最终结果 写出到结果文件 先看最外层的循环： 123456789101112131415161718192021222324252627282930313233343536373839func Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) { // 单机运行，直接使用 PID 作为 Worker ID，方便 debug id := strconv.Itoa(os.Getpid()) log.Printf(&quot;Worker %s started\\n&quot;, id) // 进入循环，向 Coordinator 申请 Task var lastTaskType string var lastTaskIndex int for { args := ApplyForTaskArgs{ WorkerID: id, LastTaskType: lastTaskType, LastTaskIndex: lastTaskIndex, } reply := ApplyForTaskReply{} call(&quot;Coordinator.ApplyForTask&quot;, &amp;args, &amp;reply) if reply.TaskType == &quot;&quot; { // MR 作业已完成，退出 log.Printf(&quot;Received job finish signal from coordinator&quot;) break } log.Printf(&quot;Received %s task %d from coordinator&quot;, reply.TaskType, reply.TaskIndex) if reply.TaskType == MAP { // 处理 MAP Task // ... } else if reply.TaskType == REDUCE { // 处理 REDUCE Task // ... } // 记录已完成 Task 的信息，在下次 RPC 调用时捎带给 Coordinator lastTaskType = reply.TaskType lastTaskIndex = reply.TaskIndex log.Printf(&quot;Finished %s task %d&quot;, reply.TaskType, reply.TaskIndex) } log.Printf(&quot;Worker %s exit\\n&quot;, id)} 然后是 MAP Task 的处理： 12345678910111213141516171819202122232425// 读取输入数据file, err := os.Open(reply.MapInputFile)if err != nil { log.Fatalf(&quot;Failed to open map input file %s: %e&quot;, reply.MapInputFile, err)}content, err := ioutil.ReadAll(file)if err != nil { log.Fatalf(&quot;Failed to read map input file %s: %e&quot;, reply.MapInputFile, err)}// 传递输入数据至 MAP 函数，得到中间结果kva := mapf(reply.MapInputFile, string(content))// 按 Key 的 Hash 值对中间结果进行分桶hashedKva := make(map[int][]KeyValue)for _, kv := range kva { hashed := ihash(kv.Key) % reply.ReduceNum hashedKva[hashed] = append(hashedKva[hashed], kv)}// 写出中间结果文件for i := 0; i &lt; reply.ReduceNum; i++ { ofile, _ := os.Create(tmpMapOutFile(id, reply.TaskIndex, i)) for _, kv := range hashedKva[i] { fmt.Fprintf(ofile, &quot;%v\\t%v\\n&quot;, kv.Key, kv.Value) } ofile.Close()} 最后是 REDUCE Task 的处理： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// 读取输入数据var lines []stringfor mi := 0; mi &lt; reply.MapNum; mi++ { inputFile := finalMapOutFile(mi, reply.TaskIndex) file, err := os.Open(inputFile) if err != nil { log.Fatalf(&quot;Failed to open map output file %s: %e&quot;, inputFile, err) } content, err := ioutil.ReadAll(file) if err != nil { log.Fatalf(&quot;Failed to read map output file %s: %e&quot;, inputFile, err) } lines = append(lines, strings.Split(string(content), &quot;\\n&quot;)...)}var kva []KeyValuefor _, line := range lines { if strings.TrimSpace(line) == &quot;&quot; { continue } parts := strings.Split(line, &quot;\\t&quot;) kva = append(kva, KeyValue{ Key: parts[0], Value: parts[1], })}// 按 Key 对输入数据进行排序sort.Sort(ByKey(kva))ofile, _ := os.Create(tmpReduceOutFile(id, reply.TaskIndex))// 按 Key 对中间结果的 Value 进行归并，传递至 Reduce 函数i := 0for i &lt; len(kva) { j := i + 1 for j &lt; len(kva) &amp;&amp; kva[j].Key == kva[i].Key { j++ } var values []string for k := i; k &lt; j; k++ { values = append(values, kva[k].Value) } output := reducef(kva[i].Key, values) // 写出至结果文件 fmt.Fprintf(ofile, &quot;%v %v\\n&quot;, kva[i].Key, output) i = j}ofile.Close() 可点击链接 https://github.com/Mr-Dai/MIT-6.824/blob/master/src/mr/worker.go 查看我的完整实现。 思考延伸在这个 Lab 中，我们实现了 单机多进程 的 MapReduce 框架。在 Lab 文档的最后，也有建议同学们尝试实现 多机分布式 的版本。这里我就不给出具体代码了，简单分析下要做到这一点大致需要解决以下问题： 调整 Worker ID 的生成方式，保证在多机分布式模式下不重复 实现多机 RPC 通信。Worker 如何知道 Coordinator 的 Hostname 及端口？ 中间结果数据的传输？有两类方案： 直接写入到如 AWS S3 等共享存储。改动成本低，但依赖外部服务 参考 Google MapReduce 的做法，保存在 Map Worker 的本地磁盘，Reduce Worker 通过 RPC 向 Map Worker 拉取数据 此外，我在上文中给出的实现代码也比较简单，在大数据量的场景下也有着不小的改进空间，包括： Worker 是否可以得知自己的 Task 已超出 Deadline 并主动处理？ 调整 Map / Reduce 函数签名，让整个 Map / Reduce 过程 Streaming 化，避免因总输入/输出数据量过大导致进程 OOM 比起在 Reduce Task 开始时对完整输入数据进行全排序，也可在各个 Map Task 末尾先进行局部排序，再在 Reduce Task 开始时进行有序归并 时至今日，随着 Hadoop 生态的流行，MapReduce 的运行时实现方案已经非常成熟，上述问题的答案想必都能在 Hadoop 的实现中找到。感兴趣的读者也可在此次 Lab 后自行翻阅 Hadoop MapReduce 的源码，了解并学习我们的实现相比真实的大数据集生产环境还有哪些可以改进的地方。","link":"/mit-6824-lab1/"},{"title":"MongoDB CRUD Cookbook","text":"This post is just a cookbook for me to look up all those MongoDB operations, so there won’t be too many basic concepts about MongoDB or advanced usage. I’ll try to make this post perfect for those who don’t know MongoDB at all to get the hang of it as easily as possible. In this post, I’ll show you MongoDB’s basic CRUD operations. Note that this post will not teach you how to install MongoDB on your computer or how to open its shell. For these basic usage, you should check out the official tutorial. What is MongoDB?So, first of all, what is MongoDB? I guess some of you may have already learned that MongoDB is a NoSQL database, it is somehow faster than traditional relational database like MySQL and Oracle, and maybe some of you even know that MongoDB store data in key-value pairs. But does it make sense? If it truly stores data in key-value pairs, then it probably looks just like the HashMap in Java. Is that a data structure that is good enough to support all those complex computations in relational database? Absolutely not, and MongoDB is absolutely more than that. Before MongoDB, we know there are different types of database systems. Like there is distributed memory database like Memcached. Although Memcached has its remarkable performance, Memcached provides way less functions comparing to all those relational databases. Speaking of relational databases, MySQL provides lots of useful functionalities, like aggregation, JOIN, and support for transactions. But you may have already known that relational databases are pretty slow and they can hardly meet the requirement of high-performance computing like Big Data Processing. After all these, here comes the MongoDB. Unlike Memcached and all those relational databases, MongoDB always try to strive a perfect balance between performance and functionality. You may have alreay known that MongoDB is much faster than relational database, but you still don’t know that MongoDB also provides lots of functions, which make it a totally sufficient substitution for all those relational databases. CRUD - CFirst of all, you have to know, MongoDB stores data in JSON format, and its shell operation bases on JavaScript. Instead of storing records in tables like relational database does, MongoDB stores data as documents in collections. So in this section, I’ll teach you how to insert documents into a collection. Inserting DocumentThe following JavaScript code insert a document into a collection called inventory. If the collection does not exist, it will be created automatically. 1234567891011db.inverntory.insert( { item: &quot;ABC1&quot;, details: { model: &quot;14Q3&quot;, manufacturer: &quot;XYZ Company&quot;, }, stock: [ { size: &quot;S&quot;, qty: 25 }, { size: &quot;M&quot;, qty: 50 } ], category: &quot;clothing&quot; }); This insert method returns a WriteResult object with the status of the operation. A successful insertion of the document returns the following object: 1WriteResuls({ &quot;nInserted&quot; : 1 }) The nInserted field gives you the number of documents inserted by this operation. If the operation encounters an error, the WriteResult object will contain the error information. Inserting an Array of DocumentsYou can groups the documents you want to save into an array of JavaScript object, than insert the whole array: 12345678910111213141516171819202122var myDocuments= [ { item: &quot;ABC2&quot;, details: { model: &quot;14Q3&quot;, manufacturer: &quot;M1 Corporation&quot; }, stock: [ { size: &quot;M&quot;, qty: 50 } ], category: &quot;clothing&quot; }, { item: &quot;MNO2&quot;, details: { model: &quot;14Q3&quot;, manufacturer: &quot;ABC Company&quot; }, stock: [ { size: &quot;S&quot;, qty: 5 }, { size: &quot;M&quot;, qty: 5 }, { size: &quot;L&quot;, qty: 1 } ], category: &quot;clothing&quot; }, { item: &quot;IJK2&quot;, details: { model: &quot;14Q2&quot;, manufacturer: &quot;M5 Corporation&quot; }, stock: [ { size: &quot;S&quot;, qty: 5 }, { size: &quot;L&quot;, qty: 3 } ], category: &quot;houseware&quot; } ];db.inventory.insert(myDocuments); This method will return a BulkWriteResult object with the status of the operation. The result of a successful insertion of multiple documents might looks as follows: 12345678910BulkWriteResult({ &quot;writeErrors&quot; : [ ], &quot;writeConcernErrors&quot; : [ ], &quot;nInserted&quot; : 3, &quot;nUpserted&quot; : 0, &quot;nMatched&quot; : 0, &quot;nModified&quot; : 0, &quot;nRemoved&quot; : 0, &quot;upserted&quot; : [ ]}) Similarly, the nInserted field gives you the number of documents inserted. If the operation encounters an error, the error message will be given in the returned object. Inserting Multiple Documents Using Bulk OperationFirst, you need to use the following code to initialize a bulk operation: 1var bulk = db.inventory.initializeUnorderedBulkOp(); By “unordered opertion”, it means the execution order of the operations in the bulk is not important, and MongoDB can execute all these operations in parallel. If an error occurs when processing one of the write operations, MongoDB will continue to process the remaining write operations. 1234567891011121314151617bulk.insert( { item: &quot;BE10&quot;, details: { model: &quot;14Q2&quot;, manufacturer: &quot;XYZ Company&quot; }, stock: [ { size: &quot;L&quot;, qty: 5 } ], category: &quot;clothing&quot; });bulk.insert( { item: &quot;ZYT1&quot;, details: { model: &quot;14Q1&quot;, manufacturer: &quot;ABC Company&quot; }, stock: [ { size: &quot;S&quot;, qty: 5 }, { size: &quot;M&quot;, qty: 5 } ], category: &quot;houseware&quot; });bulk.execute(); The execute method execute the operations specified in the bulk and returns a BulkWriteResult object. CRUD - RSelecting All Documents in a Collection12db.inventory.find( {} );db.inventory.find(); Both commands do the same thing. Specifying Equality Condition1db.inventory.find( { type: &quot;snacks&quot; } ); Specifying Conditions Using Query Operators1db.inventory.find( { type: { $in: [ &quot;food&quot;, &quot;snacks&quot; ] } } ); Although you can express this query using the $or operator, use the $in operator rather than the $or operator when performing equality checks on the same field. Specifying AND Conditions1db.inventory.find( { type: 'food', price: { $lt: 9.95 } } ); Specifying OR Conditions12345db.inventory.find( { $or: [ { qty: { $gt: 100 } }, { price: { $lt: 9.95 } } ] }); Specifying AND as well as OR Conditions123456db.inventory.find( { type: &quot;food&quot;, $or: [ { qty: { $gt: 100 } }, { price: { $lt: 9.95 } } ] }); Embedded DocumentsAs you may have already seen that documents can be hierarchical, meaning document contains documents, which is also called embedded documents. When the field holds an embedded document, a query can either specify an exact match on the embedded document or specify a match by individual fields in the embedded document using the dot notation. Exact Match on the Embedded DocumentTo specify an equality match on the whole embedded document, use the query document { &lt;field&gt;: &lt;value&gt; } where &lt;value&gt; is the document to match. Equality match on an embedded document requires an exact match of the specified &lt;value&gt;, including the field order. 123456789db.invntory.find( { producer: { company: &quot;ABC123&quot;, address: &quot;123 Street&quot; } }); Equality Match on Fields within an Embedded DocumentUse the dot notation to match by specific fields in an embedded document. Equality matches for specific fields in an embedded document will select documents in the collection where the embedded document contains the specified fields with the specified values. The embedded document can contain additional fields. 1db.inventory.find( { &quot;producer.company&quot;: &quot;ABC123&quot; } ); ArraysWhen the field holds an array, you can query for an exact array match or for specific values in the array. If the array contains embedded documents, you can query for specific fields in the embedded documents using dot notation. If you specify multiple conditions using the $elemMatch operator, the array must contain at least one element that satisfies all the conditions. If you specify multiple conditions without using the $elemMatch operator, then some combination of the array elements, not necessarily a single element, must satisfy all the conditions; i.e. different elements in the array can satisfy different parts of the conditions. Consider an inventory collection that contains the following documents: 123{ &quot;_id&quot;: 5, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;aaa&quot;, &quot;ratings&quot;: [ 5, 8, 9 ]}{ &quot;_id&quot;: 6, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;bbb&quot;, &quot;ratings&quot;: [ 5, 9 ]}{ &quot;_id&quot;: 7, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;ccc&quot;, &quot;ratings&quot;: [ 9, 5, 8 ]} Exact Match on an ArrayTo specify an equality match on an array, use the query document { &lt;field&gt;: &lt;value&gt; } where &lt;value&gt; is the array to match. Equality matches on the array require an exact match of the specified &lt;value&gt;, including the element order. 1db.inventory.find( { ratings: [ 5, 8, 9 ] } ); This operation returns the following document: 1{ &quot;_id&quot;: 5, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;aaa&quot;, &quot;ratings&quot;: [ 5, 8, 9 ]} Matching an Array ElementEquality matches can be specified to match only a single element in the array. These specifications match if the array contains at least one element with the specified value. 1db.inventory.find( { ratings : 5 } ); This operation returns the following documents: 123{ &quot;_id&quot;: 5, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;aaa&quot;, &quot;ratings&quot;: [ 5, 8, 9 ]}{ &quot;_id&quot;: 6, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;bbb&quot;, &quot;ratings&quot;: [ 5, 9 ]}{ &quot;_id&quot;: 7, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;ccc&quot;, &quot;ratings&quot;: [ 9, 5, 8 ]} Matching a Specific Element in an ArrayEquality matches can be specified to match an element with particular index using the dot notation. 1db.inventory.find( { &quot;ratings.0&quot;: 5 } ); This operation returns the following documents: 12{ &quot;_id&quot;: 5, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;aaa&quot;, &quot;ratings&quot;: [ 5, 8, 9 ]}{ &quot;_id&quot;: 6, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;bbb&quot;, &quot;ratings&quot;: [ 5, 9 ]} Specifying Multiple Criteria for Array ElementsYou can use $elemMatch to specify multiple criteria on the elements of an array so that the returned documentshave at least one element in the array field satisfies all the specified criteria. 1db.inventory.find( { ratings: { $elemMatch: { $gt: 5, $lt: 9 } } } ); This operation returns the following documents: 12{ &quot;_id&quot;: 5, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;aaa&quot;, &quot;ratings&quot;: [ 5, 8, 9 ]}{ &quot;_id&quot;: 7, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;ccc&quot;, &quot;ratings&quot;: [ 9, 5, 8 ]} The following code queries for documents whose ratings array contains elements that in some combination satisfy the query conditions;e.g., one element satisfying the “greater than 5: condition while another element satisfying the “less than 9” condition, or a single element satisfying both: 1db.inventory.find( { ratings: { $gt: 5, $lt: 9 } } ); This operation returns the following documents: 123{ &quot;_id&quot;: 5, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;aaa&quot;, &quot;ratings&quot;: [ 5, 8, 9 ]}{ &quot;_id&quot;: 6, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;bbb&quot;, &quot;ratings&quot;: [ 5, 9 ]}{ &quot;_id&quot;: 7, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;ccc&quot;, &quot;ratings&quot;: [ 9, 5, 8 ]} Array of Embedded DocumentsConsider that the inventory collection includes the following documents: 123456789101112131415161718192021[ { &quot;_id&quot;: 100, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;xyz&quot;, &quot;qty&quot;: 25, &quot;price&quot;: 2.5, &quot;ratings&quot;: [ 5, 8, 9 ], &quot;memos&quot;: [ { &quot;memo&quot;: &quot;on time&quot;, &quot;by&quot;: &quot;shipping&quot; }, { &quot;memo&quot;: &quot;approved&quot;, &quot;by&quot;: &quot;billing&quot; } ] }, { &quot;_id&quot;: 101, &quot;type&quot;: &quot;fruit&quot;, &quot;item&quot;: &quot;jkl&quot;, &quot;qty&quot;: 10, &quot;price&quot;: 4.25, &quot;ratings&quot;: [ 5, 9 ], &quot;memos&quot;: [ { &quot;memo&quot;: &quot;on time&quot;, &quot;by&quot;: &quot;payment&quot; }, { &quot;memo&quot;: &quot;delayed&quot;, &quot;by&quot;: &quot;shipping&quot; } ] }] If you know the index of the embedded document, you can specify the criteria with the embedded document’s position using the dot notation. 1db.inventory.find( { &quot;memos.0.by&quot;: &quot;shiping&quot; } ); This operation returns the following document: 123456789{ &quot;_id&quot;: 100, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;xyz&quot;, &quot;qty&quot;: 25, &quot;price&quot;: 2.5, &quot;ratings&quot;: [ 5, 8, 9 ], &quot;memos&quot;: [ { &quot;memo&quot;: &quot;on time&quot;, &quot;by&quot;: &quot;shipping&quot; }, { &quot;memo&quot;: &quot;approved&quot;, &quot;by&quot;: &quot;billing&quot; } ]} If you do not know the index position of the document in the array, concatenate the name of the array field, a dot (.) and the name of the field in the embedded document. 1db.inventory.find( { &quot;memos.by&quot;: &quot;shipping&quot; } ); This operation returns the following documents: 123456789101112131415161718{ &quot;_id&quot;: 100, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;xyz&quot;, &quot;qty&quot;: 25, &quot;price&quot;: 2.5, &quot;ratings&quot;: [ 5, 8, 9 ], &quot;memos&quot;: [ { &quot;memo&quot;: &quot;on time&quot;, &quot;by&quot;: &quot;shipping&quot; }, { &quot;memo&quot;: &quot;approved&quot;, &quot;by&quot;: &quot;billing&quot; } ]}{ &quot;_id&quot;: 101, &quot;type&quot;: &quot;fruit&quot;, &quot;item&quot;: &quot;jkl&quot;, &quot;qty&quot;: 10, &quot;price&quot;: 4.25, &quot;ratings&quot;: [ 5, 9 ], &quot;memos&quot;: [ { &quot;memo&quot;: &quot;on time&quot;, &quot;by&quot;: &quot;payment&quot; }, { &quot;memo&quot;: &quot;delayed&quot;, &quot;by&quot;: &quot;shipping&quot; } ]} Specify Multiple Criteria for Array of DocumentsUse $elemMatch operator to specify multiple criteria on an array of embedded documents.Such operations only return documents that have at least one embedded document satisfies all the specified criteria. 123456789101112db.inventory.find( { memos: { $elemMatch: { memo: &quot;on time&quot;, by: &quot;shipping&quot; } } }); This operation returns the following document: 123456789{ &quot;_id&quot;: 100, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;xyz&quot;, &quot;qty&quot;: 25, &quot;price&quot;: 2.5, &quot;ratings&quot;: [ 5, 8, 9 ], &quot;memos&quot;: [ { &quot;memo&quot;: &quot;on time&quot;, &quot;by&quot;: &quot;shipping&quot; }, { &quot;memo&quot;: &quot;approved&quot;, &quot;by&quot;: &quot;billing&quot; } ]} The following example queries for documents whose memos array contains elements that in some combination satisfy the query conditions; e.g. one element satisfying the “field memo equal to 'on time'“ condition and another element satisfying the “field by equal to 'shipping'“ condition, or a single element satisfying both criteria: 123456db.inventory.find( { &quot;memos.memo&quot;: &quot;on time&quot;, &quot;memos.by&quot;: &quot;shipping&quot; }); This query returns the following documents: 123456789101112131415161718{ &quot;_id&quot;: 100, &quot;type&quot;: &quot;food&quot;, &quot;item&quot;: &quot;xyz&quot;, &quot;qty&quot;: 25, &quot;price&quot;: 2.5, &quot;ratings&quot;: [ 5, 8, 9 ], &quot;memos&quot;: [ { &quot;memo&quot;: &quot;on time&quot;, &quot;by&quot;: &quot;shipping&quot; }, { &quot;memo&quot;: &quot;approved&quot;, &quot;by&quot;: &quot;billing&quot; } ]}{ &quot;_id&quot;: 101, &quot;type&quot;: &quot;fruit&quot;, &quot;item&quot;: &quot;jkl&quot;, &quot;qty&quot;: 10, &quot;price&quot;: 4.25, &quot;ratings&quot;: [ 5, 9 ], &quot;memos&quot;: [ { &quot;memo&quot;: &quot;on time&quot;, &quot;by&quot;: &quot;payment&quot; }, { &quot;memo&quot;: &quot;delayed&quot;, &quot;by&quot;: &quot;shipping&quot; } ]} Limiting Fields Returned by a QueryThe projection document limits the returned fields of matching documents. The projection document can specify inclusion or exclusion rules of fields. 1234db.users.find( { age: { $gt: 18 } }, { name: 1, age: 1, id_: 0 }); Projection operation has inclusive mode and exclusive mode, in which you can specify which fields to include and which fields to exclude respectively. You cannot specify inclusion rules and exclusion rules in one query, but you can exclude the _id field when you are using inclusion projections. Returning All Fields of Matched Documents1db.inventory.find( { type: 'food' } ) Returning the Specified Fields and the _id Field Only1db.inventory.find( { type: 'food' }, { item: 1, qty: 1 } ) Returning Specified Fields Only (Inclusive Mode)1db.inventory.find( { type: 'food' }, { item: 1, qty: 1, _id:0 } ) Returning All But the Specified Fields (Exclusive Mode)1db.inventory.find( { type: 'food' }, { type:0 } ) Projection on Array FieldsFor fields that contain arrays, MongoDB provides the following projection operators: $elemMatch, $slice, and $. For example, consider the inventory collection contains the following document: 1{ &quot;_id&quot; : 5, &quot;type&quot; : &quot;food&quot;, &quot;item&quot; : &quot;aaa&quot;, &quot;ratings&quot; : [ 5, 8, 9 ] } Then the following operation uses the $slice projection operator to return just the first two elements in the ratings array. 1db.inventory.find( { _id: 5 }, { ratings: { $slice: 2 } } ) $elemMatch, $slice, and $ are the only way to project portions of an array. For instance, you cannot project a portion of an array using the array index; e.g. { &quot;ratings.0&quot;: 1 } is not equivalent to returning just the first element of the array field. CRUD - UMongoDB provides the update() method to update the documents of a collection. This method accepts the following parameters: an update conditions document to match the documents to update, an update operations document to specify the modification to perform, and an options document. To specify the update condition, use the same structure and syntax as the query conditions. Updating Specific Fields in a DocumentUsing update operators to change field valuesThe following code uses the $set operator to update the category field and the details field to the specified values and the $currentDate operator to update the field lastModified with the current date for the document with item equal to &quot;MNO2&quot;: 12345678910db.inventory.update( { item: &quot;MNO2&quot; }, { $set: { category: &quot;apparel&quot;, details: { model: &quot;14Q3&quot;, manufacturer: &quot;XYZ Company&quot; } }, $currentDate: { lastModified: true } }); The update operation returns a WriteResult object which contains the status of the operation. A successful update of document returns the following object: 1WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 }) The nMatched field specifies the number of existing documents matched for the update, and nModified specifies the number of existing documents modified. Updating an embedded fieldTo update a field within an embedded document, use the dot notation. When using the dot notation, enclose the whole dotted field name in quotes. 1234db.inventory.update( { item: &quot;ABC1&quot; }, { $set: { &quot;details.model&quot;: &quot;14Q2&quot; } }); Updating multiple documentsBy default, the update() method updates a single document. To update multiple documents, use the multi option in the update() method. 12345678db.inventory.update( { category: &quot;clothing&quot; }, { $set: { category: &quot;apparel&quot; }, $currentDate: { lastModified: true } }, { multi: true }); Replacing the Whole DocumentTo replace the entire content of a document except for the _id field, pass an entirely new document as the second argument to update(). The replacement document can have different fields than the original document. In the replacement document, you can omit the _id field since the _id field is immutable. If you do include the _id field, it must be the same value as the existing value. 12345678db.inventory.update( { item: &quot;BE10&quot; }, { item: &quot;BE05&quot;, stock: [ { size: &quot;S&quot;, qty: 20 }, { size: &quot;M&quot;, qty: 5 } ], category: &quot;apparel&quot; }); The upsert OptionBy default, if no document matches the update query, the update() method does nothing. However, by specifying upsert: true, the update() method either updates matching document or documents, or inserts a new document using the update specification if no matching document exists. Specifying upsert: true in a Document Replacement OperationWhen you specify upsert: true for an update operation to replace a document and no matching documents are found, MongoDB creates a new document using the equality conditions in the update conditions document, and replaces this document, except for the _id field if specified, with the update document. 12345678910db.inventory.update( { item: &quot;TBD1&quot; }, { item: &quot;TBD1&quot;, details: { &quot;model&quot; : &quot;14Q4&quot;, &quot;manufacturer&quot; : &quot;ABC Company&quot; }, stock: [ { &quot;size&quot; : &quot;S&quot;, &quot;qty&quot; : 25 } ], category: &quot;houseware&quot; }, { upsert: true }) The update operation returns a WriteResult object which contains the status of the operation, including whether the db.collection.update() method modified an existing document or added a new document. 123456WriteResult({ &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 1, &quot;nModified&quot; : 0, &quot;_id&quot; : ObjectId(&quot;53dbd684babeaec6342ed6c7&quot;)}); The nMatched field shows that the operation matched 0 documents. The nUpserted of 1 shows that the update added a document. The nModified of 0 specifies that no existing documents were updated. The _id field shows the generated _id field for the added document. Specifying upsert: true in a Field Updating OperationWhen you specify upsert: true for an update operation that modifies specific fields and no matching documents are found, MongoDB creates a new document using the equality conditions in the update conditions document, and applies the modification as specified in the update document. 12345678910db.inventory.update( { item: &quot;TBD2&quot; }, { $set: { details: { &quot;model&quot; : &quot;14Q3&quot;, &quot;manufacturer&quot; : &quot;IJK Co.&quot; }, category: &quot;houseware&quot; } }, { upsert: true }); The update operation also returns a WriteResult object. CRUD - DIn MongoDB, you can use the db.collection.remove() method to remove documents from a collection. You can choose to remove all documents from a collection, remove documents that match given condition, or just remove a single document. Remove All DocumentsTo remove all documents from a collection (clearing the collection), pass an empty query document {} to the remove() method. Such operation will not remove the indexes. 1db.inventory.remove( {} ) To remove all documents from a collection, it may be more efficient to use the drop() method to drop the entire collection, including the indexes, and then recreate the collection and rebuild the indexes. Removing Documents that Match Given ConditionTo remove the documents that match a deletion criteria, call the remove() method with the &lt;query&gt; parameter. 1db.inventory.remove( { type : &quot;food&quot; } ) To remove a single document, call the remove() method with the justOne parameter set to true or 1. 1db.inventory.remove( { type : &quot;food&quot; }, 1 ) To delete a single document sorted by some specified order, use the findAndModify()Specific Fields method.","link":"/mongodb_crud/"},{"title":"MongoDB 分布式部署教程","text":"本文将介绍如何使用 MongoDB 提供的 Replica Set 和 Shards 功能构建一个分布式 MongoDB 集群。 Replica Set 部署我们先从部署一个三节点的 Replica Set 开始。 首先，我们要为每个 mongod 实例创建它自己的 dbpath： 123mkdir 1mkdir 2mkdir 3 然后，我们便可以开始启动这三个 mongod 实例了： 123mongod --dbpath 1 --port 27001 --replSet myRSmongod --dbpath 2 --port 27002 --replSet myRSmongod --dbpath 3 --port 27003 --replSet myRS 注意，这里我是为了在同一台机器上运行三个 mongod 实例，所以需要为它们分别指定不同的端口。如果是真实的分布式 Replica Set，在每台机器上使用默认的 27017 端口是完全可行的。 除此之外，我使用 --replSet 参数指定了 mongod 实例所属 Replica Set 的名字。这个名字是可以随意起的，但必须确保属于同一个 Replica Set 的 mongod 实例设置了相同的 --replSet，否则可能会产生一些不可预期的后果。 在顺利打开这些 mongod 实例后以后，不出意外的话我们应该能在输出的日志信息中看到如下记录： 12345678910111213141516172015-11-14T16:25:46.060+0800 I JOURNAL [initandlisten] journal dir=3\\journal2015-11-14T16:25:46.061+0800 I JOURNAL [initandlisten] recover : no journal files present, no recovery needed2015-11-14T16:25:46.078+0800 I JOURNAL [durability] Durability thread started2015-11-14T16:25:46.078+0800 I JOURNAL [journal writer] Journal writer thread started2015-11-14T16:25:46.613+0800 I CONTROL [initandlisten] MongoDB starting : pid=9812 port=27003 dbpath=3 64-bit host=mrdai-Laptop2015-11-14T16:25:46.613+0800 I CONTROL [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R22015-11-14T16:25:46.613+0800 I CONTROL [initandlisten] db version v3.0.72015-11-14T16:25:46.614+0800 I CONTROL [initandlisten] git version: 6ce7cbe8c6b899552dadd907604559806aa2e9bd2015-11-14T16:25:46.614+0800 I CONTROL [initandlisten] build info: windows sys.getwindowsversion(major=6, minor=1, build=7601, platform=2, service_pack='Service Pack 1') BOOST_LIB_VERSION=1_492015-11-14T16:25:46.614+0800 I CONTROL [initandlisten] allocator: tcmalloc2015-11-14T16:25:46.614+0800 I CONTROL [initandlisten] options: { net: { port: 27003 }, replication: { replSet: &quot;myRS&quot; }, storage: { dbPath: &quot;3&quot; } }2015-11-14T16:25:46.615+0800 I INDEX [initandlisten] allocating new ns file 3\\local.ns, filling with zeroes...2015-11-14T16:25:47.542+0800 I STORAGE [FileAllocator] allocating new datafile 3\\local.0, filling with zeroes...2015-11-14T16:25:47.543+0800 I STORAGE [FileAllocator] creating directory 3\\_tmp2015-11-14T16:25:47.544+0800 I STORAGE [FileAllocator] done allocating datafile 3\\local.0, size: 64MB, took 0 secs2015-11-14T16:25:47.551+0800 I REPL [initandlisten] Did not find local replica set configuration document at startup; NoMatchingDocument Did not find replica set configuration document in local.system.replset2015-11-14T16:25:47.552+0800 I NETWORK [initandlisten] waiting for connections on port 27003 可以注意到，倒数第二条记录显示 mongod 未能在本地数据中找到 Replica Set 的设置信息。这是正常的，因为这是第一次创建的 Replica Set。最后一条信息显示 mongod 启动完毕，等待外界连接它的端口。 那么，我们开始启动 Replica Set。使用 mongo 连入随便一个 mongod 实例，并进行设置： 12345678910var conf = { _id : &quot;myRS&quot;, members : [ { _id : 1, host : &quot;localhost:27001&quot; }, { _id : 2, host : &quot;localhost:27002&quot; }, { _id : 3, host : &quot;localhost:27003&quot; } ]}rs.initiate(conf) 在 conf 中，我们将 _id 设置为 Replica Set 的名称，并在 members 中设置了 Replica Set 所有成员的信息，其中包括成员的名称 _id 以及成员的主机名 host。 注意，尽管这里可以直接使用了 IP:端口 的形式来指定 mongod 实例，但在真实环境中，不要这么做，这种做法十分糟糕。不过现在搭建分布式，大家的做法似乎更倾向于为每台机器修改 hosts 文件。同样，不要这么做，这两种做法都属于 bad practice。最好的做法，是在你的集群环境中配置一台 DNS 服务器。这样，当你的某一个结点的 IP 发生变化时，你就只需要修改 DNS 服务器中的那条解析条目，而不需要修改每个结点的 hosts 文件了。 直接以数字作为每个结点的名称也是不好的做法，因为这个名称在 mongod 的日志信息中会经常出现。使用更加可读的名称是更好的做法。 一切正常的话，你应该会在其中一个结点上看到如下日志信息： 123456789101112131415161718192021222324252627282930313233343536373839404142432015-11-14T16:41:54.946+0800 I NETWORK [initandlisten] connection accepted from 127.0.0.1:61875 #1 (1 connection now open)2015-11-14T16:41:54.951+0800 I NETWORK [conn1] end connection 127.0.0.1:61875 (0 connections now open)2015-11-14T16:41:54.953+0800 I NETWORK [initandlisten] connection accepted from 127.0.0.1:61877 #2 (1 connection now open)2015-11-14T16:41:55.013+0800 I NETWORK [initandlisten] connection accepted from 127.0.0.1:61882 #3 (2 connections now open)2015-11-14T16:41:55.018+0800 I NETWORK [conn3] end connection 127.0.0.1:61882 (1 connection now open)2015-11-14T16:41:55.078+0800 I REPL [WriteReplSetConfig] Starting replication applier threads2015-11-14T16:41:55.082+0800 I REPL [ReplicationExecutor] New replica set config in use: { _id: &quot;myRS&quot;, version: 1, members: [ { _id: 1, host: &quot;localhost:27001&quot;, arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: &quot;localhost:27002&quot;, arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id:3, host: &quot;localhost:27003&quot;, arbiterOnly: false, buildIndexes: true, hidden: false, priority...(line truncated)...2015-11-14T16:41:55.086+0800 I NETWORK [initandlisten] connection accepted from 127.0.0.1:61884 #4 (2 connections now open)2015-11-14T16:41:55.115+0800 I REPL [ReplicationExecutor] This node is localhost:27003 in the config2015-11-14T16:41:55.128+0800 I REPL [ReplicationExecutor] transition to STARTUP22015-11-14T16:41:55.134+0800 I REPL [rsSync] ******2015-11-14T16:41:55.136+0800 I REPL [rsSync] creating replication oplog of size: 6172MB...2015-11-14T16:41:55.137+0800 I STORAGE [FileAllocator] allocating new datafile 3\\local.1, filling with zeroes...2015-11-14T16:41:55.139+0800 I REPL [ReplicationExecutor] Member localhost:27001 is now in state STARTUP22015-11-14T16:41:55.151+0800 I STORAGE [FileAllocator] done allocating datafile 3\\local.1, size: 2047MB, took 0.001 secs2015-11-14T16:41:55.153+0800 I STORAGE [FileAllocator] allocating new datafile 3\\local.2, filling with zeroes...2015-11-14T16:41:55.161+0800 I STORAGE [FileAllocator] done allocating datafile 3\\local.2, size: 2047MB, took 0.001 secs2015-11-14T16:41:55.170+0800 I STORAGE [FileAllocator] allocating new datafile 3\\local.3, filling with zeroes...2015-11-14T16:41:55.171+0800 I REPL [ReplicationExecutor] Member localhost:27002 is now in state STARTUP22015-11-14T16:41:55.186+0800 I STORAGE [FileAllocator] done allocating datafile 3\\local.3, size: 2047MB, took 0.001 secs2015-11-14T16:41:56.198+0800 I REPL [rsSync] ******2015-11-14T16:41:56.198+0800 I REPL [rsSync] initial sync pending2015-11-14T16:41:56.200+0800 I REPL [rsSync] no valid sync sources found in current replset to do an initial sync2015-11-14T16:41:57.139+0800 I REPL [ReplicationExecutor] Member localhost:27001 is now in state SECONDARY2015-11-14T16:41:57.206+0800 I REPL [rsSync] initial sync pending2015-11-14T16:41:57.206+0800 I REPL [ReplicationExecutor] syncing from: localhost:270012015-11-14T16:41:57.221+0800 I REPL [rsSync] initial sync drop all databases2015-11-14T16:41:57.222+0800 I STORAGE [rsSync] dropAllDatabasesExceptLocal 12015-11-14T16:41:57.222+0800 I REPL [rsSync] initial sync clone all databases2015-11-14T16:41:57.229+0800 I REPL [rsSync] initial sync data copy, starting syncup2015-11-14T16:41:57.234+0800 I REPL [rsSync] oplog sync 1 of 32015-11-14T16:41:57.239+0800 I REPL [rsSync] oplog sync 2 of 32015-11-14T16:41:57.254+0800 I REPL [rsSync] initial sync building indexes2015-11-14T16:41:57.258+0800 I REPL [rsSync] oplog sync 3 of 32015-11-14T16:41:57.265+0800 I REPL [rsSync] initial sync finishing up2015-11-14T16:41:57.268+0800 I REPL [rsSync] replSet set minValid=5646f3d4:12015-11-14T16:41:57.274+0800 I REPL [rsSync] initial sync done2015-11-14T16:41:57.290+0800 I REPL [ReplicationExecutor] transition to RECOVERING2015-11-14T16:41:57.292+0800 I REPL [ReplicationExecutor] transition to SECONDARY2015-11-14T16:41:58.136+0800 I REPL [ReplicationExecutor] could not find member to sync from2015-11-14T16:41:58.971+0800 I REPL [ReplicationExecutor] replSetElect voting yea for localhost:27001 (1)2015-11-14T16:41:59.140+0800 I REPL [ReplicationExecutor] Member localhost:27001 is now in state PRIMARY2015-11-14T16:41:59.171+0800 I REPL [ReplicationExecutor] Member localhost:27002 is now in state SECONDARY 从日志中，我们可以很清晰地看到，发起 rs.initiate 的 mongod 向其他 mongod 开启了连接，其他 mongod 获取到了我们配置的 conf 信息。而后，Replica Set 开始启动。首先是各结点进行初始化同步，从发起 rs.initiate 的 mongod 处同步了 oplog，并进入 Secondary 状态。然后，3 个 Secondary 发现 Replica Set 中没有 Primary，于是发起选举。日志里，我们甚至可以看到这个 mongod 把票投给了谁。最后，选举结束，localhost:27001 成为了 Primary。 使用 Java 驱动连接至 Replica Set我们通过如下语句连接至单一的 MongoDB 实例： 1MongoClient client = new MongoClient(&quot;localhost&quot;, 27001); 我们为 MongoClient 对象指定了一个 MongoDB 实例的主机名和端口号。以这种方式初始化的 MongoClient 会假设目标 MongoDB 实例只是一个 standalone 的实例，如果该实例不是 Primary 时，客户端执行写操作则可能被该 MongoDB 实例拒绝。 通过如下语句可使 MongoClient 进入 Replica Set 模式： 123MongoClient client = new MongoClient(asList( new ServerAddress(&quot;localhost&quot;, 27001) )); 我们通过 Arrays#asList 方法为 MongoClient 传入了一个 List，MongoClient 便会进入 Replica Set 模式。在这种模式下，客户端会利用给定的主机（seedlist）来发现 Replica Set 的其他所有结点，其中就包括了 Primary。因此，即使 localhost:27001 不是 Primary 也没关系，客户端会通过它获知 Primary 的地址并自动连接至 Primary。 但以上做法仍不全面：如果 localhost:27001 进程已经挂了，或者它并不是 Replica Set 的成员，我们便无法通过上述语句连接至 Replica Set。我们可以为构造函数传入更多的 MongoDB 实例的地址来降低这种情况发生的几率： 12345MongoClient client = new MongoClient(asList( new ServerAddress(&quot;localhost&quot;, 27001), new ServerAddress(&quot;localhost&quot;, 27002), new ServerAddress(&quot;localhost&quot;, 27003) )); 当然，也有可能正好你指定的这多个结点都同时挂掉，那样自然是防不胜防了。不过，提高 Replica Set 拓扑可用性就是网络架构的问题了。当我们在执行写操作时，我们还需要考虑 Primary 会突然挂掉。比如说，我们正在执行这样的写操作： 123456MongoCollection collection = client.getDatabase(&quot;foo&quot;).getCollection(&quot;bar&quot;);for (int i = 0; i &lt; Integer.MAX_VALUE; i++) { collection.insertOne(new Document(&quot;_id&quot;, new ObjectId()).append(&quot;i&quot;, i)); Thread.sleep(500);} 在执行插入时，如果 Primary 突然失效（如调用了 rs.stepDown()），那么上述代码中的 insertOne 方法会抛出一个错误。因此，更为健壮的做法，是为该 insertOne 语句加上 try/catch 块： 12345678for (int i = 0; i &lt; Integer.MAX_VALUE; i++) { try { collection.insertOne(new Document(&quot;_id&quot;, new ObjectId()).append(&quot;i&quot;, i)); } catch (MongoException e) { // Handle the exception } Thread.sleep(500);} 遗憾的是，抛出错误的 insertOne 操作恐怕无法由 MongoDB 驱动自动重试。实际上，不只是触发错误的那一次操作，在 Replica Set 自动选举出新的 Primary 前，所有写操作都会抛出错误。但幸运的是，由于加上了 try/catch 块，应用程序不会因为单次写入失败便直接退出。在触发错误后，下一次插入前驱动都会重新尝试利用 seedlist 来获取新的 Primary 的地址。当 Replica Set 重新选举出新的 Primary 后，驱动便可以再次进行写操作了。 通过观察 MongoDB Java 驱动输出的日志信息，你可以更细致地观察驱动的行为。这里就不直接给出了，有兴趣可自己尝试。 Shard 集群部署在本节中，我们将会在本机上部署一个完整的生产级别的 MongoDB Shard 集群。集群由 4 个 Shard 负责存储数据，其中每个 Shard 都是包含三个结点的 Replica Set。除此之外，集群还包括 4 个 mongos 和 3 个 Config Server。 注意，用于生产环境的 Shard 集群必须遵循如下几个原则：必须使用 Replica Set 来作为 Shard，任何一个 Shard 的不可用都会导致集群出现异常；必须使用正好 3 个 Config Server，Config Server 不可用将导致整个集群不可用。除此之外，使用两个以上的 mongos 实例可以更好地分散压力。 4 个 Replica Set 的信息分别如下： 123456789101112131415161718192021222324252627282930313233343536{ _id : &quot;a&quot;, members : [ { _id : &quot;a1&quot;, host : &quot;localhost:27001&quot; }, { _id : &quot;a2&quot;, host : &quot;localhost:27002&quot; }, { _id : &quot;a3&quot;, host : &quot;localhost:27003&quot; } ]}{ _id : &quot;b&quot;, members : [ { _id : &quot;b1&quot;, host : &quot;localhost:27101&quot; }, { _id : &quot;b2&quot;, host : &quot;localhost:27102&quot; }, { _id : &quot;b3&quot;, host : &quot;localhost:27103&quot; } ]}{ _id : &quot;c&quot;, members : [ { _id : &quot;c1&quot;, host : &quot;localhost:27201&quot; }, { _id : &quot;c2&quot;, host : &quot;localhost:27202&quot; }, { _id : &quot;c3&quot;, host : &quot;localhost:27203&quot; } ]}{ _id : &quot;d&quot;, members : [ { _id : &quot;d1&quot;, host : &quot;localhost:27301&quot; }, { _id : &quot;d2&quot;, host : &quot;localhost:27302&quot; }, { _id : &quot;d3&quot;, host : &quot;localhost:27303&quot; } ]} 集群各成员启动首先我们分别启动集群的各个成员，分别是 Shard、Config Server 和 Query Router。其中前两种成员均为 mongod，而 Query Router 则是 mongos。 单个 Replica Set 的配置方式大致上无太大变化，只是作为 Shard Server 在启动 mongod 时需要加上--shardsvr选项。以 Replica Set a 为例： 12345mkdir a{1,2,3}mongod --shardsvr --replSet a --dbpath a1 --logpath log.a1 --port 27001 --forkmongod --shardsvr --replSet a --dbpath a2 --logpath log.a2 --port 27002 --forkmongod --shardsvr --replSet a --dbpath a3 --logpath log.a3 --port 27003 --fork 注意：当 --shardsvr 选项被打开时，mongod 的默认端口号变为 27018。 再使用 mongo 连接至任意一个 mongod 实例，启动 Replica Set： 12345678910var conf = { _id : &quot;a&quot;, members : [ { _id : &quot;a1&quot;, host : &quot;localhost:27001&quot; }, { _id : &quot;a2&quot;, host : &quot;localhost:27002&quot; }, { _id : &quot;a3&quot;, host : &quot;localhost:27003&quot; } ]}rs.initiate(conf) 重复上述操作即可启动其余三个 Replica Set。 接下来开始启动 Config Server： 12345mkdir cfg{1,2,3}mongod --configsvr --dbpath cfg1 --logpath log.cfg1 --port 26050 --forkmongod --configsvr --dbpath cfg2 --logpath log.cfg2 --port 26051 --forkmongod --configsvr --dbpath cfg3 --logpath log.cfg3 --port 26052 --fork 注意：当 --configvr 选项被打开时，mongod 的默认端口号变为27019。 最后，启动 Query Router： 1234mongos --configdb localhost:26050,localhost:26051,localhost:26052 --logpath log.mongos1 --forkmongos --configdb localhost:26050,localhost:26051,localhost:26052 --logpath log.mongos2 --port 26061 --forkmongos --configdb localhost:26050,localhost:26051,localhost:26052 --logpath log.mongos3 --port 26062 --forkmongos --configdb localhost:26050,localhost:26051,localhost:26052 --logpath log.mongos4 --port 26063 --fork 注意：mongos 的默认端口号为27017，与 mongod 、 mongo 的默认端口号相同。 如此一来，集群的各个成员都启动完毕了，可以开始配置集群了。 添加 Shard实际上，在启动 mongos 时，我们已经指定了集群所使用的 Config Server 的地址。接下来就是为集群指定每个 Shard 的地址了。 打开 mongo 连接至任意一个 mongos，并执行如下指令： 1234sh.addShard(&quot;a/localhost:27001&quot;)sh.addShard(&quot;b/localhost:27101&quot;)sh.addShard(&quot;c/localhost:27201&quot;)sh.addShard(&quot;d/localhost:27301&quot;) 注意到，我们添加 Shard 时，输入了 Replica Set 的名称以及其中一个成员的地址。该成员并不一定得是 Primary，只要它是该 Replica Set 的成员，mongos 就能自动发现 Replica Set 的其他所有成员。 在添加了 4 个 Shard 以后，整个 Shard 集群便配置完毕，可以开始使用了。","link":"/mongodb_distribution_tutorial/"},{"title":"MongoDB Replica Set","text":"本篇文章将脱离基本的 MongoDB 数据存储和操作，立足于 MongoDB 提供的高可用方案。MongoDB 的高可用方案包括 Replica Set 和 Sharding，这篇文章将介绍 MongoDB 的 Replica Set。 什么是 Replica SetReplica Set 由一组 mongod 实例组成，这些 mongod 都维护着相同的数据集，通过冗余备份的方式提高数据的可用性。其中一个被称为 Primary（等同于 Master）的 mongod 负责接收所有客户端发来的写操作请求。其他被称为 Secondary（等同于 Slave）的 mongod 也会执行相同的操作，以与 Primary 进行同步。 Primary 会将发生在数据集上的改动记录在自己的 oplog 中，其他 Secondary 则负责复制 Primary 的 oplog，并把相关的改动应用在自己的数据集上。由此，Secondary 将始终持有与 Primary 完全相同的数据。 Replica Set 中的任何成员都可以接受客户端发来的读请求，但默认情况下，应用程序的驱动程序只会把读请求发往 Primary，客户端必须通过显式设置 Read Preference 才能改变这种行为。 整个 Replica Set 通过在实例间发送心跳信号来互相知会哪些实例在正常运转，任意一个实例都会与其他所有实例进行心跳通信。 当 Primary 超过 10 秒不与 Replica Set 中其他成员通信时，Replica Set 则认为该 Primary 因各种原因不可用了（断电、断网、死机）。此时，Secondary 们会开始选举，得票最高的 Secondary 则成为新的 Primary。Replica Set 的这项特性被称为 Automatic Failover（自动恢复）。 通过配置，负责维持备份数据的 Secondary 结点还能成为如下三种特殊 Secondary 结点： Priority 0 Secondary：优先级为 0 的 Secondary 结点，无法在 Primary 不可用时被选举为 Primary，适合作为次级数据中心的 Secondary 结点 Hidden Secondary：Hidden Secondary 对客户端不可见，客户端无法将读请求发往 Hidden Secondary。Hidden Secondary 同时也必须是 Priority 0 的 Delayed Secondary：Delayed Secondary 会在指定的延时后才应用 Primary oplog 上的记录，其维护的数据反映了整个 Replica Set 在一定时间以前的状态。Delayed Secondary 必须也是 Priority 0 的，也应该是 Hidden 的 除此之外，Replica Set 中还可以存在一种特殊的 Secondary 结点，它不负责备份任何数据，只负责在 Primary 不可用时进行投票，这样的 Secondary 被称为 Arbiter（仲裁者）。 在 Primary 不可用时，其他某个 Secondary 可能成为新的 Primary，而原本的 Primary 在重新上线后变为 Secondary，但 Arbiter 永远都是 Arbiter。 Replica Set 主从数据同步 为确保每个 Secondary 上都能保持一份最新的数据备份，Replica Set 的实例间会相互进行同步。 具体来说，MongoDB 所使用的同步方式可以被分为两种形式： MongoDB 会使用初始化同步为新加入的成员在本地迅速生成一份完整的数据备份 之后，Secondary 便不断地将数据集发生的变化同步应用在自己的数据集上，以确保自己的数据集能和 Primary 保持同步。 接下来我们就分开来看一下这两种同步形式。 初始化同步 初始化同步（Initial Sync）相当于从同步源将整个数据集复制到本地。在以下两种情况下会触发 mongod 的初始化同步操作： 一个新的 mongod 实例加入了 Replica Set 某个已经加入 Replica Set 的 mongod 实例由于种种原因，其同步状态落后过多，以至于 Primary 的 oplog 中那些它还未同步的修改记录已经被覆写 初始化同步操作大概包含 2 个方面： 复制同步源上除 local 数据库以外的所有数据。这个过程也会同时构建索引； 使用同步源的 oplog 将后来发生的数据集变动应用到本地的备份数据集上。这个过程在后面会一直进行，以始终确保 Secondary 的同步性 在上述两步完成后，新的节点也就会进入到了正常运转的状态，成为一个可用的 Secondary。 有关手动触发初始化同步的更多细节，详见这里。 后续数据同步在完成初始化同步后，Secondary 便会持续地从其同步源处同步数据，异步地读取同步源 oplog 上的新纪录并应用到自己的数据集上。 值得注意的是，Secondary 的同步源并不一定是 Primary 结点，其同步源也有可能随着结点间 Ping 延时长短和状态的变化而发生改变。 Replica Set Oplog Oplog（Operations Log）用于保存所有应用在数据库上的数据修改。MongoDB 会把数据库的写操作发往 Primary，由 Primary 将该操作所产生的数据修改记录到 oplog 上。其他所有的 Secondary 异步地将 Primary 的 oplog复制到本地并将其中的数据修改应用到自己的数据集中。 所有的 mongod 实例都会有一个自己的 oplog。Oplog 实际上储存在 Collection local.oplog.rs 中，而这是一个 Capped Collection（大小固定的 Collection，当达到容量上限时会覆写旧的 Document）。 有关 oplog 的更多信息，详见这里。 Replica Set 数据同步的异步性Secondary 与 Primary 之间的同步实际上是异步的。当用户发送写操作到 Primary 时，Primary 在 oplog 上写下记录并将操作应用在自己的数据集上，然后便会立刻向客户端返回写操作的结果（成功或失败、修改了多少个 Document）。如果使用线程同步的同步操作，Primary 则会等到所有的 Secondary 都将该操作应用到自己的数据集上时才向客户端返回响应，而 MongoDB Replica Set 的同步操作是异步的，Primary 不会等待其他 Secondary。Secondary 会通过心跳信号发现 Primary 的 oplog 上的新修改，而后将其复制到自己的 oplog，并应用在自己的数据集上。 Oplog 记录的幂等性Primary 并不会把用户传来的写操作直接原封不动地记录在 oplog 上，oplog 上记录的操作必须确保是幂等的（idempotent）。所谓幂等，即指这些操作对于同一个 Collection，无论应用多少次都应产生相同的结果。 举个例子。比如说我们发送了一个删除记录的请求：db.users.remove({ age : 30 })。这个记录删除掉 users 中所有年龄为 30 的用户记录。这个操作不是幂等的，因为对于同一个 users，users 中实际存放的数据会影响执行的结果。实际上，Primary 接收到这样的请求，真正记录到 oplog 上的记录会是： 12345db.users.remove({ _id : ... })db.users.remove({ _id : ... })db.users.remove({ _id : ... })db.users.remove({ _id : ... })db.users.remove({ _id : ... }) 也就是说，Primary 会先计算这个操作所会影响的 Document，并在 oplog 中写下对这些 Document 的修改。正是由于 _id 的唯一性，我们可以确保这样的操作记录是幂等的。 Read Preference 客户端可以使用 Read Preference 来决定将读操作发往 Replica Set 的哪个成员。 默认的 Read Preference 会把读操作发往 Primary。考虑到 Replica Set 数据同步的异步性，Secondary 的备份数据集实际上总是滞后于 Primary 的，其滞后程度甚至有可能十分严重以至于其实际上已经和 Primary 失去同步。客户端是没有办法得知某个 Secondary 具体滞后多少的。因此，将读操作发往 Primary 可以确保拿到的数据只会是最新的数据。 而对于那些对数据同步性要求不是那么高的应用程序，将读操作发往距离客户端最近的 Replica Set 成员可以有效地降低 Primary 的压力，提高客户端请求的响应速度。总的来讲，可以直接从 Secondary 读取数据的应用程序用例包括如下几种： 执行不会影响前端应用程序的系统操作 直接从位于本地的 Replica Set 成员读取数据。在这种情况下，Primary 可能距离客户端十分遥远，一次读操作光是花费在链路上的时间可能就达到了上百毫秒， 而直接访问位于本地数据中心的 Secondary 往往可以在几毫秒之内得到响应 确保应用程序在 Primary 不可用时不会受到影响。尽管 MongoDB 有着自动恢复的特性，但仍然需要花费几十秒的时间来重新选举出一个新的 Primary 但实际上，更多时候我们并不推荐使用这种读取模式，因为： Replica Set 中的每个成员都有着大致相同的写操作压力。尽管写操作会被直接发往 Primary，但数据的同步使得这些操作也会发生在每一个 Secondary 上， 而每一个写操作往往都隐含着读操作，因此 Replica Set 的每个成员也有着大致相同的读操作压力； Replica Set 的数据同步是异步的，这意味着 Secondary 的数据必然或多或少落后于 Primary，从 Secondary 读取数据很有可能会拿到过时的数据； 读操作分布在 Secondary 上，这同样要求在某个 Secondary 不可用时，其他 Secondary 要能够处理所有这些多出来的请求。在某种程度上， 这样的读取模式会使得系统的可用性难以估量； 总的来看，在大多数时候，使用 Sharding 来分散压力会是更好的选择。我将在之后的文章里介绍 MongoDB 的 Sharding 功能。 MongoDB 驱动所支持的 Read Preference 包括如下几种： Read Preference 模式 功能 primary 默认的模式，所有读操作都发往 Primary primaryPreferred 优先将读操作发往 Primary，仅在 Primary 不可用时才把读操作发往 Secondary secondary 所有读操作都发往 Secondary secondaryPreferred 优先将读操作发往 Secondary，仅在所有 Secondary 都不可用时才把读操作发往 Primary nearest 将读操作发往与客户端有着最低网络延迟（最“近”）的 Replica Set 成员，不管它是 Primary 还是 Secondary 有关 Read Preference 的更多内容，详见这里。 Write Concern 前文的 Read Preference 决定了驱动程序从 Replica Set 读取数据时的行为，而相对的 Write Concern 则决定了驱动程序写数据时的行为。 实际上，比起只用于 Replica Set 的 Read Preference，Write Concern 还适用于 standalone 的 MongoDB 实例。驱动程序在发出写操作请求时同时发出 Write Concern，MongoDB 实例则会根据 Write Concern 的设置来决定什么时候给客户端返回写操作成功的响应。因此，对于越强的 Write Concern，当 MongoDB 成功返回响应时，你就更能确定数据已被安全保存。但实际上，越强的 Write Concern 往往需要更多的时间等待 MongoDB 返回。 Write Concern 由三个参数组成，分别是 w 、 j 和 wtimeout。接下来将一一介绍。 w 参数w 参数决定了 MongoDB 在多少个实例上写入数据后返回响应。该响应实际上是在 MongoDB 将数据写入到内存缓存后返回，因此返回后仍有可能因为断电等原因导致内存缓存丢失从而丢失该操作引起的数据修改。更高等级的 Write Concern 需由 j 参数给出，我们将在后文讲述。 w 参数可选的值包括如下： 值 功能 1 默认值，在 standalone 的 MongoDB 实例或者 Replica Set 的 Primary 成功执行该写操作后返回响应 0 完全关闭 MongoDB 的执行响应，但由网络错误引起的 Exception 仍会抛出。值得注意的是，即使使用了该设定，如果你还用 j 参数要求 MongoDB 返回日志提交的响应，MongoDB 还是会返回写操作响应的 大于 1 的任意整数 MongoDB 会确保该操作顺利传播到指定数量的 Replica Set 成员上再返回。如果设定的值大于 Replica Set 的总成员数，MongoDB 会继续等待那些“不存在”的成员接收到该操作，这意味着驱动可能会一直阻塞。此时我们应搭配使用 wtimeout 参数 &quot;majority&quot; MongoDB 确保该操作顺利传播到大多数 Replica Set 数据成员上后再返回 &lt;tag set&gt; MongoDB 确定该操作顺利传播到属于给定 Tag Set 的 Replica Set 成员上后再返回。关于 Tag Set，详见这里。 j 参数j 参数决定了 MongoDB 是否要在将该操作成功写入到日志后再返回。j 参数只有两种值：false 和 true。当设定 j: true 时，驱动将会等待 MongoDB 把该写操作记录到日志上以后再返回。 通常情况下，j: true 意味着最高级别的 Write Concern —— 操作一旦记录到日志上以后就意味着该改动已被安全保存，即使因断电导致内存数据丢失，MongoDB 实例在重新启动时也可根据日志恢复该操作。不过，对于 Replica Set 的 Primary 而言，如果该操作在传播到其它 Replica Set 成员的 oplog 之前，该 Primary 就降级了的话，在该结点重新上线时还是会回滚该操作，导致该操作丢失。因此，对于 Replica Set 而言，最安全的做法是 {w: &quot;majority&quot;, j: true}。 wtimeout 参数wtimeout 参数只在 w 参数被设定为大于 1 的值时才会生效，它用于为 Write Concern 设定以毫秒为单位的超时时间，在时限到达后直接抛出错误。 如果你设定的 w 参数值大于 Replica Set 当前成员数且没有设置 wtimeout，驱动将会一直阻塞。设定 wtimeout: 0 会使 MongoDB 忽略 wtimeout 参数。 关于 Write Concern 的更多内容，请看这里和这里。 Replica Set 自动恢复 MongoDB Replica Set 的高可用性体现在其所提供的自动恢复功能（Automatic Failover）上。当 Replica Set 的 Primary 因各种原因而不可用时，Replica Set 的自动恢复特性使得某个 Secondary 可以升级为 Primary 而替代原本的已经不可用的 Primary。这个过程是完全自动的，不需要任何人的手动干预即可自然发生。 Replica Set 的自动恢复特性包含两个方面： Secondary 会在发现 Primary 不可用时进行选举，获得最高票数的 Secondary 成为新的 Primary 有些情况下，自动恢复的过程可能还需要进行一次回滚 接下来我们逐个看一下这两种不同的恢复动作。 Replica Set 选举 Replica Set 会使用选举的形式来决定谁成为 Primary。这个过程在启动 Replica Set 时就会发生一次。在 Replica Set 运行的过程中，每当其他 Secondary 达成共识认为 Primary 已经不可用时，它们就会发起一次选举，得票数最高的 Secondary 成为新的 Primary。 尽管这样的功能十分 fancy，但选举的过程仍然是需要时间的。在进行选举时，整个 Replica Set 没有 Primary，因此整个 Replica Set 也无法接受任何写操作，所有的结点都会是只读的。因此，MongoDB 会尽可能地避免进行选举。 除了上述两种情况，当 Primary 降级时，Replica Set 也会进行一次选举。总的来说，Primary 在如下情况下会降级： 接收到 replSetStepDown 命令 某个 Secondary 被选举为新的 Primary 而且它有更高的优先级 Primary 无法连接到 Replica Set 中的其他大多数成员（Primary 会得出自己已经断网了的结论） Replica Set 通过让每个成员相互之间发送心跳信号来判断某个成员是否已经变得不可用。因此，成员所处的网络拓补结构将会影响选举的结果。在选取网络拓补结构时，我们应确保在某个 Primary 节点不可用时，剩下的节点确实能够选举出新的 Primary，因为 Secondary 成为新的 Primary 除了需要得到最高的票数，还需要其所得票数达到集群的总票数的大多数。如果有相当一部分成员同时不可用，Replica Set 将始终无法收集到足够多的票数，那么 Replica Set 将无法选举出新的 Primary。由此，我们应该将 Replica Set 的大多数投票成员以及所有可以成为 Primary 的成员放置在与应用程序系统相同的设施中，使得 Replica Set 不至于因为网络隔离而无法收集到大多数成员的选票。 更多有关选举的内容，详见这里。 回滚 Replica Set 的 Secondary 在发现 Primary 已经不可用前是需要花费一些时间的：每个成员每隔 2 秒会发送一次心跳信号，如果该心跳信号在 10 秒内没有收到某个节点的响应，则这个节点被标记为“不可达”；当大多数 Secondary 都将 Primary标记为“不可达”时，它们才会达成共识认为 Primary 已经从 Replica Set 不可达。而原本的 Primary如果只是从大多数节点不可达但不是因为断电或死机之类的原因导致其不可用的话，它在意识到自己已经不可达并主动降级为 Secondary前也需要一定的时间。 然而网络环境是复杂的。有些时候，Replica Set 成员间的不可达并不代表这个结点是客户端所不可达的。在 Replica Set 的 Secondary选举出新的 Primary 以及原有的 Primary 主动降级之前，只要客户端仍能连接上原本的 Primary ，那么客户端就会认为一切正常，依旧将写操作发往这个原有的 Primary，即使由于该 Primary 已与大多数 Secondary 无法连接，这些写操作很有可能永远不会被同步到其他 Secondary 上。 在选举的过程中，Secondary 们会倾向于将票投给拥有最“新”的备份数据（上一次备份时间最近）的 Secondary。但尽管如此，原本的 Primary 仍然可能已经执行了其他所有 Secondary 都没有同步的数据修改。在原本的 Primary 降级为 Secondary，或者说它以 Secondary 的身份重新加入 Replica Set 时，这些数据修改是仍然会保留在它的本地数据里的。 这个时候，为了让它与新的 Primary 进行同步，它会根据自己的 oplog 对本地数据进行回滚，撤销那些未同步的数据修改，然后再从新的 Primary 上同步新的数据修改。 当回滚发生后，被回滚的数据会以 BSON文件的形式保存在 dbPath目录下的 rollback/ 目录中，文件名格式为 &lt;database&gt;.&lt;collection&gt;.&lt;timestamp&gt;.bson。管理员可以使用 bsondump 工具查看具体内容并自行决定如何处理。 由于 MongoDB 的这项设置，我们需要确保 Replica Set 中的成员全都有着大致相同的性能。由于网络不可达而导致回滚是一回事，但如果 Secondary 由于性能限制无法跟上 Primary 的节拍，Secondary 的备份数据则会始终落后于 Primary，也就导致 Primary 在降级时需要回滚更多的操作了。 为保证部分关键的写操作数据不因 Primary 不可用而导致被回滚，客户端可在请求时使用 {w: &quot;majority&quot;} Write Concern来确保该写操作能顺利传递到其他 Replica Set 成员上。 由此我们也可以认识到，同一个 Replica Set 中的所有成员都始终经受着大致相同的读写压力，企图通过将读操作分布在不同的 Replica Set成员上来提高吞吐量并不是最好的做法，更好的做法应该是使用 Sharding。 更多有关回滚的内容，详见这里。 Arbiter 正如我之前所说，一旦涉及到分布式，网络环境可能出现的情况是繁杂多样的，我们很难考虑全面。考虑这么一种情况：你的 Replica Set 中有两个服务器，突然 Primary 不可用了。实际上在这个时候，你的 Replica Set 无法进行自动恢复，因为整个 Replica Set 中只剩下一个可用的 Secondary，它无法获得“大多数”的选票，因为它只能获得它自己的那一票，而整个 Replica Set 总共应有两张票，50% 可不是“大多数”。 最直观的解决方案，便是为 Replica Set 引入一名新的成员。在成员数为 3 的情况下，任意一名成员不可用都不会导致 Replica Set 无法自动恢复，因为新的 Primary 会在选举时获得 2/3 的票数，正好属于“大多数”。 但这个解决方案有个不好的地方。我们引入一个新的节点只是为了打破原有结构的选票平局，我们真正需要的是它的选票。但加入 Replica Set成为 Secondary 意味着它也要备份 Primary 的数据，经受和 Primary 相当的写压力，也就要求它的性能起码能与 Primary 相当了。这么想来，性价比还是比较低的。MongoDB 为此提供的解决方案，便是 Arbiter（仲裁者）。 Arbiter 也是 Replica Set 的成员，但它不是 Secondary：Arbiter 会在 Replica Set 发起选举时参与投票，但它不会像 Secondary 那样备份 Primary 的数据，而这正是我们想要的：我们想要的就是 Arbiter 的投票能力。没有了备份 Primary 数据所带来的读写压力，Arbiter 的性能要求实际上会很低很低，我们完全可以用一台小型机来支撑起一个担任 Arbiter 的 mongod。 更多关于 Arbiter 的内容，详见这里。 结语关于 MongoDB 高可用解决方案之一的 Replica Set，其分布式备份的功能固然使其需要考虑更多的情况。本文只是对 Replica Set 相关概念的一个 Intro，想要了解更多详细内容还需读者自行查阅 MongoDB 的官方文档。 Anyway, I did what I could. Hopefully you can enjoy it.","link":"/mongodb_replica_set/"},{"title":"Spark Core 源码解析：RDD","text":"我的上一个系列的 Spark 源码解析已经完结了一段时间了。当时我出于实习工作的需要阅读了 SparkSQL 以及 Spark REPL 的源代码，并顺势写下了那个系列的源码解析文章。但读 Spark 源代码怎么能只读那些外围插件的源代码呢？于是我又开一个新坑了。 要理解 Spark 的中心思想，首先当然得从 Spark Core 开始。Spark Core 中包含了所有 Spark 的核心类的定义，其中就有我们用得最多的 SparkContext 和 RDD。在开始阅读本文之前，我希望各位可以先完整阅读[这篇](/file/RDDs.pdf”&gt;这篇论文以及&lt;a href=”/file/Spark.pdf)论文。这两篇论文的撰写者相同，均属 UC Berkeley 大学，虽然我不确定他们是不是，但我想他们应该就是 Spark 的创始人了。前一篇论文在第二节详细介绍了 RDD 的概念，并在第五节详细介绍了 Spark 的一些实现原理。后一篇论文在内容上并不如前一篇论文充分，而且有大量的重复内容，但其中也包含了一些新内容，值得大家学习一下。源代码中会出现很多奇怪的名词，恐怕你必须通过完整阅读这两篇论文才能够理解。我不会在文中重复解释这些术语的确切意思，因此我希望你能静下心来读完这两篇论文再继续往下看。我相信这样的阅读是完全值得的。 首先这个系列的出发点其实就有两个，一个是 SparkContext，另一个就是 RDD。我有思考过哪个更好，最终我选择了 RDD，因为它的实现更简单，与 Spark 其他类的依赖也少得多。在我们完整阅读了 RDD 的源代码后，想必阅读 SparkContext 的源代码也会变得轻松很多。但这并不代表在这篇文章中不会出现 SparkContext 的代码。这篇文章将涵盖与 RDD 功能实现有关的代码，至于这些代码来自于哪个类并不重要。 那我们开始吧。 RDD在开始跳进去看 RDD 的方法之前，我们应该先了解一下 RDD 的一些基本信息。首先，我们先来看看 RDD 的构造方法： 123456789101112131415abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) { // This is a warning instead of an exception in order to avoid breaking user programs that // might have defined nested RDDs without running jobs with them. logWarning(&quot;Spark does not support nested RDDs (see SPARK-5063)&quot;) } /** Construct an RDD with just a one-to-one dependency on one parent */ def this(@transient oneParent: RDD[_]) = this(oneParent.context , List(new OneToOneDependency(oneParent)))} 这里我们看到，RDD 在创建时便会放入一个 SparkContext 和它的 Dependency 们。关于 Dependency 类，在上面的论文中有介绍，它包含了当前 RDD 的父 RDD 的引用，以及足够从父 RDD 恢复丢失的 partition 的信息。 接下来我们看看 RDD 需要子类实现的虚函数： 1234567891011121314// 由子类实现来计算一个给定的 Partitiondef compute(split: Partition, context: TaskContext): Iterator[T]// 由子类实现，返回这个 RDD 的 Partition 集合protected def getPartitions: Array[Partition]// 由子类实现，返回这个 RDD 的 Dependency 集合protected def getDependencies: Seq[Dependency[_]] = deps// 可由子类重载，以提供更加偏好的 Partition 放置策略protected def getPreferredLocations(split: Partition): Seq[String] = Nil// 可由子类重载来改变 partition 的方式@transient val partitioner: Option[Partitioner] = None 这些函数基本都是用于执行 Spark 计算的方法，也包括了论文中提到的三大 RDD 接口中的两个，即 getPartitions 以及 getPreferredLocations。其中有两个函数是子类必须实现的，即 compute 和 getPartitions。我们记住它们的功能定义，以免它们在子类中再次出现时一时想不起来它们的功能。 继续往下，我们看到除了包含 SparkContext 变量和 Dependency 们，一个 RDD 还包含了自己的 id 以及 name： 1234567891011121314// 创建该 RDD 的 SparkContextdef sparkContext: SparkContext = sc// SparkContext 内部的唯一 IDval id: Int = sc.newRddId()// RDD 的名字@transient var name: String = null// 给 RDD 一个新的名字def setName(_name: String): this.type = { name = _name this} 再继续往下，便是 RDD 的公用 API 了。 RDD ActionRDD 提供了大量的 API 供我们使用。通过浏览 RDD 的ScalaDoc，不难发现 RDD 拥有数十种 public 的接口，更不要提那些我们即将面对的非 public 的接口了。因此直接跳进 RDD.scala 从上往下阅读源代码是不科学的。这里我使用另外一种阅读方式。 正如 Spark 的论文中所描述的，RDD 的 API 并不是每一个都会启动 Spark 的计算。被称之为 Transformation 的操作可以用一个 RDD 产生另一个 RDD，但这样的操作实际上是 lazy 的：它们并不会被立即计算，而是当你真正触发了计算动作的时候，所有你提交过的 Transformation 们会在经过 Spark 优化以后再顺序执行。那么怎么样的操作会触发 Spark 的计算呢？ 这些被称之为 Action 的 RDD 操作便会触发 Spark 的计算动作。根据上图，Action 包括 count、collect、reduce、lookup 和 save（已被更名为 saveAsTextFile 和 saveAsObjectFile）。不难发现，除了 save，其他四个操作都是将结果直接获取到 driver 程序中的操作，由这些操作来启动 Spark 的计算也是十分合理的。 那么我们不妨先来看一下这几个函数的源代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// RDD.scaladef collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray) Array.concat(results: _*)}// 返回 RDD 的元素个数def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum// 使用给定的二元运算符来 reduce 该 RDDdef reduce(f: (T, T) =&gt; T): T = withScope { // Clean 一下用户传入的 closure，以准备将其序列化 val cleanF = sc.clean(f) // 应用在每个 partition 上的 reduce 函数。相当于 Hadoop MR 中的 combine val reducePartition: Iterator[T] =&gt; Option[T] = iter =&gt; { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) // 在单个 Partition 内部使用 Iterator#reduceLeft 来计算结果 } else { None } } var jobResult: Option[T] = None // 合并每个 partition 的 reduce 结果 val mergeResult = (index: Int, taskResult: Option[T]) =&gt; { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) =&gt; Some(f(value, taskResult.get)) case None =&gt; taskResult } } } // 启动 Spark Job sc.runJob(this, reducePartition, mergeResult) jobResult.getOrElse(throw new UnsupportedOperationException(&quot;empty collection&quot;))}// PairRDDFunctions.scala// 根据给定的 RDD 的 key 来查找它对应的 Seq[value]// 如果该 RDD 有给定的 Partitioner，该方法会先利用 getPartition 方法定位 Partition 再进行搜索，// 如此一来便能提高效率 def lookup(key: K): Seq[V] = self.withScope { self.partitioner match { case Some(p) =&gt; // 存在特定的 Partitioner val index = p.getPartition(key) // 定位具体的 Partition val process = (it: Iterator[(K, V)]) =&gt; { val buf = new ArrayBuffer[V] for (pair &lt;- it if pair._1 == key) { buf += pair._2 } buf } : Seq[V] // 仅在该 Partition 上查找 val res = self.context.runJob(self, process, Array(index), false) res(0) case None =&gt; // 若找不到特定的 Partitioner，则使用 RDD#filter 来查找 self.filter(_._1 == key).map(_._2).collect() }} 上述四个函数都有一个特点：它们都直接或间接地调用了 sparkContext.runJob 方法来获取结果。可见这个方法便是启动 Spark 计算任务的入口。我们记下这个入口，留到研读 SparkContext 源代码的时候再进行解析。 RDD Transformations讲完了 Action，自然就轮到了 Transformation 了。可是有那~么多的 Transformation 啊。我们就一个一个地看看这些常用的 Transformation 吧。 map我们先从用得最多的开始。我们直接看源码： 1234567/** * Return a new RDD by applying a function to all elements of this RDD. */def map[U: ClassTag](f: T =&gt; U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))} 和论文中说的一样，map 函数会利用当前 RDD 以及用户传入的匿名函数构建出一个 MapPartitionsRDD。毋庸置疑这个东西肯定是继承自 RDD 类的。我们可以看看它的源代码： 12345678910111213private[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag]( prev: RDD[T], f: (TaskContext, Int, Iterator[T]) =&gt; Iterator[U], // (TaskContext, partition index, iterator) preservesPartitioning: Boolean = false) extends RDD[U](prev) { override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None override def getPartitions: Array[Partition] = firstParent[T].partitions override def compute(split: Partition, context: TaskContext): Iterator[U] = f(context, split.index, firstParent[T].iterator(split, context))} 可以看到，MapPartitionsRDD 实现了 getPartitions 和 compute 方法。 getPartitions 方法直接返回了它的 firstParent 的 partition。实际上 MapPartitionsRDD 也只会有一个 parent，也就是构造函数传入的 prev。 compute 方法在这里直接应用了构造参数传入的方法 f。我们看回 RDD#map，传入的方法是 (context, pid, iter) =&gt; iter.map(cleanF)。结合到 MapPartitionsRDD 的源代码里就不难看出其实现原理了。这里我们最好记住匿名函数的 context 是 TaskContext 、 pid 是 Partition 的 id、iter 即该 Partition 的 iterator。记住这些以免后面再次出现的时候一时晕菜。 注意到，MapPartitionsRDD 还重载了 partitioner 变量，其值取决于构造函数传入的 preservesPartitioning 参数，该参数默认为 false。在 RDD#map 方法里并未对该参数赋值。 withScope我们回到刚才的 RDD#map 方法，注意到它还调用了一个函数，就是 withScope。这个函数出现的次数相当多，你在很多 RDD API 里都能发现它。我们来看看它的源代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135// RDD.scalaprivate[spark] def withScope[U](body: =&gt; U): U = RDDOperationScope.withScope[U](sc)(body)// RDDOperationScope.scala/** * A general, named code block representing an operation that instantiates RDDs. * * All RDDs instantiated in the corresponding code block will store a pointer to this object. * Examples include, but will not be limited to, existing RDD operations, such as textFile, * reduceByKey, and treeAggregate. * * An operation scope may be nested in other scopes. For instance, a SQL query may enclose * scopes associated with the public RDD APIs it uses under the hood. * * There is no particular relationship between an operation scope and a stage or a job. * A scope may live inside one stage (e.g. map) or span across multiple jobs (e.g. take). */@JsonInclude(Include.NON_NULL)@JsonPropertyOrder(Array(&quot;id&quot;, &quot;name&quot;, &quot;parent&quot;))private[spark] class RDDOperationScope( val name: String, val parent: Option[RDDOperationScope] = None, val id: String = RDDOperationScope.nextScopeId().toString) { def toJson: String = { RDDOperationScope.jsonMapper.writeValueAsString(this) } /** * Return a list of scopes that this scope is a part of, including this scope itself. * The result is ordered from the outermost scope (eldest ancestor) to this scope. */ @JsonIgnore def getAllScopes: Seq[RDDOperationScope] = { parent.map(_.getAllScopes).getOrElse(Seq.empty) ++ Seq(this) } override def equals(other: Any): Boolean = { other match { case s: RDDOperationScope =&gt; id == s.id &amp;&amp; name == s.name &amp;&amp; parent == s.parent case _ =&gt; false } } override def toString: String = toJson}/** * A collection of utility methods to construct a hierarchical representation of RDD scopes. * An RDD scope tracks the series of operations that created a given RDD. */private[spark] object RDDOperationScope extends Logging { private val jsonMapper = new ObjectMapper().registerModule(DefaultScalaModule) private val scopeCounter = new AtomicInteger(0) def fromJson(s: String): RDDOperationScope = { jsonMapper.readValue(s, classOf[RDDOperationScope]) } /** Return a globally unique operation scope ID. */ def nextScopeId(): Int = scopeCounter.getAndIncrement /** * Execute the given body such that all RDDs created in this body will have the same scope. * The name of the scope will be the first method name in the stack trace that is not the * same as this method's. * * Note: Return statements are NOT allowed in body. */ private[spark] def withScope[T]( sc: SparkContext, allowNesting: Boolean = false)(body: =&gt; T): T = { val ourMethodName = &quot;withScope&quot; val callerMethodName = Thread.currentThread.getStackTrace() .dropWhile(_.getMethodName != ourMethodName) // 去掉了 withScope 之后的所有函数调用 .find(_.getMethodName != ourMethodName) // 找到调用 withScope 的函数，如 RDD#withScope .map(_.getMethodName) .getOrElse { // Log a warning just in case, but this should almost certainly never happen logWarning(&quot;No valid method name for this RDD operation scope!&quot;) &quot;N/A&quot; } withScope[T](sc, callerMethodName, allowNesting, ignoreParent = false)(body) } /** * Execute the given body such that all RDDs created in this body will have the same scope. * * If nesting is allowed, any subsequent calls to this method in the given body will instantiate * child scopes that are nested within our scope. Otherwise, these calls will take no effect. * * Additionally, the caller of this method may optionally ignore the configurations and scopes * set by the higher level caller. In this case, this method will ignore the parent caller's * intention to disallow nesting, and the new scope instantiated will not have a parent. This * is useful for scoping physical operations in Spark SQL, for instance. * * Note: Return statements are NOT allowed in body. */ private[spark] def withScope[T]( sc: SparkContext, name: String, allowNesting: Boolean, ignoreParent: Boolean)(body: =&gt; T): T = { // Save the old scope to restore it later val scopeKey = SparkContext.RDD_SCOPE_KEY val noOverrideKey = SparkContext.RDD_SCOPE_NO_OVERRIDE_KEY val oldScopeJson = sc.getLocalProperty(scopeKey) val oldScope = Option(oldScopeJson).map(RDDOperationScope.fromJson) val oldNoOverride = sc.getLocalProperty(noOverrideKey) try { if (ignoreParent) { // Ignore all parent settings and scopes and start afresh with our own root scope sc.setLocalProperty(scopeKey, new RDDOperationScope(name).toJson) } else if (sc.getLocalProperty(noOverrideKey) == null) { // Otherwise, set the scope only if the higher level caller allows us to do so sc.setLocalProperty(scopeKey, new RDDOperationScope(name, oldScope).toJson) } // Optionally disallow the child body to override our scope if (!allowNesting) { sc.setLocalProperty(noOverrideKey, &quot;true&quot;) } // 在执行传入的函数前先将一个新的 RDDOperationScope 设定到 sc 中 body } finally { // 执行完毕后再还原 // Remember to restore any state that was modified before exiting sc.setLocalProperty(scopeKey, oldScopeJson) sc.setLocalProperty(noOverrideKey, oldNoOverride) } }} 暂时来讲，withScope 方法所涉及到的环境变量包括 scopeKey 和 noOverrideKey。以我们目前的高度，这两个变量的具体使用应该是不会接触到的，我们不妨留到深入探讨 SparkContext 的时候再仔细研究这两个变量。 filter1234567def filter(f: T =&gt; Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (context, pid, iter) =&gt; iter.filter(cleanF), preservesPartitioning = true)} 可见，filter 本质上也是一种 map。 flatMap1234def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.flatMap(cleanF))} 基本同上。 sample1234567891011121314151617181920/** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be &gt;= 0 * @param seed seed for the random number generator */def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = withScope { require(fraction &gt;= 0.0, &quot;Negative fraction value: &quot; + fraction) if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) }} 可见，sample 方法生成了一个 PartitionwiseSampledRDD，并根据参数的不同分别传入 PoissonSampler 或 BernoulliSampler。从名字上看，这两个 Sampler 自然是对应着泊松分布和贝努利分布，只是两种不同的随机采样器。因此这里我们就不解析这两个采样器了。我们来看一下这个 PartitionwiseSampledRDD： 12345678910111213141516171819202122232425262728293031323334353637383940414243private[spark]class PartitionwiseSampledRDDPartition(val prev: Partition, val seed: Long) extends Partition with Serializable { override val index: Int = prev.index}/** * A RDD sampled from its parent RDD partition-wise. For each partition of the parent RDD, * a user-specified [[org.apache.spark.util.random.RandomSampler]] instance is used to obtain * a random sample of the records in the partition. The random seeds assigned to the samplers * are guaranteed to have different values. * * @param prev RDD to be sampled * @param sampler a random sampler * @param preservesPartitioning whether the sampler preserves the partitioner of the parent RDD * @param seed random seed * @tparam T input RDD item type * @tparam U sampled RDD item type */private[spark] class PartitionwiseSampledRDD[T: ClassTag, U: ClassTag]( prev: RDD[T], sampler: RandomSampler[T, U], @transient preservesPartitioning: Boolean, @transient seed: Long = Utils.random.nextLong) extends RDD[U](prev) { @transient override val partitioner = if (preservesPartitioning) prev.partitioner else None override def getPartitions: Array[Partition] = { val random = new Random(seed) firstParent[T].partitions.map(x =&gt; new PartitionwiseSampledRDDPartition(x, random.nextLong())) } override def getPreferredLocations(split: Partition): Seq[String] = firstParent[T].preferredLocations(split.asInstanceOf[PartitionwiseSampledRDDPartition].prev) override def compute(splitIn: Partition, context: TaskContext): Iterator[U] = { val split = splitIn.asInstanceOf[PartitionwiseSampledRDDPartition] val thisSampler = sampler.clone thisSampler.setSeed(split.seed) thisSampler.sample(firstParent[T].iterator(split.prev, context)) }} 实现逻辑也十分直观：getPartitions 方法表明 PartitionwiseSampledRDD 直接利用它的 parent RDD 的 partition 作为自己的 partition；compute 方法则表明 PartitionwiseSampledRDD 将通过调用 RandomSampler 的 sample 方法来对 Iterator 进行取样。 cartesian1234567/** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`. */def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other)} 使用两个 RDD 构建了一个 CartesianRDD，似乎也十分合理。那我们来看一下这个 CartesianRDD： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071private[spark] class CartesianPartition( idx: Int, @transient rdd1: RDD[_], @transient rdd2: RDD[_], s1Index: Int, s2Index: Int ) extends Partition { var s1 = rdd1.partitions(s1Index) var s2 = rdd2.partitions(s2Index) override val index: Int = idx // 重载了 Serializable 的 writeObject 方法，在任务序列化时更新 s1、s2 @throws(classOf[IOException]) private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException { // Update the reference to parent split at the time of task serialization s1 = rdd1.partitions(s1Index) s2 = rdd2.partitions(s2Index) oos.defaultWriteObject() }}private[spark]class CartesianRDD[T: ClassTag, U: ClassTag]( sc: SparkContext, var rdd1 : RDD[T], var rdd2 : RDD[U]) extends RDD[Pair[T, U]](sc, Nil) with Serializable { val numPartitionsInRdd2 = rdd2.partitions.length // 以 rdd1 与 rdd2 的 partition 来生成自己的 partition override def getPartitions: Array[Partition] = { // create the cross product split val array = new Array[Partition](rdd1.partitions.length * rdd2.partitions.length) for (s1 &lt;- rdd1.partitions; s2 &lt;- rdd2.partitions) { val idx = s1.index * numPartitionsInRdd2 + s2.index array(idx) = new CartesianPartition(idx, rdd1, rdd2, s1.index, s2.index) } array } // preferredLocations 依赖于 rdd1 和 rdd2 的 preferredLocations override def getPreferredLocations(split: Partition): Seq[String] = { val currSplit = split.asInstanceOf[CartesianPartition] (rdd1.preferredLocations(currSplit.s1) ++ rdd2.preferredLocations(currSplit.s2)).distinct } // 直接使用 rdd1 和 rdd2 生成自身结果 override def compute(split: Partition, context: TaskContext): Iterator[(T, U)] = { val currSplit = split.asInstanceOf[CartesianPartition] for (x &lt;- rdd1.iterator(currSplit.s1, context); y &lt;- rdd2.iterator(currSplit.s2, context)) yield (x, y) } // 指明自己依赖于 rdd1 和 rdd2 override def getDependencies: Seq[Dependency[_]] = List( new NarrowDependency(rdd1) { def getParents(id: Int): Seq[Int] = List(id / numPartitionsInRdd2) }, new NarrowDependency(rdd2) { def getParents(id: Int): Seq[Int] = List(id % numPartitionsInRdd2) } ) override def clearDependencies() { super.clearDependencies() rdd1 = null rdd2 = null }} 也比较直观。 distinct123456/** * Return a new RDD containing the distinct elements in this RDD. */def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)} 使用了 reduceByKey 的功能实现了 distinct，可以理解。 groupBy123456789101112131415def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy[K](f, defaultPartitioner(this))}def groupBy[K](f: T =&gt; K, numPartitions: Int) (implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy(f, new HashPartitioner(numPartitions))}def groupBy[K](f: T =&gt; K, p: Partitioner) (implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope { val cleanF = sc.clean(f) // 利用传入的 f 为每个记录生成 key 以后再 groupByKey this.map(t =&gt; (cleanF(t), t)).groupByKey(p)} 总结至此，我们便基本能够理解了：RDD Transformation 将以原本的 RDD 作为 parent 来构造一个新的 RDD，不断地调用 Transformation Operation 就可以产生出一条 RDD 操作链，但整条流水线的启动被一直延后到 RDD Action；RDD Action 通过调用 SparkContext#runJob 启动整条流水线。","link":"/spark_core_source_1/"},{"title":"Spark Catalyst 源码解析：LogicalPlan","text":"在上一篇文章中，我们了解了 SparkSQL 如何将各式语句分别委派到三个不同的 Parser 中进行解析，并返回一个 Unresolved Logical Plan。 在这篇文章中，我打算在讲解 Analyzer 之前先为大家讲解一下 Spark 里的 LogicalPlan 数据结构。 TreeNode在进入 Analyzer 的学习前，我们不妨先花点时间了解一下这个 LogicalPlan 是一个怎么样的数据结构，为何 Spark 可以在产生一个这样的实例后还能进行如此多的优化操作。 实际上，有学习过数据库原理，或者有看过我之前说的这篇论文，也基本能猜到，LogicalPlan 这个类本质上是一棵抽象语法树（AST）。我们先来看看核心类 LogicalPlan： 12345678910// LogicalPlan 本身是一个虚类，父类是 QueryPlanabstract class LogicalPlan extends QueryPlan[LogicalPlan] with Logging { // LogicalPlan 通过自身类型规定子类必须混入 Product 特质 self: Product =&gt; override protected def statePrefix = if (!resolved) &quot;'&quot; else super.statePrefix // ... } LogicalPlan 继承自 QueryPlan，但实际上 LogicalPlan 只定义了一个带有 override 关键字的方法。那么我们先不着急看 LogicalPlan，我们先去看看它的父类 QueryPlan： 123456789101112131415abstract class QueryPlan[PlanType &lt;: TreeNode[PlanType]] extends TreeNode[PlanType] { self: PlanType with Product =&gt; // ... /** * A prefix string used when printing the plan. * * We use &quot;!&quot; to indicate an invalid plan, and &quot;'&quot; to indicate an unresolved plan. */ protected def statePrefix = if (missingInput.nonEmpty &amp;&amp; children.nonEmpty) &quot;!&quot; else &quot;&quot; override def simpleString: String = statePrefix + super.simpleString } 在 QueryPlan 中，关键字 override 同样只出现了一次。我们看到之前在 LogicalPlan 出现的 statePrefix 函数，是一个和计算过程本身没啥关系的函数，我们先跳过它。我们注意到 QueryPlan 继承自 TreeNode，同时其泛型的类型参数十分有意思，而且考虑到 LogicalPlan 本身继承的父类是 QueryPlan[LogicalPlan]。这是个很有意思的类型设定，我们不妨在看过 TreeNode 以后再来仔细推敲这个问题： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071abstract class TreeNode[BaseType &lt;: TreeNode[BaseType]] { self: BaseType with Product =&gt; val origin: Origin = CurrentOrigin.get /** Returns a Seq of the children of this node */ def children: Seq[BaseType] // 选择不去重载 Object.equals 方法，以免 Scala 编译器不为 case class 生成该方法 def fastEquals(other: TreeNode[_]): Boolean = { this.eq(other) || this == other } // 从本节点开始，先序遍历整棵树，返回第一个符合 f 命题的节点 def find(f: BaseType =&gt; Boolean): Option[BaseType] = f(this) match { case true =&gt; Some(this) case false =&gt; children.foldLeft(None: Option[BaseType]) { (l, r) =&gt; l.orElse(r.find(f)) } } // 以下函数名字中的的 Up 和 Down，可以理解为先序遍历的自上而下（down）和后序遍历的自下而上（up） // 先序地对整棵树遍历使用 f 函数 def foreach(f: BaseType =&gt; Unit): Unit = { f(this) children.foreach(_.foreach(f)) } // 后序地对整棵树遍历使用 f 函数 def foreachUp(f: BaseType =&gt; Unit): Unit = { children.foreach(_.foreachUp(f)) f(this) } // 先序地对整棵树遍历使用 f 函数并以 Seq 的形式返回结果 def map[A](f: BaseType =&gt; A): Seq[A] = { val ret = new collection.mutable.ArrayBuffer[A]() foreach(ret += f(_)) ret } def flatMap[A](f: BaseType =&gt; TraversableOnce[A]): Seq[A] = // ... // map 的偏函数版 def collect[B](pf: PartialFunction[BaseType, B]): Seq[B] = // ... // 先序遍历整棵树调用传入的偏函数，并返回第一个结果 def collectFirst[B](pf: PartialFunction[BaseType, B]): Option[B] = // ... // 返回该节点的拷贝，该拷贝的所有子节点已被应用 f 函数 def mapChildren(f: BaseType =&gt; BaseType): this.type = // ... // 返回该节点的拷贝，该拷贝的子节点为传入的子节点 // 注：传入的子节点数必须与原本的节点数相同 def withNewChildren(newChildren: Seq[BaseType]): this.type = // ... // 对整棵树先序地遍历使用传入的规则（rule）并返回结果树的根节点 // 当某个节点无法被应用于该规则时，该节点保持不变。 def transform(rule: PartialFunction[BaseType, BaseType]): BaseType = // ... // 同上 def transformDown(rule: PartialFunction[BaseType, BaseType]): BaseType = // ... // 从当前节点的子节点开始先序地遍历使用传入的规则（当前节点不会应用该规则），并返回结果树的根节点 def transformChildrenDown(rule: PartialFunction[BaseType, BaseType]): this.type = // ... // 对整棵树后序地遍历使用传入的规则（rule）并返回结果树的根节点 def transformUp(rule: PartialFunction[BaseType, BaseType]): BaseType = // ... // 从当前节点的子节点开始后序地遍历使用传入的规则（当前节点不会应用该规则），并返回结果树的根节点 def transformChildrenUp(rule: PartialFunction[BaseType, BaseType]): this.type = // ... // ... } 我们可以看到，除去一些比较无关痛痒的函数以外（上述源代码已忽略这些函数），TreeNode 类包含的都是一些对整棵树操作的接口。这种设计其实并不难理解。TreeNode 作为虚类，它并没有实现自己的 children 函数。但实际上同样在 TreeNode.scala 文件里，我们可以找到下述三个特质： 123456789101112131415trait BinaryNode[BaseType &lt;: TreeNode[BaseType]] { def left: BaseType def right: BaseType def children: Seq[BaseType] = Seq(left, right)}trait LeafNode[BaseType &lt;: TreeNode[BaseType]] { def children: Seq[BaseType] = Nil}trait UnaryNode[BaseType &lt;: TreeNode[BaseType]] { def child: BaseType def children: Seq[BaseType] = child :: Nil} 这里就定义了一棵树中的所有节点类型，包括有两个子节点的二元节点 BinaryNode 、只有一个子节点的一元节点 UnaryNode 以及没有子节点的叶子节点 LeafNode。每个节点特质都实现了 TreeNode 中的 children 函数。 现在我们似乎还不能直接解答 QueryPlan 奇怪的泛型类型是怎么回事，但我们可以先看看 TreeNode 的泛型类型。TreeNode[BaseType &lt;: TreeNode[BaseType]]，这个 &lt;: TreeNode 好像有点死循环的意思。但其实看到 children 的类型是 Seq[BaseType] 就能理解，这里的 BaseType 指的是当前结点的子结点类型，它必然也应该是一个 TreeNode。 QueryPlan我们回到 QueryPlan： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 这里我们就可以理解，TreeNode 指的是用于构建一般的树的结点，它比起 QueryPlan 更加的 general；而 QueryPlan 则专门指一棵执行计划树// 执行计划树仅仅要求根结点属于 QueryPlan 类，并不要求子结点们都是该类的子类，因此 QueryPlan 的泛型参数仅要求为 TreeNode 的子类abstract class QueryPlan[PlanType &lt;: TreeNode[PlanType]] extends TreeNode[PlanType] { self: PlanType with Product =&gt; // 返回该查询计划包含的所有表达式（Expression） def expressions: Seq[Expression] = { productIterator.flatMap { case e: Expression =&gt; e :: Nil case Some(e: Expression) =&gt; e :: Nil case seq: Traversable[_] =&gt; seq.flatMap { case e: Expression =&gt; e :: Nil case other =&gt; Nil } case other =&gt; Nil }.toSeq } // 先序地对当前结点的 Expression 遍历使用传入的规则 def transformExpressions(rule: PartialFunction[Expression, Expression]): this.type = // ... // 同上 def transformExpressionsDown(rule: PartialFunction[Expression, Expression]): this.type = // ... // 你懂的 def transformExpressionsUp(rule: PartialFunction[Expression, Expression]): this.type = // ... // 对整棵树先序地遍历使用 transformExpressions def transformAllExpressions(rule: PartialFunction[Expression, Expression]): this.type = { transform { case q: QueryPlan[_] =&gt; q.transformExpressions(rule).asInstanceOf[PlanType] }.asInstanceOf[this.type] } // 结果的模式。用过 SparkSQL 的读者应该对 StructType 不会陌生 lazy val schema: StructType = StructType.fromAttributes(output) // 结果表的所有属性（Attribute，特指表中的一个字段） def output: Seq[Attribute] def outputSet: AttributeSet = AttributeSet(output) // 出现在当前结点表达式中的属性 def references: AttributeSet = AttributeSet(expressions.flatMap(_.references)) // 通过子结点输入到当前结点的所有属性 def inputSet: AttributeSet = AttributeSet(children.flatMap(_.asInstanceOf[QueryPlan[PlanType]].output)) // 上述两个集合的差，缺失的输入 def missingInput: AttributeSet = (references -- inputSet).filter(_.name != VirtualColumn.groupingIdName)} 通过阅读上述代码，我们发现了 3 个新名词：schema 、 Attribute 和 Expression。schema 的类型是 StructType，使用过 SparkSQL 的读者就会明白，这个指的就是一个表的模式。Attribute 这个词曾经出现在我之前说过的那篇论文中，指的是表中的一个字段。而 Expression 意为表达式，从 QueryPlan 中的方法可以看出这个 Expression 也是一棵树。我们可以去看看它的源代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657abstract class Expression extends TreeNode[Expression] { self: Product =&gt; /** The narrowest possible type that is produced when this expression is evaluated. */ type EvaluatedType &lt;: Any // 树状结构的 fold 指收缩（展开是 expand） // 这里即指该表达式树是否可以直接变为某个常量，比如像 1 = 1 这样不需要计算也知道结果的表达式就属于 foldable 的 def foldable: Boolean = false // 确定性：对于相同的输入，该表达式是否总能返回相同的结果 def deterministic: Boolean = true // 虚函数，暂不明具体是否指&quot;结果是否可 null&quot; def nullable: Boolean // 表达式中引用的所有 Attribute def references: AttributeSet = AttributeSet(children.flatMap(_.references.iterator)) // 在给定的行数据上应用该 Expression 并返回结果 def eval(input: Row = null): EvaluatedType lazy val resolved: Boolean = childrenResolved def childrenResolved: Boolean = children.forall(_.resolved) // 结果的数据类型 def dataType: DataType // 格式比较漂亮的 toString def prettyString: String = // ... // Returns true when two expressions will always compute the same result def semanticEquals(other: Expression): Boolean = // ...}// 二元 Expression 结点abstract class BinaryExpression extends Expression with trees.BinaryNode[Expression] { self: Product =&gt; def symbol: String override def foldable: Boolean = left.foldable &amp;&amp; right.foldable override def nullable: Boolean = left.nullable || right.nullable override def toString: String = s&quot;($left $symbol $right)&quot;}// 一元 Expression 结点abstract class UnaryExpression extends Expression with trees.UnaryNode[Expression] { self: Product =&gt;}// 叶子 Expression 结点abstract class LeafExpression extends Expression with trees.LeafNode[Expression] { self: Product =&gt;} 在学习过 TreeNode 以后，这个类就显得很好懂了。UML 类图变成了这样： 为了有助于吸收，我们可以把 QueryPlan 理解为单个执行计划，其中包括唯一的一个 SELECT 或 CREATE 等关键字。这类关键字在一条 SQL 语句中可以多次出现，因此 SparkSQL 把我们输入的语句解析为多个 QueryPlan，并以树状结构把它们组织起来，方便优化以及分清他们执行的先后顺序。在这里，QueryPlan 这棵树并不是那篇论文中提到的抽象语法树。每个查询计划对应着一句表达式，这些表达式从我们输入的 SQL 语句中拆分出来，也就是 Expression 树。一句表达式的词素被 Parser 以树状结构组织，这棵树才是那篇论文中提到的抽象语法树。不信的话，你可以在项目中找到 Literal 类（用于表示 SQL 语句中的一个常量词素），它继承自 LeafExpression。 除了 Expression，QueryPlan 还出现了 Attribute 类。也许你和我一开始一样会认为这个类的角色相当于一个 bean，实则不然。我们先来看看它的源代码： 1234567891011121314abstract class Attribute extends NamedExpression { self: Product =&gt; override def references: AttributeSet = AttributeSet(this) override def toAttribute: Attribute = this def withNullability(newNullability: Boolean): Attribute def withQualifiers(newQualifiers: Seq[String]): Attribute def withName(newName: String): Attribute def newInstance(): Attribute} 可以看到该类继承自 NamedExpression，从命名上看也能猜出这个类继承自 Expression。Attribute 类重载了 Expression 的 references 函数使其指向自身，可见 Spark 认为出现在语句中的 Attribute 本身也应该属于一个 Expression，因为在之前的分析中我们就知道，Expression 这个类不仅仅用来表达一句表达式，还用来表达表达式中的一个词素，因此这样的设计也是合情合理的。 我们再来看看 NamedExpression： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546object NamedExpression { private val curId = new java.util.concurrent.atomic.AtomicLong() def newExprId: ExprId = ExprId(curId.getAndIncrement()) def unapply(expr: NamedExpression): Option[(String, DataType)] = Some(expr.name, expr.dataType)}/** * A globally unique (within this JVM) id for a given named expression. * * Used to identify which attribute output by a relation is being * referenced in a subsequent computation. */case class ExprId(id: Long)abstract class NamedExpression extends Expression { self: Product =&gt; def name: String def exprId: ExprId /** * All possible qualifiers for the expression. * * For now, since we do not allow using original table name to qualify a column name once the * table is aliased, this can only be: * * 1. Empty Seq: when an attribute doesn't have a qualifier, * e.g. top level attributes aliased in the SELECT clause, or column from a LocalRelation. * 2. Single element: either the table name or the alias name of the table. */ def qualifiers: Seq[String] /** * Returns a dot separated fully qualified name for this attribute. Given that there can be * multiple qualifiers, it is possible that there are other possible way to refer to this * attribute. */ def qualifiedName: String = (qualifiers.headOption.toSeq :+ name).mkString(&quot;.&quot;) def toAttribute: Attribute /** Returns the metadata when an expression is a reference to another expression with metadata. */ def metadata: Metadata = Metadata.empty protected def typeSuffix = // ...} NamedExpression 相对于 Expression 做出的扩展并不多，仅仅是加上了 name 、exprId 、 qualifiers 、 metadata 字段以及相关方法。 至此，QueryPlan 就全部解析完了，让我们再次回到 LogicalPlan。 LogicalPlan我们回到梦开始的地方： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113/** * 执行计划的代价估计。默认叶子节点的代价为 1，非叶子节点的代价为各子结点代价的乘积。 * 不同类型的执行计划通过重载其 statistics 函数来改变代价计算方式。 */private[sql] case class Statistics(sizeInBytes: BigInt)abstract class LogicalPlan extends QueryPlan[LogicalPlan] with Logging { self: Product =&gt; // 计算该执行计划的 Statistics。默认结果为子结点的 Statistics 乘积。叶子节点不支持该函数 def statistics: Statistics = // ... def childrenResolved: Boolean = !children.exists(!_.resolved) lazy val resolved: Boolean = !expressions.exists(!_.resolved) &amp;&amp; childrenResolved override protected def statePrefix = if (!resolved) &quot;'&quot; else super.statePrefix // 当给定 LogicalPlan 与当前计划返回相同结果时返回 true。当无法直接决定是否返回相同结果时将返回 false def sameResult(plan: LogicalPlan): Boolean = // ... def resolveChildren( nameParts: Seq[String], resolver: Resolver, throwErrors: Boolean = false): Option[NamedExpression] = resolve(nameParts, children.flatMap(_.output), resolver, throwErrors) // 注：Resolver 实际上是一个用 type 关键字设定的类型别名，原本是一个(String, String) =&gt; Boolean // 根据设置的不同，Resolver 只可能是两个字符串之间的 equalsIgnoreCase 或者 equals def resolve( nameParts: Seq[String], resolver: Resolver, throwErrors: Boolean = false): Option[NamedExpression] = resolve(nameParts, output, resolver, throwErrors) def resolveQuoted( name: String, resolver: Resolver): Option[NamedExpression] = resolve(parseAttributeName(name), resolver, true) // 把传入的 name 按‘.’分开。由一对'`'包裹的字符串不会被拆开 // `[scope].AttributeName.[nested].[fields]...` // &quot;a.`b.c`.d&quot; -&gt; [&quot;a&quot;, &quot;b.c&quot;, &quot;d&quot;] private def parseAttributeName(name: String): Seq[String] = // ... // 根据传入的 nameParts 和 attribute，可能返回一个(Attribute, [nested fields])对 // 结果对的[nested] fields 实际上是 nameParts 的[2, ...]，因为第一个元素是 table name，第二个是 AttributeName，实际的 field 名从第三个开始 // 当传入的 nameParts 就是个 table.column 时，结果对的_2 就是个 Nil private def resolveAsTableColumn( nameParts: Seq[String], resolver: Resolver, attribute: Attribute): Option[(Attribute, List[String])] = // ... /** Performs attribute resolution given a name and a sequence of possible attributes. */ protected def resolve( nameParts: Seq[String], input: Seq[Attribute], resolver: Resolver, throwErrors: Boolean): Option[NamedExpression] = { // 根据传入的 nameParts 以及输入的所有 Attribute，产生所有吻合的[Attribute, [field]]对 var candidates: Seq[(Attribute, List[String])] = { // 假设传入的 nameParts 格式为 table.column if (nameParts.length &gt; 1) { input.flatMap { attr =&gt; resolveAsTableColumn(nameParts, resolver, attr) } } else { Seq.empty } } // 如果没有匹配，就假设 nameParts 只包含 column 名 if (candidates.isEmpty) { candidates = input.flatMap { candidate =&gt; resolveAsColumn(nameParts, resolver, candidate) } } def name = UnresolvedAttribute(nameParts).name candidates.distinct match { // 只有一个匹配，没有 nested field，直接返回 case Seq((a, Nil)) =&gt; Some(a) // 只有一个匹配，但有 nested fields，对其进行解压 case Seq((a, nestedFields)) =&gt; try { // The foldLeft adds GetFields for every remaining parts of the identifier, // and aliases it with the last part of the identifier. // For example, consider &quot;a.b.c&quot;, where &quot;a&quot; is resolved to an existing attribute. // Then this will add GetField(&quot;c&quot;, GetField(&quot;b&quot;, a)), and alias // the final expression as &quot;c&quot;. val fieldExprs = nestedFields.foldLeft(a: Expression)((expr, fieldName) =&gt; ExtractValue(expr, Literal(fieldName), resolver)) val aliasName = nestedFields.last Some(Alias(fieldExprs, aliasName)()) } catch { case a: AnalysisException if !throwErrors =&gt; None } // 没有匹配 case Seq() =&gt; logTrace(s&quot;Could not find $name in ${input.mkString(&quot;, &quot;)}&quot;) None // 多个匹配 case ambiguousReferences =&gt; val referenceNames = ambiguousReferences.map(_._1).mkString(&quot;, &quot;) throw new AnalysisException( s&quot;Reference '$name' is ambiguous, could be: $referenceNames.&quot;) } }} 可见，LogicalPlan 比起 QueryPlan 扩展了 resolve 相关的操作，还加上了一个 statistics 变量。该变量实际上就是一个 BigInt，代表计划的执行代价，猜想在后续的执行计划优化过程中将会使用这个变量。 实现类我们由浅到深地研究了三个核心类：LogicalPlan 、 QueryPlan 和 TreeNode，也学习了它们周边的一些核心类，如 Expression 、 Attribute 等。但以上的这些类都有一个特点：它们都是虚类，我们至今没有见到一个 concrete 的类。同时，早在 TreeNode 就已经有使用 productIterator 等 Product 特质的方法，但直到 LogicalPlan 都仍然把混入 Product 特质的工作交给子类，我们仍然不知道 Product 的元素究竟意味着什么。 现在我们就先来看看 LogicalPlan 的子类。实际上就在 LogicalPlan.scala 中我们就能看到三个 LogicalPlan 的子类： 1234567891011abstract class LeafNode extends LogicalPlan with trees.LeafNode[LogicalPlan] { self: Product =&gt;}abstract class UnaryNode extends LogicalPlan with trees.UnaryNode[LogicalPlan] { self: Product =&gt;}abstract class BinaryNode extends LogicalPlan with trees.BinaryNode[LogicalPlan] { self: Product =&gt;} 但暂时来讲并没有什么用，这三个类也是虚类，依然没有混入 Product 特质，甚至什么方法都没有实现。我们随便抓一个他们的子类： 123case class Intersect(left: LogicalPlan, right: LogicalPlan) extends BinaryNode { override def output: Seq[Attribute] = left.output} 这下就真相大白了。所有的这些虚类的实现类都是 Scala 的 case class，而 Scala 的样例类都会自动实现 Product 特质，并以 case class 的数据成员作为 Product 的元素。现在你再回到之前的三个核心类中去看那些调用了 productIterator 的方法你就能理解了。 总结在这篇文章中，我们学习了以 LogicalPlan 类为核心的执行计划树数据结构。下一次我们将开始讲解 Analyzer 的相关代码，敬请期待。","link":"/sparksql_catalyst_source_3/"},{"title":"Spark Catalyst 进阶：CacheManager","text":"在上一篇文章中，我们详细讲解了 SparkSQL 如何一步一步地将用户输入的 SQL 语句变为 LogicalPlan 再变为 PhysicalPlan。至此，这个流程本身的内容已经全部讲完了，因此接下来的文章我们将脱离这个主要流程，去讲解 SparkSQL 的其他常用功能。 在今天的这篇文章中，我们先从 SparkSQL 的 DataFrame Cache 机制开始讲起。 CacheManager在我之前推荐的那篇论文中实际上有稍微提到 SparkSQL 的缓存机制。我们都知道 RDD 可以以 Partition 为单位进行缓存，对于一些经常需要大量计算但计算结果基本不变且经常需要查询的数据，我们就会考虑使用 RDD 的缓存机制。SparkSQL 中也是同理。平日的数据库访问中我们经常需要访问一些由两张表 Join 得到的数据。这些数据查询频次高、计算复杂度高，但计算的结果在短时间内是基本不变的。为了做到实时性，对于这样的 DataFrame 我们就可以考虑使用 DataFrame 的 Cache 机制。 通常，我们通过调用 DataFrame 的 cache 方法或 persist 方法来对其进行缓存。实际上这两个操作是完全相同的。我们来看一下它们的源代码： 1234567891011121314151617181920212223class DataFrame private[sql]( @transient val sqlContext: SQLContext, @DeveloperApi @transient val queryExecution: SQLContext#QueryExecution) extends RDDApi[Row] with Serializable { // ... override def cache(): this.type = persist() // 由此可见，这两个接口是完全相同的 // 调用了 SQLContext 的 cacheManager 来完成 Cache 动作 override def persist(): this.type = { sqlContext.cacheManager.cacheQuery(this) this } // 除此之外，persist 接口还允许用户传入不同的存储级别。可用于 DataFrame 的存储级别与 RDD 的完全相同 override def persist(newLevel: StorageLevel): this.type = { sqlContext.cacheManager.cacheQuery(this, None, newLevel) this } // ...} 那我们再去看看 SQLContext 的这个 cacheManager 是什么： 1protected[sql] val cacheManager = new CacheManager(this) // 很好，简单粗暴 由此一来我们就知道这个变量实际上就是个 CacheManager 实例，DataFrame 通过以自己为参数调用它的 cacheQuery 方法来完成缓存动作。那么我们就来看一下 CacheManager： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158/** Holds a cached logical plan and its data */private[sql] case class CachedData(plan: LogicalPlan, cachedRepresentation: InMemoryRelation)// 从命名上看，这应该是个用来表示单张缓存表的 bean 类，其中包含一个表示其所代表的查询的 LogicalPlan。// InMemoryRelation 类尚不明朗，但从名字上看，这应该是个 LogicalPlan + LeafNode 的实现类private[sql] class CacheManager(sqlContext: SQLContext) extends Logging { @transient private val cachedData = new scala.collection.mutable.ArrayBuffer[CachedData] // 通过一个 ArrayBuffer 管理注册到该 manager 中的 DataFrame @transient private val cacheLock = new ReentrantReadWriteLock // 使用一个可重入读写锁来对 Cache 内容进行加锁 /** Returns true if the table is currently cached in-memory. */ def isCached(tableName: String): Boolean = lookupCachedData(sqlContext.table(tableName)).nonEmpty // 检查某张表是否被 cache 到了内存中。这里调用了一个新方法 lookupCachedData /** Caches the specified table in-memory. */ def cacheTable(tableName: String): Unit = cacheQuery(sqlContext.table(tableName), Some(tableName)) // 将某张表 cache 到内存中。这里再次调用了 cacheQuery 方法 /** Removes the specified table from the in-memory cache. */ def uncacheTable(tableName: String): Unit = uncacheQuery(sqlContext.table(tableName)) // 将某张表从内存中移除 /** 为 f 过程赋一个读锁 */ private def readLock[A](f: =&gt; A): A = { val lock = cacheLock.readLock() lock.lock() try f finally { lock.unlock() } } /** 为 f 过程赋一个写锁 */ private def writeLock[A](f: =&gt; A): A = { val lock = cacheLock.writeLock() lock.lock() try f finally { lock.unlock() } } /** 清除所有缓存表。涉及缓存内容修改，因此这里申请了一个写锁 */ private[sql] def clearCache(): Unit = writeLock { // 这里调用了 InMemoryRelation 的 cachedColumnBuffers 变量的 unpersist 方法来从内存中物理地移除缓存 cachedData.foreach(_.cachedRepresentation.cachedColumnBuffers.unpersist()) // 之所以说是物理的，毫无疑问 CachedData 本身只是一些元数据，单纯的 cacheData.clear 是不够的 cachedData.clear() // 当然最后还是得 clear 一下才行 } /** 检查是否有缓存内容。涉及读取缓存内容，申请了一个读锁 */ private[sql] def isEmpty: Boolean = readLock { cachedData.isEmpty } /** * 对传入的 Logical Plan（实际指 DataFrame）进行缓存。这里使用的默认存储级别为 MEMORY_AND_DISK， * 因为计算表的列存储表示的过程代价过高。 * * 涉及缓存写操作，申请了一个写锁 */ private[sql] def cacheQuery( query: DataFrame, tableName: Option[String] = None, storageLevel: StorageLevel = MEMORY_AND_DISK): Unit = writeLock { // 获取到 DataFrame 的 Analyzed Logical Plan val planToCache = query.queryExecution.analyzed // 先看看这个 Plan 是否已经 cache 了 if (lookupCachedData(planToCache).nonEmpty) { logWarning(&quot;Asked to cache already cached data.&quot;) } else { // 没有的话才 cache cachedData += CachedData( planToCache, // CachedData 中保存的是一个 Analyzed Logical Plan InMemoryRelation( sqlContext.conf.useCompression, sqlContext.conf.columnBatchSize, storageLevel, query.queryExecution.executedPlan, // 但 InMemoryRelation 中保存的是一个 Prepared Physical Plan tableName)) } } /** 根据给定的 DataFrame 从缓存中移除数据。申请了一个写锁 */ private[sql] def uncacheQuery(query: DataFrame, blocking: Boolean = true): Unit = writeLock { val planToCache = query.queryExecution.analyzed // 通过调用 LogicalPlan 的 sameResult 方法来在 cachedData 中找到对应位置 val dataIndex = cachedData.indexWhere(cd =&gt; planToCache.sameResult(cd.plan)) require(dataIndex &gt;= 0, s&quot;Table $query is not cached.&quot;) // 物理移除 cachedData(dataIndex).cachedRepresentation.uncache(blocking) // 逻辑移除 cachedData.remove(dataIndex) } /** * 尝试根据给定的 DataFrame 从缓存中移除数据。申请了一个写锁。 * * 该方法与上一个方法的不同在于，上一个方法如果没有在 cachedData 中找到对应的元素会直接抛出一个错误， * 但这个方法不会。 */ private[sql] def tryUncacheQuery( query: DataFrame, blocking: Boolean = true): Boolean = writeLock { val planToCache = query.queryExecution.analyzed val dataIndex = cachedData.indexWhere(cd =&gt; planToCache.sameResult(cd.plan)) val found = dataIndex &gt;= 0 if (found) { cachedData(dataIndex).cachedRepresentation.cachedColumnBuffers.unpersist(blocking) cachedData.remove(dataIndex) } found } /** 使用传入 DataFrame 的 Analyzed Logical Plan 来查找 cachedData */ private[sql] def lookupCachedData(query: DataFrame): Option[CachedData] = readLock { lookupCachedData(query.queryExecution.analyzed) } /** 使用传入的 Analyzed Logical Plan 来查找 cachedData */ private[sql] def lookupCachedData(plan: LogicalPlan): Option[CachedData] = readLock { // 这里同样利用了 LogicalPlan 的 sameResult 方法 cachedData.find(cd =&gt; plan.sameResult(cd.plan)) } /** * 尝试将传入的 LogicalPlan 中吻合的子树替换为缓存内容 * 在 SQLContext#QueryExecution 中，得出 Analyzed Logical Plan 以后， * 会在转换为 PhysicalPlan 之前调用该方法。 */ private[sql] def useCachedData(plan: LogicalPlan): LogicalPlan = { plan transformDown { case currentFragment =&gt; lookupCachedData(currentFragment) .map(_.cachedRepresentation.withOutput(currentFragment.output)) .getOrElse(currentFragment) // 在 cachedData 中找到相同的 Plan，便将其替换为了一个 InMemoryRelation // 这里还调用了 InMemoryRelation 的 withOutput 方法，传入了原本的 LogicalPlan 的 output } } /** * 使包含传入 LogicalPlan 的缓存数据失效 */ private[sql] def invalidateCache(plan: LogicalPlan): Unit = writeLock { cachedData.foreach { // 只要某个 cachedData 包含了该子树，便会调用它的 InMemoryRelation 的 recache 方法 case data if data.plan.collect { case p if p.sameResult(plan) =&gt; p }.nonEmpty =&gt; data.cachedRepresentation.recache() case _ =&gt; } }} 经过一番阅读，我们了解到，SparkSQL 通过对 Analyzed Logical Plan 调用 useCachedData 方法，便会将执行计划树中与某个已缓存数据相吻合的子树替换为一个 InMemoryRelation。我们之前就接触过 Relation，它主要指的是 SQL 中 FROM 关键字指明的表名，所以这里的 InMemoryRelation 也可以理解为直接从内存中 SELECT FROM。在注册缓存时，CacheManager 利用了一些设置参数、表名、DataFrame 的 Physical Plan 来实例化一个 InMemoryRelation。 InMemoryRelation那我们就来看一下这个 InMemoryRelation： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184private[sql] object InMemoryRelation { // CacheManager 就是应用这个方法来创建 InMemoryRelation 实例的 def apply( useCompression: Boolean, batchSize: Int, storageLevel: StorageLevel, child: SparkPlan, tableName: Option[String]): InMemoryRelation = new InMemoryRelation(child.output, useCompression, batchSize, storageLevel, child, tableName)() // 并未对参数进行任何特别的处理，只是把一个 child.output 提取出来又传了进去 }// 暂不清楚这是什么，但它包含了一个 Array[Array[Byte]]，这个很有可能就是缓存数据保存在内存中的形式private[sql] case class CachedBatch(buffers: Array[Array[Byte]], stats: Row)private[sql] case class InMemoryRelation( output: Seq[Attribute], useCompression: Boolean, batchSize: Int, storageLevel: StorageLevel, child: SparkPlan, tableName: Option[String])( // 注意这里有个 CachedBatch 的 RDD，这个应该就是指这张表的缓存数据 private var _cachedColumnBuffers: RDD[CachedBatch] = null, private var _statistics: Statistics = null, private var _batchStats: Accumulable[ArrayBuffer[Row], Row] = null) extends LogicalPlan with MultiInstanceRelation { // 果然 InMemoryRelation 继承自 LogicalPlan，但这个 MultiInstanceRelation 倒是个新名词 private val batchStats: Accumulable[ArrayBuffer[Row], Row] = if (_batchStats == null) { child.sqlContext.sparkContext.accumulableCollection(ArrayBuffer.empty[Row]) } else { _batchStats } // 暂不清楚是什么 val partitionStatistics = new PartitionStatistics(output) // 计算缓存数据的大小 private def computeSizeInBytes = { val sizeOfRow: Expression = // 需要先了解一下 BindReferences 是什么 BindReferences.bindReference( output.map(a =&gt; partitionStatistics.forAttribute(a).sizeInBytes).reduce(Add), partitionStatistics.schema) batchStats.value.map(row =&gt; sizeOfRow.eval(row).asInstanceOf[Long]).sum } // 传播用的 statistics private def statisticsToBePropagated = if (_statistics == null) { val updatedStats = statistics if (_statistics == null) null else updatedStats } else { _statistics } // 重载了 Statistics 逻辑（原本的默认实现是左子 * 右子） override def statistics: Statistics = { if (_statistics == null) { if (batchStats.value.isEmpty) { // Underlying columnar RDD hasn't been materialized, no useful statistics information // available, return the default statistics. Statistics(sizeInBytes = child.sqlContext.conf.defaultSizeInBytes) } else { // Underlying columnar RDD has been materialized, required information has also been // collected via the `batchStats` accumulator, compute the final statistics, // and update `_statistics`. _statistics = Statistics(sizeInBytes = computeSizeInBytes) _statistics } } else { // Pre-computed statistics _statistics } } // If the cached column buffers were not passed in, we calculate them in the constructor. // As in Spark, the actual work of caching is lazy. if (_cachedColumnBuffers == null) { // 构建缓存 buildBuffers() } // 重新缓存 def recache(): Unit = { // 清空了缓存 _cachedColumnBuffers.unpersist() _cachedColumnBuffers = null // 建立缓存 buildBuffers() } // 建立缓存 private def buildBuffers(): Unit = { // 注意：child 是传进来的那个 DataFrame 的 Physical Plan val output = child.output // 执行 val cached = child.execute().mapPartitions { rowIterator =&gt; // 为每一个 Partition 都生成了一个 Iterator，想必之后会利用这些 Iterator 来访问缓存数据 new Iterator[CachedBatch] { // 这里我们就了解到，CachedBatch 表示的是一个 Partition 的缓存 def next(): CachedBatch = { // 这里对每个 Attribute 都生成了一个 ColumnBuilder // 考虑到 SparkSQL 的缓存是以列存储的形式组织的，那么下一步大概就是要利用这些 ColumnBuilder 构建缓存了 val columnBuilders = output.map { attribute =&gt; val columnType = ColumnType(attribute.dataType) val initialBufferSize = columnType.defaultSize * batchSize // 这里看到 ColumnBuilder 本身包含的信息只是一些元数据 ColumnBuilder(attribute.dataType, initialBufferSize, attribute.name, useCompression) }.toArray var rowCount = 0 // 遍历整个 Partition while (rowIterator.hasNext &amp;&amp; rowCount &lt; batchSize) { val row = rowIterator.next() // ... // 将该行的数据放入到各自的 ColumnBuilder 中 var i = 0 while (i &lt; row.length) { columnBuilders(i).appendFrom(row, i) i += 1 } rowCount += 1 } // 不知道在干什么 val stats = Row.merge(columnBuilders.map(_.columnStats.collectedStatistics) : _*) batchStats += stats // 返回了该 Partition 的缓存数据 CachedBatch(columnBuilders.map(_.build().array()), stats) } def hasNext: Boolean = rowIterator.hasNext } }.persist(storageLevel) // 将整个 RDD 缓存。注意：这个动作是 lazy 的 cached.setName(tableName.map(n =&gt; s&quot;In-memory table $n&quot;).getOrElse(child.toString)) _cachedColumnBuffers = cached } // 利用传入的 output 新建一个实例 def withOutput(newOutput: Seq[Attribute]): InMemoryRelation = { InMemoryRelation( newOutput, useCompression, batchSize, storageLevel, child, tableName)( _cachedColumnBuffers, statisticsToBePropagated, batchStats) } // 无 children，说到底它还是一个叶子节点 override def children: Seq[LogicalPlan] = Seq.empty // 拷贝构造函数 override def newInstance(): this.type = { new InMemoryRelation( output.map(_.newInstance()), useCompression, batchSize, storageLevel, child, tableName)( _cachedColumnBuffers, statisticsToBePropagated, batchStats).asInstanceOf[this.type] } def cachedColumnBuffers: RDD[CachedBatch] = _cachedColumnBuffers override protected def otherCopyArgs: Seq[AnyRef] = Seq(_cachedColumnBuffers, statisticsToBePropagated, batchStats) // 移除缓存 private[sql] def uncache(blocking: Boolean): Unit = { // 不清楚在干啥 Accumulators.remove(batchStats.id) // 移除了缓存数据 cachedColumnBuffers.unpersist(blocking) _cachedColumnBuffers = null }} 目前来讲，我们已经能看懂大部分的代码。其中出现了一个 ColumnBuilder，正是用来构建列缓存的类。那我们去看看这个 ColumnBuilder： 1234567891011121314// 完蛋了，这居然只是个接口private[sql] trait ColumnBuilder { // 初始化 def initialize(initialSize: Int, columnName: String = &quot;&quot;, useCompression: Boolean = false) // 将该行指定的元素放入到 ColumnBuilder def appendFrom(row: Row, ordinal: Int) // Statistics Information def columnStats: ColumnStats // 返回最终的列缓存 def build(): ByteBuffer} 我们先不着急看它的实现类，我们先去看看它的实例化方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152private[sql] object ColumnBuilder { // 默认的初始缓存大小，1MB val DEFAULT_INITIAL_BUFFER_SIZE = 1024 * 1024 // 保证空余空间。这里我们就可以看出来，ByteBuffer 就是最底层的缓存数据容器了 private[columnar] def ensureFreeSpace(orig: ByteBuffer, size: Int) = { // 有足够的空闲空间，则不需要做任何操作 if (orig.remaining &gt;= size) { orig } else { // 空闲空间不足，尝试扩充 ByteBuffer // grow in steps of initial size val capacity = orig.capacity() val newSize = capacity + size.max(capacity / 8 + 1) val pos = orig.position() // 新建一个更大的 ByteBuffer 并放入原 ByteBuffer 的数据 ByteBuffer .allocate(newSize) .order(ByteOrder.nativeOrder()) .put(orig.array(), 0, pos) } } // InMemoryRelation 就是通过这个方法实例化 ColumnBuilder 的 def apply( dataType: DataType, initialSize: Int = 0, columnName: String = &quot;&quot;, useCompression: Boolean = false): ColumnBuilder = { // 如此看来，ColumnBuilder 是根据传入的数据类型来实例化不同的子类 val builder: ColumnBuilder = dataType match { case IntegerType =&gt; new IntColumnBuilder case LongType =&gt; new LongColumnBuilder case FloatType =&gt; new FloatColumnBuilder case DoubleType =&gt; new DoubleColumnBuilder case BooleanType =&gt; new BooleanColumnBuilder case ByteType =&gt; new ByteColumnBuilder case ShortType =&gt; new ShortColumnBuilder case StringType =&gt; new StringColumnBuilder case BinaryType =&gt; new BinaryColumnBuilder case DateType =&gt; new DateColumnBuilder case TimestampType =&gt; new TimestampColumnBuilder case DecimalType.Fixed(precision, scale) if precision &lt; 19 =&gt; new FixedDecimalColumnBuilder(precision, scale) case _ =&gt; new GenericColumnBuilder } // 初始化后便实例化完毕 builder.initialize(initialSize, columnName, useCompression) builder }} 由此看来，ColumnBuilder 的工作是构建一个列缓存，但列缓存本身由一个 ByteBuffer 表示，所以 build 方法返回的是一个 ByteBuffer。ByteBuffer 实际上是一个抽象类，它来自 java.nio 包。通过调用 ByteBuffer 的静态方法来获取其子类实例可以让外部调用者不去在意其底部的内存分配方式。 实际上，ColumnBuilder 的子类们有着极为复杂的继承关系。画成类图大致如下： 由此看来，我们最好不要再往下深究。 除了 ColumnBuilder，我们还需要关注出现在 InMemoryRelation 中的 PartitionStatistics。我们来看看它的代码： 123456789101112131415161718192021222324252627// 实例化时，InMemoryRelation 会把 Physical Plan 的 output 作为参数传入private[sql] class PartitionStatistics(tableSchema: Seq[Attribute]) extends Serializable { // 这里同时设定了它的两个变量 val (forAttribute, schema) = { // 这里形成了一个从 Attribute 到它的 ColumnStatisticsSchema 实例的映射 val allStats = tableSchema.map(a =&gt; a -&gt; new ColumnStatisticsSchema(a)) (AttributeMap(allStats), allStats.map(_._2.schema).foldLeft(Seq.empty[Attribute])(_ ++ _)) // 这里前者是一个从 Attribute.exprId 到 ColumnStatisticsSchema 的映射 // 后者则是 ColumnStatisticsSchema 们的 schema 变量的首尾相连 }}// 实际上 ColumnStatisticsSchema 的定义就在这个类的上面private[sql] class ColumnStatisticsSchema(a: Attribute) extends Serializable { // AttributeReference 是 Attribute 的一个实现类，是一个 case class val upperBound = AttributeReference(a.name + &quot;.upperBound&quot;, a.dataType, nullable = true)() val lowerBound = AttributeReference(a.name + &quot;.lowerBound&quot;, a.dataType, nullable = true)() val nullCount = AttributeReference(a.name + &quot;.nullCount&quot;, IntegerType, nullable = false)() val count = AttributeReference(a.name + &quot;.count&quot;, IntegerType, nullable = false)() val sizeInBytes = AttributeReference(a.name + &quot;.sizeInBytes&quot;, LongType, nullable = false)() // 这里看到对于每个传入的 Attribute，生成的 schema 实际上就是这样 5 个元素组成的 Seq // 从上面可以看到，这其中的信息包括了 Attribute 的名字、类型、上下界、是否可为 null、大小，以及一个不知道指代什么的 count val schema = Seq(lowerBound, upperBound, nullCount, count, sizeInBytes) // 该类的名字叫 ColumnStatisticsSchema，从它把一个 Attribute 拆成 5 个 Attribute 的行为来看，它确实是一个 Schema， // 而这五个元素应该就是这个 Column 的 Statistics 了} 这里只能看出，PartitionStatistics 倒是做了个很奇怪的工作，而且 InMemoryRelation 再没用到过它。也许后面会有什么地方用到它。 至此，我们就知道，在实例化 InMemoryRelation 的时候就已经完成了 RDD.persist 的动作，但我们也要知道 RDD 的缓存本身是 lazy 的，即使调用了这个 persist 方法，真正的缓存动作是还没有执行的。 接下来我们开始看看 SparkSQL 会如何获取这些缓存数据。 InMemoryColumnarTableScan之前我们学习到，Optimized Logical Plan 由 SparkPlanner 转变为 Physical Plan，而 SparkPlanner 所应用的转换策略都位于 SparkStrategies 中。那么我们就去看一下： 12345678910111213// 经搜索发现，InMemoryRelation 仅在此处出现过object InMemoryScans extends Strategy { def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match { // 这里对传入的 plan 调用了 PhysicalOperation 的 unapply 方法 case PhysicalOperation(projectList, filters, mem: InMemoryRelation) =&gt; pruneFilterProject( projectList, filters, identity[Seq[Expression]], // All filters still need to be evaluated. InMemoryColumnarTableScan(_, filters, mem)) :: Nil case _ =&gt; Nil }} 那我们先去看看这个 PhysicalOperation： 123456789101112131415161718192021222324252627282930313233object PhysicalOperation extends PredicateHelper { type ReturnType = (Seq[NamedExpression], Seq[Expression], LogicalPlan) def unapply(plan: LogicalPlan): Option[ReturnType] = { val (fields, filters, child, _) = collectProjectsAndFilters(plan) // 从之前 InMemoryScans 的代码可知，InMemoryRelation 指的是这里的 child，也就是 collectProjectsAndFilters 的第三个结果 Some((fields.getOrElse(child.output), filters, child)) } // 我们只考虑传入的 LogicalPlan 是个 InMemoryRelation 的情况 def collectProjectsAndFilters(plan: LogicalPlan): (Option[Seq[NamedExpression]], Seq[Expression], LogicalPlan, Map[Attribute, Expression]) = plan match { // Project 是个 case class，InMemoryRelation 不会进入这个分支 case Project(fields, child) =&gt; val (_, filters, other, aliases) = collectProjectsAndFilters(child) val substitutedFields = fields.map(substitute(aliases)).asInstanceOf[Seq[NamedExpression]] (Some(substitutedFields), filters, other, collectAliases(substitutedFields)) // Filter 同样是个 case class case Filter(condition, child) =&gt; val (fields, filters, other, aliases) = collectProjectsAndFilters(child) val substitutedCondition = substitute(aliases)(condition) (fields, filters ++ splitConjunctivePredicates(substitutedCondition), other, aliases) // 毫无疑问，InMemoryRelation 会进入这个分支，作为第三个结果被原封不动地返回，同时前两个结果都是空 case other =&gt; (None, Nil, other, Map.empty) } // ...} 那么回到刚才的 Strategy： 12345678910111213object InMemoryScans extends Strategy { def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match { case PhysicalOperation(projectList, filters, mem: InMemoryRelation) =&gt; // 也就是说到了这里，projectList 和 filters 都是空 pruneFilterProject( projectList, filters, identity[Seq[Expression]], // All filters still need to be evaluated. InMemoryColumnarTableScan(_, filters, mem)) :: Nil // 这里构建了一个 InMemoryColumnarTableScan 实例 case _ =&gt; Nil }} 这下好像找到点眉头了。那么我们来看一下这个 InMemoryColumnarTableScan： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104private[sql] case class InMemoryColumnarTableScan( attributes: Seq[Attribute], predicates: Seq[Expression], relation: InMemoryRelation) extends LeafNode { // 它通过 LeafNode 继承自 SparkPlan，由此可以确定这个类正是 InMemoryRelation 对应的 Physical Plan override def output: Seq[Attribute] = attributes // 这里再次用到了 InMemoRelation 那个很奇怪的变量 // 这个变量的 forAttribute 是一个基于 Attribute.exprId 的从 Attribute 到 ColumnStatisticsSchema 的映射 private def statsFor(a: Attribute) = relation.partitionStatistics.forAttribute(a) // ... // SparkPlan 的入口方法 protected override def doExecute(): RDD[Row] = { // ... // 这个 cachedColumnBuffers 就是之前 InMemoryRelation 构建好的 RDD[CachedBatch] relation.cachedColumnBuffers.mapPartitions { cachedBatchIterator =&gt; // ... // 找出需要的列的索引以及数据类型 val (requestedColumnIndices, requestedColumnDataTypes) = if (attributes.isEmpty) { // 未传入任何属性，返回默认体积最小的列 val (narrowestOrdinal, narrowestDataType) = relation.output.zipWithIndex.map { case (a, ordinal) =&gt; // Index -&gt; DataType ordinal -&gt; a.dataType } minBy { case (_, dataType) =&gt; ColumnType(dataType).defaultSize } Seq(narrowestOrdinal) -&gt; Seq(narrowestDataType) } else { // 否则，根据传入的 exprId 找到对应的 Index attributes.map { a =&gt; relation.output.indexWhere(_.exprId == a.exprId) -&gt; a.dataType }.unzip } val nextRow = new SpecificMutableRow(requestedColumnDataTypes) // 将 CachedBatch 转换为 Row def cachedBatchesToRows(cacheBatches: Iterator[CachedBatch]): Iterator[Row] = { val rows = cacheBatches.flatMap { cachedBatch =&gt; // 创建 ColumnAccessor 读取缓存数据 val columnAccessors = requestedColumnIndices.map { batchColumnIndex =&gt; ColumnAccessor( relation.output(batchColumnIndex).dataType, ByteBuffer.wrap(cachedBatch.buffers(batchColumnIndex))) } // 通过 ColumnAccessor 将数据解压至 Row new Iterator[Row] { private[this] val rowLen = nextRow.length override def next(): Row = { var i = 0 while (i &lt; rowLen) { columnAccessors(i).extractTo(nextRow, i) i += 1 } if (attributes.isEmpty) Row.empty else nextRow } override def hasNext: Boolean = columnAccessors(0).hasNext } } if (rows.hasNext &amp;&amp; enableAccumulators) { readPartitions += 1 } rows } // 需要扫描的 CachedBatch val cachedBatchesToScan = // 该参数默认为 false if (inMemoryPartitionPruningEnabled) { cachedBatchIterator.filter { cachedBatch =&gt; if (!partitionFilter(cachedBatch.stats)) { def statsString: String = relation.partitionStatistics.schema .zip(cachedBatch.stats.toSeq) .map { case (a, s) =&gt; s&quot;${a.name}: $s&quot; } .mkString(&quot;, &quot;) logInfo(s&quot;Skipping partition based on stats $statsString&quot;) false } else { if (enableAccumulators) { readBatches += 1 } true } } } else { // 默认扫描所有 CachedBatch cachedBatchIterator } cachedBatchesToRows(cachedBatchesToScan) } }} 至此其实我们就全部理解了。","link":"/sparksql_catalyst_source_7/"},{"title":"SparkSQL Hive ThriftServer 源码解析：Intro","text":"本人的第一个实习工作是在一家小公司做研发工作。这家公司以 Spark 平台为基础开发出了一款大数据分析平台作为其核心产品。工作性质使然，我需要掌握 Spark 的运行原理，工作更要求我去阅读和理解 Spark 的源代码。这篇博文只是我的一时心血来潮：一来可以巩固我所学的知识，二来也希望我的理解能够帮到后来的人。 首先，必须承认 Spark 本身是一个十分复杂的系统，Scala 作为 Spark 的主要开发语言，其相较于 Java 差得多的可读性也为源代码阅读带来了相当大的挑战。SparkSQL 作为 Spark 的一个模块，也相当的复杂，我并不认为自己有能力在如此短时间的源代码阅读过程中就能够把 SparkSQL 模块琢磨透。因此这篇文章更像是我开的一个坑：我会慢慢地更新这篇文章，不断地修正我对 SparkSQL 本身的理解。同时也希望读者不要过于相信我的一家之言，因为我很有可能是错的。如果您在某些问题上有比我更好的见解，随时欢迎您用电子邮件与我联系进行深入交流。 本文中所出现的源代码皆为写作时最新的 Spark 1.4.1 中的源代码。 SparkSQL 模块综述Spark 的主要开发语言是 Scala，同时包含部分 Java 代码。以模块为单位的话，不去管其他模块，在 Spark 1.4.1 中的 SparkSQL 模块全部由 Scala 编写而成，因此本文要求读者拥有阅读 Scala 源代码的能力。对于并未学习过 Scala 语言的读者，我由衷地建议您在学习过 Scala 后再在此文的指导下阅读 SparkSQL 的源代码。 在利用 IntelliJ 构建完 Spark 源码阅读环境后，打开项目的 sql 文件夹，就会看到有四个文件夹：catalyst 、 core 、 hive 、 hive-thriftserver。这四个文件夹分别属于 SparkSQL 的四个项目：spark-catalyst_2.10 、 spark-sql_2.10 、 spark-hive_2.10 、 spark-hive-thriftserver_2.10。初来乍到很容易被这四个文件夹吓晕，因为这四个文件夹下面各自都是一个 Maven 项目，而光从项目名称上很难看出每个项目到底有什么用途。但不用担心，Apache Spark 长期以来一直都在 Github 上开源，因此 sql 文件夹以及这四个文件夹下都有 README.md 文件对项目进行详细说明。 我们首先看一下 sql/README.md。其中有这么一段话： 123456789101112This module provides support for executing relational queries expressed in either SQL or a LINQ-like Scala DSL.Spark SQL is broken up into four subprojects: - Catalyst (sql/catalyst) - An implementation-agnostic framework for manipulating trees of relational operators and expressions. - Execution (sql/core) - A query planner / execution engine for translating Catalyst’ s logical query plans into Spark RDDs. This component also includes a new public interface, SQLContext, that allows users to execute SQL or LINQ statements against existing RDDs and Parquet files. - Hive Support (sql/hive) - Includes an extension of SQLContext called HiveContext that allows users to write queries using a subset of HiveQL and access data from a Hive Metastore using Hive SerDes. There are also wrappers that allows users to run queries that include Hive UDFs, UDAFs, and UDTFs. - HiveServer and CLI support (sql/hive-thriftserver) - Includes support for the SQL CLI (bin/spark-sql) and a HiveServer2 (for JDBC/ODBC) compatible server. （如果你不喜欢这个样式，还可以在这里看到由 Github 渲染过的 Readme 说明） 对于有一定英文基础的人来讲，上述说明并不难理解，但在这里我会再详细解释一下，可能会有点啰嗦。 熟悉 SparkSQL 的人都知道，SparkSQL 接受用户输入的 SQL 语句，并将其解析为对应的 Spark 操作，执行计算后返回结果。SparkSQL 的存在让大量的 DBA 找到存在感，同时也大大加快了各大企业开发 Spark 应用的速度，原因在于用 Java、Python、Scala 或 R 写出来的 Spark 应用脚本的专用性过强，业务逻辑中的每个运算都需要程序员明确写明步骤，而且这样的运算脚本复用性极差，基本上完全无法复用。这样的脚本也被部分开发者称为“一次性的 Spark 脚本”。毋庸置疑，开发这样的脚本，效率是极低的。SparkSQL 模块接收程序员输入的 SQL 语句并自动转化为对应的 Spark 运算。突然之间，程序员们从维护一次性脚本变成了维护 SQL 语句，而这正是 DBA 们的专业领域。维护成本大大降低，开发速度大大提高，加之大部分企业以关系型数据库组织自己的业务数据，SparkSQL 可以说为企业的传统数据业务提供了无缝转接。以笔者所在公司为例，该公司核心产品的大部分业务逻辑都是通过 JDBC 发送 SQL 语句至 SparkSQL 模块完成查询的。 说回 SparkSQL 的模块划分。首先最引人注目的，应该是 Hive Support 和 HiveServer 项目。众所周知，Apache Hive 的功能在于将数据文件以表的形式存储在 HDFS 之上。原本的 Hive 与 Hadoop 紧密结合，Hive 通过 JDBC 接收 SQL 语句并将 SQL 解析为 Hadoop 的 MapReduce 操作，处理完毕后返回结果。Spark 本身也可以使用 Hadoop HDFS 存储数据文件，所以 SparkSQL 对 Hive 做出兼容似乎也是合情合理，只是这次 Hive 不再将 SQL 语句变成 MapReduce 操作，而是变成 Spark 操作。其中，hive-thriftserver 项目除了实现了一个 HiveServer2（负责 JDBC/ODBC 连接）还实现了 CLI support 功能，也就是 bin/spark-sql 这个 SQL Shell。而 hive 这个项目，不妨理解为 hive-core，它实现了 SparkSQL 与 Hive 之间的桥梁：HiveContext。它继承自 SQLContext，而且并未改写其中最核心的 sql 方法。 Catalyst，“催化剂”，即 SQL 解析器。它接收 SQL 语句，将其解析为抽象语法树并进一步解析为对应的 Spark 操作树，供执行模块执行。执行模块 Execution，或称 sql core，负责管理和调度接收到的每个查询，同时也是这些查询的执行引擎。在 Catalyst 为每个查询生成了对应的查询计划以后，Execution 便将执行对应操作，将这些查询计划变成结果 RDD（DataFrame）。SparkSQL 核心类 SQLContext 正是位于这个项目之中。 以上便是 SparkSQL 四个模块的介绍。接下来我们先从 Hive ThriftServer 开始。 Start ThriftServer正如上文所述，Spark ThriftServer 项目负责接受 JDBC 连接，将 JDBC 客户端发来的 SQL 语句转发至 SparkSQL，并在 SparkSQL 计算完毕后将结果 DataFrame 以 ResultSet 的形式返回给客户端。ThriftServer 本身基于 Apache Hive 项目进行开发，大量使用了 Hive 本身的代码，仅在转发至 Hadoop MapReduce 的部分通过继承的方式，改而将 SQL 语句转发至 SparkSQL。因此，我们不难将整个 ThriftServer 分成两个模块进行理解： 首先是 Hive Server 模块，角色比较类似于 Tomcat 这样的 Servlet Container。Hive Server 负责监听套接字（0.0.0.0:10000）、在接收到客户端请求后维护与客户端的连接、接收客户端的请求转发至执行模块并将执行模块的结果（可能已经是 ResultSet，也可能仍然是 DataFrame）以 ResultSet 的形式返回。 然后是 Hive Service 模块，角色类似于 Servlet。它包含真正的业务逻辑或对真正的业务逻辑的直接调用。它正是上文提到的 thriftserver 的执行模块，它调用 SparkSQL 的接口（极有可能就是 SQLContext.sql 方法）并将结果返回给 Server 模块。 当然，上述只是对 ThriftServer 模块划分及分工的大概猜测，也有可能并不准确，加之如数据表缓存等与上下文（context）有关的功能也暂时无法确定具体是哪个模块负责。But, after all, talk is cheap, show me the code. 首先，我们从 ThriftServer 的启动入口开始。在 Spark 的 sbin 文件夹下有一个名为 start-thriftserver.sh 的脚本文件，通过执行该脚本便可启动 Thrift Server。我们不妨先看看它的内容： 12345# Usage 打印以及注释等无关语句已被删去CLASS=&quot;org.apache.spark.sql.hive.thriftserver.HiveThriftServer2&quot;exec &quot;$FWDIR&quot;/sbin/spark-daemon.sh submit $CLASS 1 &quot;$@&quot; 可见，该脚本利用 spark-daemon.sh，在后台调用了 spark-submit 接口，执行了 org.apache.spark.sql.hive.thriftserver.HiveThriftServer2。 入口确定，于是我们去找 HiveThriftServer2 的 main 函数吧！ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * The main entry point for the Spark SQL port of HiveServer2. Starts up a `SparkSQLContext` and a * `HiveThriftServer2` thrift server. */object HiveThriftServer2 extends Logging { var LOG = LogFactory.getLog(classOf[HiveServer2]) var uiTab: Option[ThriftServerTab] = _ var listener: HiveThriftServer2Listener = _ // ... def main(args: Array[String]) { // 使用 ServerOptionsProcessor 解析用户启动服务器时输入的参数 val optionsProcessor = new ServerOptionsProcessor(&quot;HiveThriftServer2&quot;) if (!optionsProcessor.process(args)) { System.exit(-1) } // 通过 SparkSQLEnv 初始化 SparkContext 和 HiveContext logInfo(&quot;Starting SparkContext&quot;) SparkSQLEnv.init() // 为 Spark 添加一个关闭时的任务 Utils.addShutdownHook { () =&gt; SparkSQLEnv.stop() // 关闭 SparkSQLEnv uiTab.foreach(_.detach()) } try { // 启动 HiveThriftServer2, 包括一个 SparkSQLCLIService 和一个 ThriftCliService val server = new HiveThriftServer2(SparkSQLEnv.hiveContext) server.init(SparkSQLEnv.hiveContext.hiveconf) // 调用其所有 service 的 init(HiveContext)方法 server.start() // 调用其所有 service 的 start 方法 logInfo(&quot;HiveThriftServer2 started&quot;) // 为启动的 HiveThriftServer2 设置一个 listener listener = new HiveThriftServer2Listener(server, SparkSQLEnv.hiveContext.conf) SparkSQLEnv.sparkContext.addSparkListener(listener) // Web UI 页面 uiTab = if (SparkSQLEnv.sparkContext.getConf.getBoolean(&quot;spark.ui.enabled&quot;, true)) { Some(new ThriftServerTab(SparkSQLEnv.sparkContext)) } else { None } } catch { case e: Exception =&gt; logError(&quot;Error starting HiveThriftServer2&quot;, e) System.exit(-1) } } // ... } 可见，main 函数创建了一个 HiveThriftServer2 实例，传入 HiveContext 与 HiveConf 实例对其进行初始化并启动。于是我们来看看 HiveThriftServer2： 12345678910111213141516171819202122232425262728293031323334353637/** * 继承自 Apache Hive 的 HiveServer2。 * 注意 HiveServer2 里，cliService 和 thriftCLIService 为 private， * 所以该类在初始化时利用反射机制对这两个变量进行设置 */private[hive] class HiveThriftServer2(hiveContext: HiveContext) extends HiveServer2 with ReflectedCompositeService { /** 使用给定的 HiveConf 初始化 HiveThriftServer2 */ override def init(hiveConf: HiveConf) { // 初始化 SparkSqlCliService val sparkSqlCliService = new SparkSQLCLIService(hiveContext) // super.cliService = sparkSqlCliService setSuperField(this, &quot;cliService&quot;, sparkSqlCliService) addService(sparkSqlCliService) // 初始化 Thrift 的 CliService val thriftCliService = if (isHTTPTransportMode(hiveConf)) { new ThriftHttpCLIService(sparkSqlCliService) } else { new ThriftBinaryCLIService(sparkSqlCliService) } // super.thriftCLISerivce = thriftCliService setSuperField(this, &quot;thriftCLIService&quot;, thriftCliService) addService(thriftCliService) // 启动所有 Service (cliService、thriftCLIService) initCompositeService(hiveConf) } private def isHTTPTransportMode(hiveConf: HiveConf): Boolean = { val transportMode: String = hiveConf.getVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE) // 该属性的默认值是 binary transportMode.equalsIgnoreCase(&quot;http&quot;) }} 我们可以拿上述代码对比一下 HiveServer2 原本的代码： 123456789101112131415161718192021222324252627282930public class HiveServer2 extends CompositeService { private static final Log LOG = LogFactory.getLog(HiveServer2.class); private CLIService cliService; private ThriftCLIService thriftCLIService; // ... @Override public synchronized void init(HiveConf hiveConf) { cliService = new CLIService(); addService(cliService); String transportMode = System.getenv(&quot;HIVE_SERVER2_TRANSPORT_MODE&quot;); if(transportMode == null) { transportMode = hiveConf.getVar(HiveConf.ConfVars.HIVE_SERVER2_TRANSPORT_MODE); } if(transportMode != null &amp;&amp; (transportMode.equalsIgnoreCase(&quot;http&quot;))) { thriftCLIService = new ThriftHttpCLIService(cliService); } else { thriftCLIService = new ThriftBinaryCLIService(cliService); } addService(thriftCLIService); super.init(hiveConf); } // ...} 仔细看就会发现，这两个方法仅在处理启动参数和 cliService 变量的设置上有所差别。在 Spark ThriftServer 中，cliService 从原本的 org.apache.hive.service.cli.CLIService 变成了 org.apache.spark.sql.hive.thriftserver。同时 HiveThriftServer2 启动了另一个名为 thriftCLISerivce 的服务，这一点上与原本的 HiveServer2 保持一致。这恰恰证明了我们先前的猜想，这个 thriftCLIService 就代表着 Hive Server 模块，cliService 则代表着 Servlet 模块。 除此之外，为了能够顺利的复用 Hive 的功能，Thrift Server 大量的使用了反射机制。HiveThriftServer2 除了继承自 HiveServer2，还混入了 ReflectedCompositeService 特质，而 HiveServer2 继承自 CompositeService 特质。我们可以看一下 ReflectedCompositeService 特质： 123456789101112131415161718192021/** Reflected，反射 */private[thriftserver] trait ReflectedCompositeService { this: AbstractService =&gt; /** 相当于调用 CompositeService 的 init(HiveConf) */ def initCompositeService(hiveConf: HiveConf) { // 模拟 CompositeService.init(hiveConf) 方法 val serviceList = getAncestorField[JList[Service]](this, 2, &quot;serviceList&quot;) // 获取到 CompositeService 的 serviceList serviceList.foreach(_.init(hiveConf)) // 启动 serviceList 里的所有 serivce // 模拟 AbstractService.init(hiveConf) 方法 // CompositeSerivce 继承自 AbstractService，而 CompositeService.init()的末尾调用了 super.init() invoke(classOf[AbstractService], this, &quot;ensureCurrentState&quot;, classOf[STATE] -&gt; STATE.NOTINITED) // ensureCurrentState(STATE.NOTINITED) setAncestorField(this, 3, &quot;hiveConf&quot;, hiveConf) // this.hiveConf = hiveConf invoke(classOf[AbstractService], this, &quot;changeState&quot;, classOf[STATE] -&gt; STATE.INITED) // changeState(STATE.INITED) getAncestorField[Log](this, 3, &quot;LOG&quot;).info(s&quot;Service: $getName is inited.&quot;) // LOG.info(&quot;Service:&quot; + getName() + &quot; is inited.&quot;) }} 可以对比一下 CompositleService 的源代码 12345678910111213141516171819202122232425262728293031323334353637383940414243public class CompositeService extends AbstractService { // ... private final List&lt;Service&gt; serviceList = new ArrayList&lt;Service&gt;(); // ... protected synchronized void addService(Service service) { serviceList.add(service); } protected synchronized boolean removeService(Service service) { return serviceList.remove(service); } @Override public synchronized void init(HiveConf hiveConf) { for (Service service : serviceList) { service.init(hiveConf); } super.init(hiveConf); } @Override public synchronized void start() { int i = 0; try { for (int n = serviceList.size(); i &lt; n; i++) { Service service = serviceList.get(i); service.start(); } super.start(); } catch (Throwable e) { LOG.error(&quot;Error starting services &quot; + getName(), e); stop(i); throw new ServiceException(&quot;Failed to Start &quot; + getName(), e); } } // ...} 可以看到，CompositeService 维护着一个由 Service 组成的 ArrayList（Composite 意为“复合的”、“混合的”），调用 CompositeService 的 addService 和 removeService 可以向其中添加和删除 Service，而调用 init 和 start 则可以分别调用其中所有 Service 的 init 和 start 方法。稍微对比 ReflectedCompositeService 的代码和 CompositeService 的代码即可得出结论，HiveThriftServer2 中的 initCompositeService(hiveConf) 和 HiveServer2 中的 super.init(hiveConf) 是等价的。 ReflectedCompositeService 是一处。细心的读者还会注意到在 HiveThriftServer2 中还出现了 setSuperField 方法。setSuperField 方法是来自于 org.apache.spark.sql.hive.thriftserver.RefectionUtils 的静态方法。该工具类包含的所有反射工具方法如下： setSuperField(obj : Object, fieldName: String, fieldValue: Object)：将 obj 的直接父类的指定变量置为指定值 setAncestorField(obj: AnyRef, level: Int, fieldName: String, fieldValue: AnyRef)：将 obj 上 level 级的父类的指定变量置为指定值 getSuperField[T](obj: AnyRef, fieldName: String): T：获取 obj 的直接父类的指定变量 getAncestorField[T](obj: Object, level: Int, fieldName: String): T：获取 obj 上 level 级的父类的指定变量 invokeStatic(clazz: Class[_], methodName: String, args: (Class[_], AnyRef)*): AnyRef：调用某个类的静态函数 invoke(clazz: Class[_], obj: AnyRef, methodName: String, args: (Class[_], AnyRef)*): AnyRef：调用某个对象的指定函数 为了能够顺利复用 HiveServer2 的其他方法，HiveThriftServer2 必须设置其父类的 cliService 变量和 thriftCLIService 变量，无奈这两个变量都是 private 的，所以这里才使用了反射机制对其进行设置。包括 ReflectedCompositeService 以及 ReflectionUtils 前 4 个变量相关的方法，希望各位读者能铭记于心。这几个工具方法在整个 Thrift Server 项目中被多次用到。 在 HiveThriftServer2 的 init 方法执行完毕后，Thrift Server 初始化完毕。main 函数接下来便调用了它的 start 方法。start 方法调用其所有通过 addService 注册的服务的 start 方法，服务器正式启动。 总结感谢您能细心读完本文。如果没有意外的话，您应该已对 SparkSQL ThriftServer 的启动流程有了大致的了解。该流程可用如此表示： 同时，您也了解到，ThriftCliService 充当着 Servlet Container 的角色，维护着与客户端的连接，接收客户端的请求、为客户端发送结果，但主要的业务逻辑并不在里面，而是在充当 Servlet 角色的执行模块 SparkSQLCLIService 内。 在接下来的文章中，我将分两个方向，分别讲解这两个模块的工作原理。敬请期待。","link":"/sparksql_hive_thriftserver_source_1/"},{"title":"SQL Reference","text":"My personal reference for the SQL language. Much of the post’s content comes from W3Schools.com. Basic CRUD OperationsSelecting All Columns from TableThe following SQL statement selects all the columns from the Customers table: 1SELECT * FROM Customers; Selecting Designated Columns from TableThe following SQL statement selects the CustomerName and City columns from the Customers table: 1SELECT CustomerName, City FROM Customers; Selecting Distinct Values of Given ColumnsThe following SQL statement returns all the distinct values from the City columns of the Customers table: 1SELECT DISTINCT City FROM Customers; The following SQL statement returns all the distinct combinations of the City and Country columns of the Customers table: 1SELECT DISTINCT City, Country FROM Customers; Using the WHERE clauseUsing WHERE clause can tell the RDBMS to return only records that satisfy the given criteria. The following SQL statement selects all the customers from the country Mexico, in the Customers table: 12SELECT * FROM CustomersWHERE Country='Mexico'; Note: String literals in SQL is represented by single-quoted strings like 'Mexico', while double-quoted string represents quoted identifier, being identifier that contains alpha-numeric characters, $ and #. Quoted identifier can also be specified by strings surrounded by brackets, e.g. [Contact Person]. The SQL AND, OR &amp; NOT OperatorsThe AND operator returns a record if both the first condition AND the second condition are true. The following SQL statement selects all customers from the country Germany AND the city Berlin, in the Customers table: 123SELECT * FROM CustomersWHERE Country='Germany'AND City='Berlin'; The OR operator returns a record if either the first condition OR the second condition is true. The following SQL statement selects all customers from the city Berlin OR München, in the Customers table: 123SELECT * FROM CustomersWHERE City='Berlin'OR City='München'; AND and OR operator can be combined to form complex expressions. It is important to note that AND has higher precedencethan OR, while you can also use parenthesis to group sub-expressions. The following SQL statement selects all customers from the country Germany AND the city must be equal to Berlin OR München,in the Customers table: 123SELECT * FROM CustomersWHERE Country='Germany'AND (City='Berlin' OR City='München'); Using the ORDER BY ClauseThe ORDER BY keyword is used to sort the result-set by one or more columns. The ORDER BY keyword sorts the records in ascending order by default.To sort the records in a descending order, you can use the DESC keyword. The following SQL statement selects all customers from the Customers table, sorted ascending by the Country anddescending by the CustomerName column, while the ASC keyword for Country column is omittable: 12SELECT * FROM CustomersORDER BY Country ASC, CustomerName DESC; Inserting New RecordThe INSERT INTO statement is used to insert new records in a table. The following SQL statement will insert a new row to the Customers table: 12INSERT INTO CustomersVALUES (25, 'Cardinal','Tom B. Erichsen','Skagen 21','Stavanger','4006','Norway'); While the following SQL statement will only insert data in the CustomerName, City, and Country column: 12INSERT INTO Customers (CustomerName, City, Country)VALUES ('Cardinal', 'Stavanger', 'Norway'); Updating RecordsThe UPDATE and SET keyword can be used to update existing records in a table. The following SQL statement will update all records which have the value Mexico in the field Country: 123UPDATE CustomersSET ContactName='Juan'WHERE Country='Mexico'; Note that the WHERE clause of an UPDATE statement is omittable, which causes the statement to update all records in a table. The following SQL statement will update all records in table Customers to have the value Juan in the field ContactName: 12UPDATE CustomersSET ContactName='Juan'; Deleting RecordsThe DELETE statement is used to delete rows in a table. The following SQL statement will delete all records which have the value Mexico in the field Country: 12DELETE FROM CustomersWHERE Country='Mexico'; The WHERE clause of a DELETE statement is also omittable, which causes the statement to delete all records in a table. The following SQL statement will clear the Customers table: 1DELETE FROM Customers; Advanced QueryLimiting Size of Result SetIn many cases, one may wants the DBMS to return designated part of the result set, including use cases like result pagination. In standard SQL, the SELECT TOP clause is used to specify the number of records to return. The following SQL statement selects the two first records from the Customers table: 1SELECT TOP 2 * FROM Customers; By using it in combination of the PERCENT keyword, one can also specify to select the first given percentage of the result set. The following SQL statement selects the first 50% of the records from the Customers table: 1SELECT TOP 50 PERCENT * FROM Customers; Unfortunately, TOP clause is not supported by many RDBMS, while most of them provide different alternatives for the same funtionality. For instance, in MySQL, one can use LIMIT clause to achieve the same result. The following SQL statement selects the two first records from the Customers table in MySQL: 1SELECT * FROM Customers LIMIT 5; Pattern Matching for String FieldUsing LIKE operator in WHERE clause can search for specific pattern in string fields. The following SQL statement selects all customers with a City starting with the letter s: 12SELECT * FROM CustomersWHERE City LIKE 's%'; SQL Wildcard CharactersSQL wildcard characters are typically used to specify search pattern for LIKE operator. The standard SQL includes the following wildcard: WildCard Description % Substitude for zero or more arbitary characters _ Substitude for single arbitary character [charlist] Sets or ranges of characters to match [^charlist] or [!charlist] Matches only a character NOT specified by the set or range within the brackets Searching for Value in Given SetThe IN operator allows you to specify multiple values in a WHERE clause. The following SQL statement selects all customers with a City of Paris or London: 12SELECT * FROM CustomersWHERE City IN ('Paris','London'); It can be used with NOT operator to selects the records whose designated field do not have the given values. The following SQL statement selects all customers whose are not from city of Paris or London: 12SELECT * FROM CustomersWHERE City NOT IN ('Paris','London'); Searching for Value in Given RangeThe BETWEEN operator is used to select values within a range. The following SQL statement selects all products with a price BETWEEN 10 and 20: 12SELECT * FROM ProductsWHERE Price BETWEEN 10 AND 20; It can also be used with NOT to select values which are not within the given range. The following SQL statement selects all products with a price smaller than 10 or bigger than 20: 12SELECT * FROM ProductsWHERE Price NOT BETWEEN 10 AND 20; Note that different result may be returned by different databases, as whether the border values of the given range should be treated exclusively or inclusively is not specified by the SQL standard. Giving Aliase to Table or Column of Result SetThe AS operator is used to give alias to table or column of result set which is only effective within the single SQL statement. The following SQL statement specifies two aliases, one for the CustomerName column and one for the ContactName column. 12SELECT CustomerName AS Customer, ContactName AS &quot;Contact Person&quot;FROM Customers; The following SQL statement selects all the orders issued by the customer with CustomerName of Around the Horn. We use the Customers and Orders tables, and give them the table aliases of c and o respectively: 123SELECT o.OrderID, o.OrderDate, c.CustomerNameFROM Customers AS c, Orders AS oWHERE c.CustomerName=&quot;Around the Horn&quot; AND c.CustomerID=o.CustomerID; Using the JOIN OperatorOne can use the SQL JOIN operator to combine records from different tables by using values common to each. Normally, JOIN queries on more than two tables is constructed by concatenating all the tables with JOIN operators, while the JOIN operator itself is a binary operator and is upper-associative, so we can only consider the cases where are only two tables to join. For simplicity, I will use the terms “left table” and “right table” to refer to the two table operands. There are 4 different types of JOIN operations: Name Description INNER JOIN Returns all rows when there is at least one match in BOTH tables LEFT JOIN Return all rows from the left table, and the matched rows from the right table RIGHT JOIN Return all rows from the right table, and the matched rows from the left table FULL JOIN Return all rows when there is a match in ONE of the tables Inner JoinThe INNER JOIN keyword selects all rows from both tables as long as there is a match between the columns in both tables. The following SQL statement will return all customers with orders, leaving out the customers that do not have orders recorded in the database: 12345SELECT Customers.CustomerName, Orders.OrderIDFROM CustomersINNER JOIN OrdersON Customers.CustomerID=Orders.CustomerIDORDER BY Customers.CustomerName; When not specify, INNER JOIN is the default type of JOIN operations, hence the INNER keyword in the upper code can be omitted and written like this: 12345SELECT Customers.CustomerName, Orders.OrderIDFROM CustomersJOIN OrdersON Customers.CustomerID=Orders.CustomerIDORDER BY Customers.CustomerName; Left JoinThe LEFT JOIN keyword returns all rows from the left table, with the matching rows in the right table. The result is NULL in the right side when there is no match. The following SQL statement will return all customers, and any orders they might have: 12345SELECT Customers.CustomerName, Orders.OrderIDFROM CustomersLEFT JOIN OrdersON Customers.CustomerID=Orders.CustomerIDORDER BY Customers.CustomerName; Note that in some databases, LEFT JOIN is called LEFT OUTER JOIN, hence an OUTER keyword may need to be added to the upper code depending on the database you use. Right JoinSimilar to LEFT JOIN, the RIGHT JOIN keyword returns all rows from the right table, with the matching rows in the left table. The result is NULL in the left side when there is no match. The following SQL statement will return all customers, and any orders they might have: 12345SELECT Orders.OrderId, Customers.CustomerName, Orders.OrderIDFROM OrdersRIGHT JOIN CustomersON Customers.CustomerID=Orders.CustomerIDORDER BY Customers.CustomerName; Full JoinFULL JOIN is also known as FULL OUTER JOIN, which acts as a combination of LEFT OUTER JOIN and RIGHT OUTER JOIN. The FULL OUTER JOIN keyword returns all rows from the left table and from the right table. The FULL OUTER JOIN keyword combines the result of both LEFT and RIGHT joins: it returns all the rows from the left table, and all the rows from the right table. If there are rows in the left table that do not have matches in the right table, or if there are rows in the right table that do not have matches in the left table, those rows will be listed as well. The following SQL statement selects all customers, and all orders: 12345SELECT Customers.CustomerName, Orders.OrderIDFROM CustomersFULL OUTER JOIN OrdersON Customers.CustomerID=Orders.CustomerIDORDER BY Customers.CustomerName; Using the UNION OperatorThe UNION operator is used to combine the result-set of two or more SELECT statements. Notice that each SELECT statement within the UNION must have the same number of columns. The columns must also have similar data types. Also, the columns in each SELECT statement must be in the same order. Also note that the UNION operator selects only distinct values by default. To allow duplicate values, use the ALL keyword with UNION. The following SQL statement selects all the different cities (only distinct values) from the Customers and the Suppliers tables: 1234SELECT City FROM CustomersUNIONSELECT City FROM SuppliersORDER BY City; The following SQL statement uses UNION ALL to select all (duplicate values also) German cities from the Customers and Suppliers tables: 123456SELECT City, Country FROM CustomersWHERE Country='Germany'UNION ALLSELECT City, Country FROM SuppliersWHERE Country='Germany'ORDER BY City; Using the GROUP BY StatementThe GROUP BY statement is used in conjunction with the aggregate functions, such as COUNT and SUM, to group the result-set by one or more columns. The following SQL statement counts as orders grouped by shippers: 12345SELECT Shippers.ShipperName, COUNT(Orders.OrderID) AS NumberOfOrdersFROM OrdersLEFT JOIN ShippersON Orders.ShipperID=Shippers.ShipperIDGROUP BY ShipperName; We can also use the GROUP BY statement on more than one column, like this: 12345SELECT Shippers.ShipperName, Employees.LastName, COUNT(Orders.OrderID) AS NumberOfOrdersFROM OrdersJOIN Shippers ON Orders.ShipperID = Shippers.ShipperIDJOIN Employees ON Orders.EmployeeID = Employees.EmployeeIDGROUP BY ShipperName,LastName; The HAVING ClauseThe HAVING clause was added to SQL because the WHERE keyword could not be used with aggregate functions. 123456SELECT Employees.LastName, COUNT(Orders.OrderID) AS NumberOfOrders FROM OrdersINNER JOIN EmployeesON Orders.EmployeeID=Employees.EmployeeIDWHERE LastName='Davolio' OR LastName='Fuller'GROUP BY LastNameHAVING COUNT(Orders.OrderID) &gt; 25; Using SQL CommentsComments can be used to explain sections of SQL statements. Single line comments start with --, the content between -- and the end of the line will be ignored (will not be executed): 1SELECT * FROM Customers -- WHERE City='Berlin'; Multi-line comments start with /* and end with */, the data between /* and */ will be ignored: 12345/* Select all the columns * of all the records * in the Customers table: */SELECT * FROM Customers; Storing Query Result in other TableUsing the SELECT INTO Statement to Create a New TableThe SELECT INTO statement copies the result of a SELECT query and inserts it into a new table. The following SQL statement will copy only the German customers into the new table: 1234SELECT *INTO CustomersBackup2013FROM CustomersWHERE Country='Germany'; When used in combination with IN keyword, the SELECT INTO statement can also creates a new table in another database. The following statement copies the Customers table into a new CustomersBackup2013 in Backup.mdb database: 123SELECT *INTO CustomersBackup2013 IN 'Backup.mdb'FROM Customers; Using the the INSERT INTO SELECT StatementSimilar to the SELECT INTO statement, the INSERT INTO SELECT statement copies data from one table and inserts it into an existing table. Columns can also be specified to ask the INSERT INTO SELECT statement only insert the data in designated columns of the target table. The following statement copies only the German suppliers into Customers: 123INSERT INTO Customers (CustomerName, Country)SELECT SupplierName, Country FROM SuppliersWHERE Country='Germany'; Table Schema and Index ManagementCreating DatabaseThe CREATE DATABASE statement is used to create a database. The following SQL statement creates a database called my_db: 1CREATE DATABASE my_db; Deleting DatabaseThe DROP DATABASE statement is used to delete a database. The following SQL statement deletes the my_db database. 1DROP DATABASE my_db; Creating TableThe CREATE TABLE statement is used to create a table in a database. During the creation, one will specify the name of the table, columns it has, and the name, data type and optional constrain of each column. The following SQL statement creates a table called Persons that contains five columns: PersonID, LastName, FirstName, Address, and City. 1234567CREATE TABLE Persons ( PersonID int, LastName varchar(255), FirstName varchar(255), Address varchar(255), City varchar(255)); Deleting TableThe DROP TABLE statement is used to delete a table. The following SQL statement deletes the Persons table: 1DROP TABLE Persons; Deleting all the Records in a TableThe TRUNCATE TABLE statement is used to delete all the data inside a table while not deleting the table itself. The following SQL statement deletes all the record in the Persons table: 1TRUNCATE TABLE Persons; Modifying Table SchemaThe ALTER TABLE statement is used to add, delete, or modify columns in an existing table. The following statement adds a new column named DateOfBirth in the Persons table: 12ALTER TABLE PersonsADD DateOfBirth date; The following statement changes the data type of the DateOfBirth column in the Persons table: 12ALTER TABLE PersonsMODIFY COLUMN DateOfBirth year; The following statement deletes the DateOfBirth column in the Persons table: 12ALTER TABLE PersonsDROP COLUMN DateOfBirth; The SQL ConstraintsSQL constraints can be added to table columns to specify rules for the data it stores. If there is any violation between the constraint and the data action, the action is aborted by the constraint. SQL constraints can be added to table columns during table creation or after the table is created. In SQL, we have the following constrains: Name Description NOT NULL Indicates that a column cannot store NULL value UNIQUE Ensures that each row for a column must have a unique value PRIMARY KEY A combination of a NOT NULL and UNIQUE. Ensures that a column (or combination of two or more columns) have a unique identity which helps to find a particular record in a table more easily and quickly FOREIGN KEY Ensure the referential integrity of the data in one table to match values in another table CHECK Ensures that the value in a column meets a specific condition DEFAULT Specifies a default value for a column The NOT NULL ConstraintThe NOT NULL constraint enforces a column to NOT accept NULL values, i.e. the field must always contain a value. This means that you cannot insert a new record, or update a record without adding a value to this field. The following table creation statement enforces the P_Id column and the LastName column to not accept NULL values: 1234567CREATE TABLE PersonsNotNull ( P_Id int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Address varchar(255), City varchar(255)); You can also add this constraint after the table is created using the ALTER TABLE statement. The following code achieves the same result: 12ALTER TABLE PersonsNotNull MODIFY P_Id int NOT NULL;ALTER TABLE PersonsNotNUll MODIFY LastName varchar(255) NOT NULL; The UNIQUE ConstraintThe UNIQUE constraint uniquely identifies each record in a database table. A UNIQUE constraint can be added on more than one columns. The following SQL creates a UNIQUE constraint named uc_PersonID on the P_Id and LastName column when the Persons table is created: 12345678CREATE TABLE Persons ( P_Id int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Address varchar(255), City varchar(255), CONSTRAINT uc_PersonID UNIQUE (P_Id, LastName)); The UNIQUE constraint can also be added after the table is created. The following statement achieves the same result: 12ALTER TABLE PersonsADD CONSTRAINT uc_PersonID UNIQUE (P_Id,LastName) The PRIMARY KEY ConstraintThe PRIMARY KEY constraint uniquely identifies each record in a database table. A PRIMARY KEY constraint can be seen as a combination of NOT NULL and UNIQUE, but while a table can have arbitary number of NOT NULL and UNIQUE constrains, a table can only have one PRIMARY KEY constraint. The following statement creates a table called Persons and adds a PRIMARY KEY constraint called pk_PersonID on column P_Id and LastName: 12345678CREATE TABLE Persons ( P_Id int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Address varchar(255), City varchar(255), CONSTRAINT pk_PersonID PRIMARY KEY (P_Id,LastName)); The following statement adds a PRIMARY KEY constraint on column P_Id and LastName to table Persons: 12ALTER TABLE PersonsADD CONSTRAINT pk_PersonID PRIMARY KEY (P_Id,LastName) The FOREIGN KEY ConstraintA FOREIGN KEY in one table points to a PRIMARY KEY in another table. The following SQL creates a FOREIGN KEY on the P_Id column when the Orders table is created: 1234567CREATE TABLE Orders ( O_Id int NOT NULL, OrderNo int NOT NULL, P_Id int, PRIMARY KEY (O_Id), CONSTRAINT fk_PerOrders FOREIGN KEY (P_Id) REFERENCES Persons(P_Id)); You can also use the ALTER TABLE statement to add a FOREIGN KEY constraint on an existing table: 1234ALTER TABLE OrdersADD CONSTRAINT fk_PerOrdersFOREIGN KEY (P_Id)REFERENCES Persons(P_Id); The CHECK ConstraintThe CHECK constraint allows you to add rules on the values a table can contain. The following SQL statement creates a CHECK contraint when the Persons table is created: 12345678CREATE TABLE Persons ( P_Id int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Address varchar(255), City varchar(255), CONSTRAINT chk_Person CHECK (P_Id&gt;0 AND City='Sandnes')); You can also use the ALTER TABLE statement to add a CHECK constraint on an existing table: 12ALTER TABLE PersonsADD CONSTRAINT chk_Person CHECK (P_Id&gt;0 AND City='Sandnes') The DEFAULT ConstraintThe DEFAULT constraint is used to specify a default value for a column. The default value will be used if you insert a new record without specifying the value of that column. The following SQL creates a DEFAULT constraint on the City column when the Persons table is created: 1234567CREATE TABLE Persons ( P_Id int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Address varchar(255), City varchar(255) DEFAULT 'Sandnes'); The DEFAULT constraint can also be used to insert system values, by using functions like GETDATE(): 123456CREATE TABLE Orders ( O_Id int NOT NULL, OrderNo int NOT NULL, P_Id int, OrderDate date DEFAULT GETDATE()); Creating IndexAn index can be created in a table to find data more quickly and efficiently. Updating a table with indexes takes more time than updating a table without, as the indexes also need an update ($O(1)$ to $O(log(n))$ for most cases). The following SQL statement creates an index named PIndex on the LastName and FirstName column in the Persons table: 12CREATE INDEX PIndexON Persons (LastName, FirstName); You can also create a UNIQUE index by adding UNIQUE keyword, which is quite similar to adding a UNIQUE constraint for most database systems: 12CREATE UNIQUE INDEX PIndexON Persons (LastName, FirstName); Droping IndexThe DROP INDEX statement is used to remove index from designated table. Unfortunately, the valid syntax of DROP INDEX varies from different database systems. For MySQL: 1ALTER TABLE table_name DROP INDEX index_name; ViewIn SQL, a view is a virtual table based on the result-set of an SQL statement. A view always shows up-to-date data! The database engine recreates the data, using the view’s SQL statement, every time a user queries a view. In most cases, view is used to save clients from sending complex query each time: Creating a View1234CREATE VIEW [Category Sales For 1997] ASSELECT DISTINCT CategoryName,Sum(ProductSales) AS CategorySalesFROM [Product Sales for 1997]GROUP BY CategoryName; Updating a View1234CREATE OR REPLACE VIEW [Current Product List] ASSELECT ProductID,ProductName,CategoryFROM ProductsWHERE Discontinued=No; Dropping a View1DROP VIEW view_name;","link":"/sql_reference/"},{"title":"Google File System 总结","text":"这篇文章是本人按照 MIT 6.824 的课程安排阅读 Google File System 的论文以及相关课程资料并总结而来。 GFS 的主要需求在学习 GFS 的原理前，首先我们应当了解 GFS 在设计时所面对的需求场景。简单概括，GFS 的设计主要基于以下几个需求： 节点失效是常态。系统会构建在大量的普通机器上，这使得节点失效的可能性很高。因此，GFS 必须能有较高的容错性、能够持续地监控自身的状态，同时还要能够顺畅地从节点失效中快速恢复 存储内容以大文件为主。系统需要存储的内容在通常情况下由数量不多的大文件构成，每个文件通常有几百 MB 甚至是几 GB 的大小；系统应当支持小文件，但不需要为其做出优化 主要负载为大容量连续读、小容量随机读以及追加式的连续写 系统应当支持高效且原子的文件追加操作，源于在 Google 的情境中，这些文件多用于生产者-消费者模式或是多路归并 当需要做出取舍时，系统应选择高数据吞吐量而不是低延时 GFS 集群组成本章会先给大家介绍一下一个 GFS 集群的基本组成以及各个组件的基本职责。 简单来讲，除了客户端以外，一个 GFS 集群还包括一个 Master 节点和若干个 Chunk Server。它们会作为用户级进程运行在普通的 Linux 机器上。 在存储文件时，GFS 会把文件切分成若干个拥有固定长度的 Chunk（块）并存储。Master 在创建 Chunk 时会为它们赋予一个唯一的 64 位 Handle（句柄），并把它们移交给 Chunk Server，而 Chunk Server 则以普通文件的形式将每个 Chunk 存储在自己的本地磁盘上。为了确保 Chunk 的可用性，GFS 会把每个 Chunk 备份成若干个 Replica 分配到其他 Chunk Server 上。 GFS 的 Master 负责维护整个集群的元数据，包括集群的 Namespace（命名空间，即文件元数据）以及 Chunk Lease 管理、无用 Chunk 回收等系统级操作。Chunk Server 除了保存 Chunk 以外也会周期地和 Master 通过心跳信号进行通信，Master 也借此得以收集每个 Chunk Server 当前的状态，并向其发送指令。 鉴于整个集群只有一个 Master，客户端在和 GFS 集群通信时，首先会从 Master 处获取 GFS 的元数据，而实际文件的数据传输则会与 Chunk Server 直接进行，以避免 Master 成为整个系统的数据传输瓶颈；除此以外，客户端也会在一定时间内缓存 Master 返回的集群元数据。 GFS 的元数据GFS 集群的所有元数据都会保存在 Master 的内存中。鉴于整个集群只会有一个 Master，这也使得元数据的管理变得更为简单。GFS 集群的元数据主要包括以下三类信息： 文件与 Chunk 的 Namespace 文件与 Chunk 之间的映射关系 每个 Chunk Replica 所在的位置 元数据保存在 Master 的内存中使得 Master 要对元数据做出变更变得极为容易；同时，这也使得 Master 能够更加高效地扫描集群的元数据，以唤起 Chunk 回收、Chunk 均衡等系统级管理操作。唯一的不足在于这使得整个集群所能拥有的 Chunk 数量受限于 Master 的内存大小，不过从论文的内容来看，这样的瓶颈在 Google 中从来没有被触及过，源于对于一个 64MB 大小的 Chunk，Master 只需要维持不到 64 字节的元数据。况且，相比于增加代码的复杂度，提高 Master 内存容量的成本要小得多。 为了保证元数据的可用性，Master 在对元数据做任何操作前对会用先写日志的形式将操作进行记录，日志写入完成后再进行实际操作，而这些日志也会被备份到多个机器上进行保存。不过，Chunk Replica 的位置不会被持久化到日志中，而是由 Master 在启动时询问各个 Chunk Server 其当前所有的 Replica。这可以省去 Master 与 Chunk Server 同步数据的成本，同时进一步简化 Master 日志持久化的工作。这样的设计也是合情合理的，毕竟 Chunk Server 当前实际持有哪些 Replica 也应由 Chunk Server 自己说了算。 数据一致性用户在使用 GFS 这类数据存储系统时，首先应当了解其所能提供的数据一致性，而作为学习者我们也应先理解 GFS 对外呈现的数据一致性功能。 首先，命名空间完全由单节点 Master 管理在其内存中，这部分数据的修改可以通过让 Master 为其添加互斥锁来解决并发修改的问题，因此命名空间的数据修改是可以确保完全原子的。 文件的数据修改则相对复杂。在讲述接下来的内容前，首先我们先明确，在文件的某一部分被修改后，它可能进入以下三种状态的其中之一： 客户端读取不同的 Replica 时可能会读取到不同的内容，那这部分文件是不一致的（Inconsistent） 所有客户端无论读取哪个 Replica 都会读取到相同的内容，那这部分文件就是一致的（Consistent） 所有客户端都能看到上一次修改的所有完整内容，且这部分文件是一致的，那么我们说这部分文件是确定的（Defined） 在修改后，一个文件的当前状态将取决于此次修改的类型以及修改是否成功。具体来说： 如果一次写入操作成功且没有与其他并发的写入操作发生重叠，那这部分的文件是确定的（同时也是一致的） 如果有若干个写入操作并发地执行成功，那么这部分文件会是一致的但会是不确定的：在这种情况下，客户端所能看到的数据通常不能直接体现出其中的任何一次修改 失败的写入操作会让文件进入不一致的状态 这之间的关系也被整理为了论文中的表格 1： GFS 支持的文件数据修改数据包括两种：指定偏移值的数据写入（Write）以及数据追加（Record Append）。当写入时，指定的数据会被直接写入到客户端指定的偏移位置中，覆盖原有的数据。GFS 并未为该操作提供太多的一致性保证：如果不同的客户端并发地写入同一块文件区域，操作完成后这块区域的数据可能由各次写入的数据碎片所组成，即进入不确定的状态。 与写入操作不同，GFS 确保即便是在并发时，数据追加操作也是原子且 at least once（至少一次）的。操作完成后，GFS 会把实际写入的偏移值返回给客户端，该偏移值即代表包含所写入数据的确定的文件区域的起始位置。由于数据追加操作是 at least once 的，GFS 有可能会在文件中写入填充（padding）或是重复数据，但出现的概率不高。 在读取数据时，为了避免读入填充数据或是损坏的数据，数据在写入前往往会放入一些如校验和等元信息以用于验证其可用性，如此一来 GFS 的客户端 library 便可以在读取时自动跳过填充和损坏的数据。不过，鉴于数据追加操作的 at lease once 特性，客户端仍有可能读入重复的数据，此时只能由上层应用通过鉴别记录的唯一 ID 等信息来过滤重复数据了。 对应用的影响GFS 的一致性模型是相对松散的，这就要求上层应用在使用 GFS 时能够适应 GFS 所提供的一致性语义。简单来讲，上层应用可以通过两种方式来做到这一点：更多使用追加操作而不是覆写操作；写入包含校验信息的数据。 青睐追加操作而不是覆写操作的原因是明显的：GFS 针对追加操作做出了显著的优化，这使得这种数据写入方式的性能更高，而且也能提供更强的一致性语义。尽管如此，追加操作 at least once 的特性仍使得客户端可能读取到填充或是重复数据，这要求客户端能够容忍这部分无效数据。一种可行的做法是在写入的同时为所有记录写入各自的校验和，并在读取时进行校验，以剔除无效的数据；如果客户端无法容忍重复数据，客户端也可以在写入时为每条记录写入唯一的标识符，以便在读取时通过标识符去除重复的数据。 GFS 集群常见操作流程Master Namespace 管理在前面我们已经了解到，Namespace 作为 GFS 元信息的一部分会被维持在 Master 的内存中，由 Master 负责管理。在逻辑上，GFS Master 并不会根据文件与目录的关系以分层的结构来管理这部分数据，而是单纯地将其表示为从完整路径名到对应文件元数据的映射表，并在路径名上应用前缀压缩以减少内存占用。 为了管理来自不同客户端的并发请求对 Namespace 的修改，Master 会为 Namespace 中的每个文件和目录都分配一个读写锁（Read-Write Lock）。由此，对不同 Namespace 区域的并发请求便可以同时进行。 所有 Master 操作在执行前都会需要先获取一系列的锁：通常，当操作涉及某个路径 /d1/d2/.../dn/leaf 时，Master 会需要先获取从 /d1、/d1/d2 到 /d1/d2/.../dn 的读锁，然后再根据操作的类型获取 /d1/d2/.../dn/lead 的读锁或写锁 —— 获取父目录的读锁是为了避免父目录在此次操作执行的过程中被重命名或删除。 由于大量的读写锁可能会造成较高的内存占用，这些锁会在实际需要时才进行创建，并在不再需要时被销毁。除外，所有的锁获取操作也会按照一个相同的顺序进行，以避免发生死锁：锁首先按 Namespace 树的层级排列，同一层级内则以路径名字典序排列。 读取文件客户端从 GFS 集群中读取文件内容的过程大致如下： 对于指定的文件名和读取位置偏移值，客户端可以根据固定的 Chunk 大小来计算出该位置在该文件的哪一个 Chunk 中 客户端向 Master 发出请求，其中包含要读取的文件名以及 Chunk 索引值 Master 向客户端响应该 Chunk 的 Handle 以及其所有 Replica 当前所在的位置。客户端会以文件名和 Chunk 索引值为键缓存该数据 之后，客户端便可以选取其中一个 Replica 所在的 Chunk Server 并向其发起请求，请求中会指定需要读取的 Chunk 的 Handle 以及要读取的范围 Chunk Lease在客户端对某个 Chunk 做出修改时，GFS 为了能够处理不同的并发修改，会把该 Chunk 的 Lease 交给某个 Replica，使其成为 Primary：Primary 会负责为这些修改安排一个执行顺序，然后其他 Replica 便按照相同的顺序执行这些修改。 Chunk Lease 在初始时会有 60 秒的超时时间。在未超时前，Primary 可以向 Master 申请延长 Chunk Lease 的时间；必要时 Master 也可以直接撤回已分配的 Chunk Lease。 文件写入客户端尝试将数据写入到某个 Chunk 的指定位置的过程大致如下： 客户端向 Master 询问目前哪个 Chunk Server 持有该 Chunk 的 Lease Master 向客户端返回 Primary 和其他 Replica 的位置 客户端将数据推送到所有的 Replica 上。Chunk Server 会把这些数据保存在缓冲区中，等待使用 待所有 Replica 都接收到数据后，客户端发送写请求给 Primary。Primary 为来自各个客户端的修改操作安排连续的执行序列号，并按顺序地应用于其本地存储的数据 Primary 将写请求转发给其他 Secondary Replica，Replica 们按照相同的顺序应用这些修改 Secondary Replica 响应 Primary，示意自己已经完成操作 Primary 响应客户端，并返回该过程中发生的错误（若有） 如果该过程有发生错误，可以认为修改已在 Primary 和部分 Secondary 上成功执行（如果在 Primary 上就出错了，那么写请求不会被转发出去）。此时可以认为此次修改操作没有成功，因为数据会处于不一致的状态。实际上，GFS 所使用的客户端 lib 在此时会重新尝试执行此次操作。 值得注意的是，这个流程特意将数据流与控制流分开：客户端先向 Chunk Server 提交数据，再将写请求发往 Primary。这么做的好处在于 GFS 能够更好地利用网络带宽资源。 从上述步骤可见，控制流借由写请求从客户端流向 Primary，再流向其他 Secondary Replica。实际上，数据流以一条线性数据管道进行传递的：客户端会把数据上传到离自己最近的 Replica，该 Replica 在接收到数据后再转发给离自己最近的另一个 Replica，如此递归直到所有 Replica 都能接收到数据，如此一来便能够利用上每台机器的所有出口带宽。 文件追加文件追加操作的过程和写入的过程有几分相似： 客户端将数据推送到每个 Replica，然后将请求发往 Primary Primary 首先判断将数据追加到该块后是否会令块的大小超过上限：如果是，那么 Primary 会为该块写入填充至其大小达到上限，并通知其他 Replica 执行相同的操作，再响应客户端，通知其应在下一个块上重试该操作 如果数据能够被放入到当前块中，那么 Primary 会把数据追加到自己的 Replica 中，拿到追加成功返回的偏移值，然后通知其他 Replica 将数据写入到该偏移位置中 最后 Primary 再响应客户端 当追加操作在部分 Replica 上执行失败时，Primary 会响应客户端，通知它此次操作已失败，客户端便会重试该操作。和写入操作的情形相同，此时已有部分 Replica 顺利写入这些数据，重新进行数据追加便会导致这一部分的 Replica 上出现重复数据，不过 GFS 的一致性模型也确实并未保证每个 Replica 都会是完全一致的。 GFS 只确保数据会以一个原子的整体被追加到文件中至少一次。由此我们可以得出，当追加操作成功时，数据必然已被写入到所有 Replica 的相同偏移位置上，且每个 Replica 的长度都至少超出此次追加的记录的尾部，下一次的追加操作必然会被分配一个比该值更大的偏移值，或是被分配到另一个新的块上。 文件快照GFS 还提供了文件快照操作，可为指定的文件或目录创建一个副本。 快照操作的实现采用了写时复制（Copy on Write）的思想： 在 Master 接收到快照请求后，它首先会撤回这些 Chunk 的 Lease，以让接下来其他客户端对这些 Chunk 进行写入时都会需要请求 Master 获知 Primary 的位置，Master 便可利用这个机会创建新的 Chunk 当 Chunk Lease 撤回或失效后，Master 会先写入日志，然后对自己管理的命名空间进行复制操作，复制产生的新记录指向原本的 Chunk 当有客户端尝试对这些 Chunk 进行写入时，Master 会注意到这个 Chunk 的引用计数大于 1。此时，Master 会为即将产生的新 Chunk 生成一个 Handle，然后通知所有持有这些 Chunk 的 Chunk Server 在本地复制出一个新的 Chunk，应用上新的 Handle，然后再返回给客户端 Replica 管理为了进一步优化 GFS 集群的效率，Master 在 Replica 的位置选取上会采取一定的策略。 Master 的 Replica 编排策略主要为了实现两个目标：最大化数据的可用性，以及最大化网络带宽的利用率。为此，Replica 不仅需要被保存在不同的机器上，还会需要被保存在不同的机架上，这样如果整个机架不可用了，数据仍然得以存活。如此一来，不同客户端对同一个 Chunk 进行读取时便可以利用上不同机架的出口带宽，但坏处就是进行写入时数据也会需要在不同机架间流转，不过在 GFS 的设计者看来这是个合理的 trade-off。 Replica 的生命周期转换操作实际只有两个：创建和删除。首先，Replica 的创建可能源于以下三种事件：创建 Chunk、为 Chunk 重备份、以及 Replica 均衡。 在 Master 创建一个新的 Chunk 时，首先它会需要考虑在哪放置新的 Replica。Master 会考虑如下几个因素： Master 会倾向于把新的 Replica 放在磁盘使用率较低的 Chunk Server 上 Master 会倾向于确保每个 Chunk Server 上“较新”的 Replica 不会太多，因为新 Chunk 的创建意味着接下来会有大量的写入，如果某些 Chunk Server 上有太多的新 Chunk Replica，那么写操作压力就会集中在这些 Chunk Server 上 如上文所述，Master 会倾向于把 Replica 放在不同的机架上 当某个 Chunk 的 Replica 数量低于用户指定的阈值时，Master 就会对该 Chunk 进行重备份。这可能是由 Chunk Server 失效、Chunk Server 回报 Replica 数据损坏或是用户提高了 Replica 数量阈值所触发。 首先，Master 会按照以下因素为每个需要重备份的 Chunk 安排优先级： 该 Chunk 的 Replica 数距离用户指定的 Replica 数量阈值的差距有多大 优先为未删除的文件（见下文）的 Chunk 进行重备份 除外，Master 还会提高任何正在阻塞用户操作的 Chunk 的优先级 有了 Chunk 的优先级后，Master 会选取当前拥有最高优先级的 Chunk，并指定若干 Chunk Server 直接从现在已有的 Replica 上复制数据。Master 具体会指定哪些 Chunk Server 进行复制操作同样会考虑上面提到的几个因素。除外，为了减少重备份对用户使用的影响，Master 会限制当前整个集群正在进行的复制操作的数量，同时 Chunk Server 也会限制复制操作所使用的带宽。 最后，Master 会周期地检查每个 Chunk 当前在集群内的分布情况，并在必要时迁移部分 Replica 以更好地均衡各节点的磁盘利用率和负载。新 Replica 的位置选取策略和上面提到的大体相同，除此以外 Master 还会需要选择要移除哪个已有的 Replica：简单概括的话，Master 会倾向于移除磁盘占用较高的 Chunk Server 上的 Replica，以均衡磁盘使用率。 删除文件当用户对某个文件进行删除时，GFS 不会立刻删除数据，而是在文件和 Chunk 两个层面上都 lazy 地对数据进行移除。 首先，当用户删除某个文件时，GFS 不会从 Namespace 中直接移除该文件的记录，而是将该文件重命名为另一个隐藏的名称，并带上删除时的时间戳。在 Master 周期扫描 Namespace 时，它会发现那些已被“删除”较长时间，如三天，的文件，这时候 Master 才会真正地将其从 Namespace 中移除。在文件被彻底从 Namespace 删除前，客户端仍然可以利用这个重命名后的隐藏名称读取该文件，甚至再次将其重命名以撤销删除操作。 Master 在元数据中有维持文件与 Chunk 之间的映射关系：当 Namespace 中的文件被移除后，对应 Chunk 的引用计数便自动减 1。同样是在 Master 周期扫描元数据的过程中，Master 会发现引用计数已为 0 的 Chunk，此时 Master 便会从自己的内存中移除与这些 Chunk 有关的元数据。在 Chunk Server 和 Master 进行的周期心跳通信中，Chunk Server 会汇报自己所持有的 Chunk Replica，此时 Master 便会告知 Chunk Server 哪些 Chunk 已不存在于元数据中，Chunk Server 则可自行移除对应的 Replica。 采用这种删除机制主要有如下三点好处： 对于大规模的分布式系统来说，这样的机制更为可靠：在 Chunk 创建时，创建操作可能在某些 Chunk Server 上成功了，在其他 Chunk Server 上失败了，这导致某些 Chunk Server 上可能存在 Master 不知道的 Replica。除此以外，删除 Replica 的请求可能会发送失败，Master 会需要记得尝试重发。相比之下，由 Chunk Server 主动地删除 Replica 能够以一种更为统一的方式解决以上的问题 这样的删除机制将存储回收过程与 Master 日常的周期扫描过程合并在了一起，这就使得这些操作可以以批的形式进行处理，以减少资源损耗；除外，这样也得以让 Master 选择在相对空闲的时候进行这些操作 用户发送删除请求和数据被实际删除之间的延迟也有效避免了用户误操作的问题 不过，如果在存储资源较为稀缺的情况下，用户对存储空间使用的调优可能就会受到该机制的阻碍。为此，GFS 允许客户端再次指定删除该文件，以确实地从 Namespace 层移除该文件。除外，GFS 还可以让用户为 Namespace 中不同的区域指定不同的备份和删除策略，如限制 GFS 不对某个目录下的文件进行 Chunk 备份等。 高可用机制Master前面我们提到，Master 会以先写日志（Operation Log）的形式对集群元数据进行持久化：日志在被确实写出前，Master 不会对客户端的请求进行响应，后续的变更便不会继续执行；除外，日志还会被备份到其他的多个机器上，日志只有在写入到本地以及远端备份的持久化存储中才被视为完成写出。 在重新启动时，Master 会通过重放已保存的操作记录来恢复自身的状态。为了保证 Master 能够快速地完成恢复，Master 会在日志达到一定大小后为自身的当前状态创建 Checkpoint（检查点），并删除 Checkpoing 创建以前的日志，重启时便从最近一次创建的 Checkpoint 开始恢复。Checkpoint 文件的内容会以 B 树的形式进行组织，且在被映射到内存后便能够在不做其他额外的解析操作的情况下检索其所存储的 Namespace，这便进一步减少了 Master 恢复所需的时间。 为了简化设计，同一时间只会有一个 Master 起作用。当 Master 失效时，外部的监控系统会侦测到这一事件，并在其他地方重新启动新的 Master 进程。 除外，集群中还会有其他提供只读功能的 Shadow Master：它们会同步 Master 的状态变更，但有可能延迟若干秒，其主要用于为 Master 分担读操作的压力。Shadow Master 会通过读取 Master 操作日志的某个备份来让自己的状态与 Master 同步；它也会像 Master 那样，在启动时轮询各个 Chunk Server，获知它们所持有的 Chunk Replica 信息，并持续监控它们的状态。实际上，在 Master 失效后，Shadow Master 仍能为整个 GFS 集群提供只读功能，而 Shadow Master 对 Master 的依赖只限于 Replica 位置的更新事件。 Chunk Server作为集群中的 Slave 角色，Chunk Server 失效的几率比 Master 要大得多。在前面我们已经提到，Chunk Server 失效时，其所持有的 Replica 对应的 Chunk 的 Replica 数量便会降低，Master 也会发现 Replica 数量低于用户指定阈值的 Chunk 并安排重备份。 除外，当 Chunk Server 失效时，用户的写入操作还会不断地进行，那么当 Chunk Server 重启后，Chunk Server 上的 Replica 数据便有可能是已经过期的。为此，Master 会为每个 Chunk 维持一个版本号，以区分正常的和过期的 Replica。每当 Master 将 Chunk Lease 分配给一个 Chunk Server 时，Master 便会提高 Chunk 的版本号，并通知其他最新的 Replica 更新自己的版本号。如果此时有 Chunk Server 失效了，那么它上面的 Replica 的版本号就不会变化。 在 Chunk Server 重启时，Chunk Server 会向 Master 汇报自己所持有的 Chunk Replica 及对应的版本号。如果 Master 发现某个 Replica 版本号过低，便会认为这个 Replica 不存在，如此一来这个过期的 Replica 便会在下一次的 Replica 回收过程中被移除。除外，Master 向客户端返回 Replica 位置信息时也会返回 Chunk 当前的版本号，如此一来客户端便不会读取到旧的数据。 数据完整性如前面所述，每个 Chunk 都会以 Replica 的形式被备份在不同的 Chunk Server 中，而且用户可以为 Namespace 的不同部分赋予不同的备份策略。 为了保证数据完整，每个 Chunk Server 都会以校验和的形式来检测自己保存的数据是否有损坏；在侦测到损坏数据后，Chunk Server 也可以利用其它 Replica 来恢复数据。 首先，Chunk Server 会把每个 Chunk Replica 切分为若干个 64KB 大小的块，并为每个块计算 32 位校验和。和 Master 的元数据一样，这些校验和会被保存在 Chunk Server 的内存中，每次修改前都会用先写日志的形式来保证可用。当 Chunk Server 接收到读请求时，Chunk Server 首先会利用校验和检查所需读取的数据是否有发生损坏，如此一来 Chunk Server 便不会把损坏的数据传递给其他请求发送者，无论它是客户端还是另一个 Chunk Server。发现损坏后，Chunk Server 会为请求发送者发送一个错误，并向 Master 告知数据损坏事件。接收到错误后，请求发送者会选择另一个 Chunk Server 重新发起请求，而 Master 则会利用另一个 Replica 为该 Chunk 进行重备份。当新的 Replica 创建完成后，Master 便会通知该 Chunk Server 删除这个损坏的 Replica。 当进行数据追加操作时，Chunk Server 可以为位于 Chunk 尾部的校验和块的校验和进行增量式的更新，或是在产生了新的校验和块时为其计算新的校验和。即使是被追加的校验和块在之前已经发生了数据损坏，增量更新后的校验和依然会无法与实际的数据相匹配，在下一次读取时依然能够检测到数据的损坏。在进行数据写入操作时，Chunk Server 必须读取并校验包含写入范围起始点和结束点的校验和块，然后进行写入，最后再重新计算校验和。 除外，在空闲的时候，Chunk Server 也会周期地扫描并校验不活跃的 Chunk Replica 的数据，以确保某些 Chunk Replica 即使在不怎么被读取的情况下，其数据的损坏依然能被检测到，同时也确保了这些已损坏的 Chunk Replica 不至于让 Master 认为该 Chunk 已有足够数量的 Replica。 附录节点缓存在 GFS 中，客户端和 Chunk Server 都不会对文件数据进行缓存。对于客户端而言，考虑到大多数应用都会选择顺序读取某些大文件，缓存的作用微乎其微，不过客户端确实会缓存 GFS 的元数据以减少和 Master 的通信；对于 Chunk Server 来说，缓存文件数据也是不必要的，因为这些内容本身就保存在它的本地磁盘上，Linux 内核的缓存机制也会把经常访问的磁盘内容放置在内存中。 Chunk 的大小对于 GFS 而言，Chunk 的大小是一个比较重要的参数，而 GFS 选择了使用 64MB 作为 Chunk 的大小。 较大的 Chunk 主要带来了如下几个好处： 降低客户端与 Master 通信的频率 增大客户端进行操作时这些操作落到同一个 Chunk 上的概率 减少 Master 所要保存的元数据的体积 不过，较大的 Chunk 会使得小文件占据额外的存储空间；一般的小文件通常只会占据一个 Chunk，这些 Chunk 也容易成为系统的负载热点。但正如之前所设想的需求那样，这样的文件在 Google 的场景下不是普遍存在的，这样的问题并未在 Google 中真正出现过。即便真的出现了，也可以通过提升这类文件的 Replica 数量来将负载进行均衡。 组件的快速恢复GFS 的组件在设计时着重提高了状态恢复的速度，通常能够在几秒钟内完成启动。在这样的保证下，GFS 的组件实际上并不对正常关闭和异常退出做区分：要关闭某个组件时直接 kill -9 即可。 FAQMIT 6.824 的课程材料中给出了和 GFS 有关的 FAQ，在此我简单地翻译一下其中比较重要的一些内容。 Q：为什么原子记录追加操作是至少一次（At Least Once），而不是确定一次（Exactly Once）？ 要让追加操作做到确定一次是不容易的，因为如此一来 Primary 会需要保存一些状态信息以检测重复的数据，而这些信息也需要复制到其他服务器上，以确保 Primary 失效时这些信息不会丢失。在 Lab 3 中你会实现确定一次的行为，但用的是比 GFS 更复杂的协议（Raft）。 Q：应用怎么知道 Chunk 中哪些是填充数据或者重复数据？ 要想检测填充数据，应用可以在每个有效记录之前加上一个魔数（Magic Number）进行标记，或者用校验和保证数据的有效性。应用可通过在记录中添加唯一 ID 来检测重复数据，这样应用在读入数据时就可以利用已经读入的 ID 来排除重复的数据了。GFS 本身提供了 library 来支撑这些典型的用例。 Q：考虑到原子记录追加操作会把数据写入到文件的一个不可预知的偏移值中，客户端该怎么找到它们的数据？ 追加操作（以及 GFS 本身）主要是面向那些会完整读取文件的应用的。这些应用会读取所有的记录，所以它们并不需要提前知道记录的位置。例如，一个文件中可能包含若干个并行的网络爬虫获取的所有链接 URL。这些 URL 在文件中的偏移值是不重要的，应用只会想要完整读取所有 URL。 Q：如果一个应用使用了标准的 POSIX 文件 API，为了使用 GFS 它会需要做出修改吗？ 答案是需要的，不过 GFS 并不是设计给已有的应用的，它主要面向的是新开发的应用，如 MapReduce 程序。 Q：GFS 是怎么确定最近的 Replica 的位置的？ 论文中有提到 GFS 是基于保存 Replica 的服务器的 IP 地址来判断距离的。在 2003 年的时候，Google 分配 IP 地址的方式应该确保了如果两个服务器的 IP 地址在 IP 地址空间中较为接近，那么它们在机房中的位置也会较为接近。 Q：Google 现在还在使用 GFS 吗？ Google 仍然在使用 GFS，而且是作为其他如 BigTable 等存储系统的后端。由于工作负载的扩大以及技术的革新，GFS 的设计在这些年里无疑已经经过大量调整了，但我并不了解其细节。HDFS 是公众可用的对 GFS 的设计的一种效仿，很多公司都在使用它。 Q：Master 不会成为性能瓶颈吗？ 确实有这个可能，GFS 的设计者也花了很多心思来避免这个问题。例如，Master 会把它的状态保存在内存中以快速地进行响应。从实验数据来看，对于大文件读取（GFS 主要针对的负载类型），Master 不是瓶颈所在；对于小文件操作以及目录操作，Master 的性能也还跟得上（见 6.2.4 节）。 Q：GFS 为了性能和简洁而牺牲了正确性，这样的选择有多合理呢？ 这是分布式系统领域的老问题了。保证强一致性通常需要更加复杂且需要机器间进行更多通信的协议（正如我们会在接下来几门课中看到的那样）。通过利用某些类型的应用可以容忍较为松懈的一致性的事实，人们就能够设计出拥有良好性能以及足够的一致性的系统。例如，GFS 对 MapReduce 应用做出了特殊优化，这些应用需要的是对大文件的高读取效率，还能够容忍文件中存在数据空洞、重复记录或是不一致的读取结果；另一方面，GFS 则不适用于存储银行账号的存款信息。 Q：如果 Master 失效了会怎样？ GFS 集群中会有持有 Master 状态完整备份的 Replica Master；通过论文中没有提到的某个机制，GFS 会在 Master 失效时切换到其中一个 Replica（见 5.1.3 节）。有可能这会需要一个人类管理者的介入来指定一个新的 Master。无论如何，我们都可以确定集群中潜伏着一个故障单点，理论上能够让集群无法从 Master 失效中进行自动恢复。我们会在后面的课程中学习如何使用 Raft 协议实现可容错的 Master。 问题除了 FAQ，课程还要求学生在阅读 GFS 的论文后回答一个问题，问题如下： Describe a sequence of events that result in a client reading stale data from the Google File System 描述一个事件序列，使得客户端会从 Google File System 中读取到过时的数据 通过查阅论文，不难找到两处答案：由失效后重启的 Chunk Server + 客户端缓存的 Chunk 位置数据导致客户端读取到过时的文件内容（见 4.5 和 2.7.1 节），和由于 Shadow Master 读取到的过时文件元信息（见 5.1.3 节）。以上是保证所有写入操作都成功时客户端可能读取到过时数据的两种情况 —— 如果有写入操作失败，数据会进入不确定的状态，自然客户端也有可能读取到过时或是无效的数据。 结语本文没有总结论文中第六、七章的内容：第六章是 GFS 各项指标的测试结果，受限于篇幅故没能在此放出，若读者对 Google 测试 GFS 性能指标的方法有所兴趣也可参考这一章的内容；第七章则提到了 Google 在开发 GFS 时踩过的一些坑，主要和 Linux 本身的 bug 有关，此处没能放出这部分的内容主要是考虑到这些 bug 主要涉及 Linux 2.2 和 2.4 版本，相较于今日已失去其时效性，况且这些 bug 也很有可能已经由 GFS 的开发者修复并提交到新版的 Linux 中了。 从内容上看，阅读 GFS 的论文是对高性能和强一致性之间的矛盾的一次很好的 Case Study：在强一致性面前，GFS 选择了更高的吞吐性能以及自身架构的简洁。高性能与强一致性之间的矛盾是分布式系统领域经久不衰的话题，源于它们通常是不可兼得的。除外，为了实现理想的一致性，系统也可能面临来自并发操作、机器失效、网络隔离等问题所带来的挑战。 从概念上来讲，一致指的是某个正确的状态，而一个系统往往会有很多种不同的正确的状态，它们又常被统称为系统的一致性模型。在后面要阅读的论文中，我们还会不断地看到这个概念。 Google File System 论文的发表催生了后来的 HDFS，后者直到今天依然是开源分布式文件系统解决方案的首选。Google MapReduce 加上 Google File System 这两篇论文可谓是大数据时代的开山之作，与 Google BigTable 并称 Google 的三架马车，由此看来这几篇经典论文还是很值得我们去学习一番的。","link":"/gfs/"},{"title":"Gradle 学习笔记","text":"本文为我个人的 Gradle 学习笔记，包含了 Gradle User Guide 各章节的重点归纳。归纳的内容从 User Guide 的第 4 章开始。 本文的章节顺序有所调整以方便阅读。笔记会先从使用已有的 Gradle 项目开始，然后进入创建新的 Gradle 项目的部分。在创建项目的部分则会先以 Java 项目为例，再延伸到其他 JVM 语言项目。 4 使用 Gradle 命令行4.1 执行多个任务在使用 Gradle 时，可以如命令 gradle compile test 这般指定执行多个 Gradle 任务，Gradle 会按照指定的顺序指定这些任务以及它们的以来任务。在执行时，Gradle 也会合理地绘制任务依赖 DAG，以确保被多个任务依赖的任务只执行一次。 （译者注：在 Gradle 安装目录的 bin 文件夹里实际上只有 gradle 一个可执行文件，因此所有的 Gradle 功能只能通过 gradle 命令或是后面提到的 gradlew 命令来执行） 除此之外，在代码示例 4.1 中看到我们可以使用 &lt;&lt; 运算符为某些默认任务追加逻辑： 123456789101112131415task compile &lt;&lt; { println 'compiling source'}task compileTest(dependsOn: compile) &lt;&lt; { println 'compiling unit tests'}task test(dependsOn: [compile, compileTest]) &lt;&lt; { println 'running unit tests'}task dist(dependsOn: [compile, test]) &lt;&lt; { println 'building the distribution'} 4.2 排除任务通过 -x 命令行选项可以在构建时排除指定的任务。被排除的任务不会被执行，同样其他依赖该任务的任务也不会被执行。 123456789&gt; gradle dist -x test:compilecompiling source:distbuilding the distributionBUILD SUCCESSFULTotal time: 1 secs 4.3 在任务失败后继续构建在默认情况下，Gradle 会在某个任务执行失败后立刻停止执行。使用 --continue 选项可以使 Gradle 继续执行其它不相关的任务，但当一个任务执行失败后，其它依赖它的任务仍然不会被执行。 4.4 任务名称缩写在 Gradle 命令行中指定任务并不需要给出任务的全名，只要给出任务的前面几个字母足够给 Gradle 确定唯一的任务即可。如 dist 任务的执行可写作 gradle d。 对于使用驼峰命名法的任务我们还可以给出任务名每个单词的缩写，如 compileTest 任务可写作 compTest 甚至 cT。 任务名称缩写同样可用于前面提到的 -x 参数。 4.5 选择要执行的构建执行 gradle 指令时，Gradle 会在当前路径下查找构建文件。我们还可以使用 -b 参数指定其他构建文件。但需要注意的是，使用了 -b 参数后 settings.gradle 文件便不会被使用。 除此之外，我们还可以通过 -p 来指定所使用的项目目录，对于多项目构建而言这是更好的做法。 4.6 强制任务执行需要 Gradle 任务都支持增性构建：当它们检测到输入与输出和上一次执行相比没有发生变化时便会不执行，并在执行 gradle 命令时显示为 UP-TO-DATE。我们可以通过使用 --rerun-tasks 选项来强制执行所需的所有任务。 4.7 获取构建的相关信息4.7.1 显示所有项目执行 gradle projects 即可显示所选项目的所有子项目，并显示各个项目的描述。项目的描述可在各个项目的 build.gradle 内通过修改项目的 description 属性进行设置： 1description = 'The shared API for the application' 4.7.2 显示所有任务执行 gradle tasks 即可显示所选项目的所有主要任务，包括项目的默认任务以及每个任务的描述。 默认情况下，该指令只会显示那些被赋予了分组的任务。任务的分组和描述可以分别通过修改其 group 和 description 属性进行设置： 1234dists { description = 'Builds the distribution' group = 'build'} 你也可以使用 --all 选项，如此便会显示所有分组和未分组的任务以及各个任务的依赖。 4.7.3 查看任务的具体使用方法通过执行 gradle help --task someTask 即可查看吻合给定任务名的所有任务的详细信息。这些信息包括其完整任务路径、任务类型、可用的命令行参数以及任务的描述。 4.7.4 显示任务的依赖使用 gradle dependencies 可以查看每个任务的依赖，其直接与间接的依赖将以树状的形式显示。 由于所有任务的所有依赖加起来可能会包含大量的输出信息，因此可以使用 --configuration 参数查看指定配置的依赖。 123456789&gt; gradle -q api:dependencies --configuration testCompile------------------------------------------------------------Project :api - The shared API for the application------------------------------------------------------------testCompile\\--- junit:junit:4.12 \\--- org.hamcrest:hamcrest-core:1.3 在执行 gradle 命令时使用 -q 命令行参数可以去除 Gradle 的日志信息，只保留任务本身的输出。有关 Gradle 日志的更多信息详见 22 章。 4.7.5 显示项目的构建脚本依赖运行指令 gradle buildEnvironment 可以显示项目的构建脚本依赖，显示的格式与 gradle dependencies 类似。 4.7.6 查看具体依赖的详细信息运行指令 gradle dependencyInsight 即可查看具体依赖的指定信息： 1234&gt; gradle -q webapp:dependencyInsight --dependency groovy --configuration compileorg.codehaus.groovy:groovy-all:2.4.7\\--- project :api \\--- compile 该指令可用于查看某个具体的依赖包是从如何被解析出来的。在使用该指令时，我们需要像上述示例那样通过 --dependency 和 --configuration 参数指定要查看的依赖和配置。 4.7.7 显示所有项目属性执行 gradle properties 可以查看项目的所有属性： 1234567891011121314&gt; gradle -q api:properties------------------------------------------------------------Project :api - The shared API for the application------------------------------------------------------------allprojects: [project ':api']ant: org.gradle.api.internal.project.DefaultAntBuilder@12345antBuilderFactory: org.gradle.api.internal.project.DefaultAntBuilderFactory@12345artifacts: org.gradle.api.internal.artifacts.dsl.DefaultArtifactHandler_Decorated@12345asDynamicObject: DynamicObject for project ':api'baseClassLoaderScope: org.gradle.api.internal.initialization.DefaultClassLoaderScope@12345buildDir: /home/user/gradle/samples/userguide/tutorial/projectReports/api/buildbuildFile: /home/user/gradle/samples/userguide/tutorial/projectReports/api/build.gradle 4.7.8 构建报告在执行构建时使用 --profile 选项即可在构建后于 build/reports/profile 处生成构建报告。报告的内容包括从配置到任务执行等各个阶段所花的时间。 4.8 构建预演在构建时使用 -m 选项可进入构建预演模式，会显示此次构建会以何种顺序执行哪些任务，但不会执行这些任务。 4.9 总结有关 Gradle 命令行的详细介绍可参阅附录 D：Gradle 命令行。 5 Gradle Wrapper在分发项目源代码时，源代码中可以放入特定版本的 Gradle Wrapper。此举的好处有两点： 避免代码用户因所使用的 Gradle 版本不同导致构建出错 使我们无需对所使用的持续构建服务器做过多的配置 5.1 使用 Gradle Wrapper 执行构建对于安装了 Gradle Wrapper 的项目，我们应使用 gradlew 对其执行构建，其中命令 gradlew 的使用方法和 gradle 完全一致。 Gradle Wrapper 的所有文件包括如下，注意不要让版本控制系统忽略这些文件： gradlew（Unix Shell 脚本） gradlew.bat（Windows 批处理文件） gradle/wrapper/gradle-wrapper.jar（Wrapper 的 JAR） gradle/wrapper/gradle-wrapper.properties（Wrapper 的 properties 文件） （译者注：可考虑让版本控制系统忽略 .gradle 文件夹） 在执行的过程中，gradlew 会把指定版本的 Gradle 下载至 $USER_HOME/.gradle/wrapper/dists 并执行。 5.2 为项目安装 Wrapper执行 gradle wrapper 即可为项目安装 Gradle Wrapper。在执行该指令时，我们可以通过参数 --gradle-version 和 --gradle-distribution-url 配置需要安装的 Wrapper 的版本和下载 Gradle 的路径。默认安装的 Wrapper 版本与安装时所使用的 Gradle 版本相同。 除此之外，我们还可以在构建脚本中配置一个 Wrapper 任务，如通过设置属性 gradleVersion 来改变默认的 Wrapper 版本： 123task wrapper(type: Wrapper) { gradleVersion = '2.0'} 有关 Wrapper 任务的更多配置方式，请查阅 Wrapper 的 API 文档。 6 Gradle 守护线程6.1 为何要使用 Gradle 守护线程启动 Gradle 进行项目构建的过程首先涉及到 JVM 的启动，同时 Gradle 依赖的一些库也需要比较长的时间进行初始化，这就使得启动 Gradle 进行构建首先需要等待其进行初始化。 解决的办法就是使用一个持续在后台运行的 Gradle 守护线程，每一次都将构建任务提交至该守护线程便可跳过这些初始化。除此之外，守护线程还会对项目的如依赖 DAG 等信息进行缓存，更进一步地缩小构建所需的时间。 在使用 --profile 选项生成构建报告时可以在报告中看到守护线程可以为构建节省多少时间。 从 3.0 版本开始，Gradle 守护线程将会默认开启。（译者注：实际上是如果 gradle 命令发现守护线程不存在便会自动开启守护线程） 6.2 检查守护线程的状态使用 --status 选项即可查看所有守护线程的状态。 6.3 关闭守护线程功能永久关闭守护线程功能的方式有两种： 将标识 -Dorg.gradle.daemon=false 添加至 GRADLE_OPTS 环境变量 将代码 org.gradle.daemon=false 添加至属性文件 $GRADLE_USER_HOME/gradle.properties 其中，GRADLE_USER_HOME 变量的默认值为 $USER_HOME/.gradle。该值可以通过 -g 或 --gradle-user-home 命令行参数进行设置，或是直接修改 GRADLE_USER_HOME 环境变量或 org.gradle.user.home JVM 系统属性。 在 Windows 中执行如下指令可为当前用户关闭守护线程功能： 1(if not exist &quot;%USERPROFILE%/.gradle&quot; mkdir &quot;%USERPROFILE%/.gradle&quot;) &amp;&amp; (echo org.gradle.daemon=false &gt;&gt; &quot;%USERPROFILE%/.gradle/gradle.properties&quot;) 类 Unix 系统则可通过如下 Bash 指令为当前用户关闭守护线程功能： 1mkdir -p ~/.gradle &amp;&amp; echo &quot;org.gradle.daemon=false&quot; &gt;&gt; ~/.gradle/gradle.properties 在构建时，我们还可以通过 --daemon 和 --no-daemon 选项来显式地声明此次构建是否要使用守护线程。 6.4 关闭正在运行的守护线程每个 Gradle 守护线程在闲置时都会不断对比自己占用的内存和系统剩余的内存，并在剩余内存不多时关闭自己以释放内存。因此在大多数时候我们不需要手动关闭守护线程。 不过，我们也可以通过指令 gradle --stop 关闭所有 Gradle 守护线程。 6.6 何时不该使用 Gradle 守护线程我们应在开发环境里使用 Gradle 守护线程来加速构建，但在持续整合服务器上，稳定性才是至关重要的，这时我们就应关闭 Gradle 守护线程功能，确保不同的构建之间是完全相互独立的。 7 依赖管理入门本章内容只是对 Gradle 的依赖管理系统进行了浅显的介绍，在用户手册靠后的章节中会对依赖管理系统的不同部分进行详细的阐述，本章中也会给出具体的链接。 7.2 声明依赖先看一个示例 build.gradle： 12345678910apply plugin: 'java'repositories { mavenCentral()}dependencies { compile group: 'org.hibernate', name: 'hibernate-core', version: '3.6.7.Final' testCompile group: 'junit', name: 'junit', version: '4.+'} 这个示例实际上就包含了声明依赖的几个基本元素。 7.3 依赖配置Gradle 会根据依赖所赋予的配置（Configuration）对其进行分组。对于 Java 项目所使用的 Java 插件来说，插件本身定义了的依赖配置即为 Java 程序编译到运行的几个生命周期。常用的包括： compile：编译项目源文件所需的依赖 runtime：运行项目所需的依赖。默认包含上述的编译时依赖 testCompile：编译项目测试源文件所需的依赖。默认包含上述的编译时依赖和项目源文件编译后产生的类文件 testRuntime：运行项目测试所需的依赖。默认包含上述的编译依赖、运行依赖和测试编译依赖 有关 Java 插件定义的依赖配置的更多内容详见表 45.5。 有关依赖配置的更多内容详见 23.3 小节。 7.4 外部依赖我们可以通过属性 group、name 和 version 为 Gradle 唯一地指定外部依赖： 123dependencies { compile group: 'org.hibernate', name: 'hibernate-core', version: '3.6.7.Final'} 或者我们也可以将其简写为 group:name:version 的形式： 123dependencies { compile 'org.hibernate:hibernate-core:3.6.7.Final'} 有关声明依赖的更多内容详见 23.4 小节。 7.5 库Gradle 需要在 build.gradle 脚本中为 Project.repositories 属性进行设置，指定用于下载依赖的远程库。常用的设置方式包括如下几种： 库类型 构建脚本 Maven 中心库 123repositories { mavenCentral()} JCenter 库 123repositories { jcenter()} 自定义的远程 Maven 库 12345repositories { maven { url &quot;http://repo.mycompany.com/maven2&quot; }} 自定义的远程 Ivy 库 12345repositories { ivy { url &quot;http://repo.mycompany.com/repo&quot; }} 本地 Ivy 库 123456repositories { ivy { // URL can refer to a local directory url &quot;../local-repo&quot; }} 有关库的更多内容详见 23.6 小节。 7.6 发布程序包Gradle 同样可以像 Maven 那样发布程序包。要做到这一点，我们需要将需要发布至的目标库定义到 uploadArchives 任务中： 目标库类型 构建脚本 Maven 库 123456789apply plugin: 'maven'uploadArchives { repositories { mavenDeployer { repository(url: &quot;file://localhost/tmp/myRepo/&quot;) } }} Ivy 库 1234567891011uploadArchives { repositories { ivy { credentials { username &quot;username&quot; password &quot;pw&quot; } url &quot;http://repo.mycompany.com&quot; } }} 而后执行 uploadArchives 任务，Gradle 便会构建项目并上传生成的程序包。 8 多项目构建8.1 多项目构建的基本结构部分项目可能包含多个子项目或子模块，子项目相互之间存在一定的依赖关系。构建这样的项目则需要 Gradle 支持多项目构建。 通常，一个多项目构建包括如下几个基本元素： 在项目根目录存在一个 settings.gradle 文件和一个 build.gradle 每个子项目的根目录存在各自的 *.gradle 构建文件 其中 settings.gradle 包含了项目所含子项目以及各个子项目所处位置的信息。除了直接阅读 settings.gradle，我们也可以通过执行 gradle projects 指令来查看项目下的所有子项目。 在默认情况下，Gradle 会将 settings.gradle 所处文件夹的名称作为根项目的名称，但部分持续构建服务器有可能会在构建时自动生成该文件夹名称，因此更好的做法是在 settings.gradle 中设置 rootProject.name 属性。 根目录的 build.gradle 用于进行各个子项目共享的设置，例如声明每个子项目都会使用的插件和依赖。 值得注意的是，子项目各自的 *.gradle 构建文件的名称有可能不是 build.gradle。另一种比较常见的做法是将其以子项目名称命名，如 api.gradle 或 service.gradle。 8.2 执行多项目构建对于一个项目源代码用户来说，构建包含多个子项目的项目实际上本质上也是通过 Gradle 执行不同的任务，不过控制具体执行哪个任务的方式则有所不同。我们有两种做法： 进入你感兴趣的子项目所属的目录并像平常那样通过指令 gradle < task> 执行你想要的任务 在任意目录下使用任务的限定名称来执行，如 gradle :services:webservice:build 对于第一种执行方式，Gradle 实际上会执行当前目录下所有子目录所包含的所有子项目的同名任务。比如执行命令 gradle test 则会执行所有相对于当前目录的子项目的 test 任务。 值得注意的是，使用这种执行方式时，调用 Gradle Wrapper 的方式需要进行一定的调整，因为 Gradle Wrapper 对应的 gradlew 执行文件是位于根目录下的，因此在执行时你可能需要输入像 ../../gradlew build 这样的命令。 对于第二种方式，任务的限定名称结构和 Java 类的限定名类似，只是以冒号 : 作为分隔符，同时以一个起始的冒号指代根项目。 9 持续构建在使用 Gradle 命令进行构建时，添加 -t 或 --continuous 选项即可打开持续构建模式：在该模式下，Gralde 会持续监控所指定任务的输入，并在输入发生变化时自动重新执行任务。例如在执行 build 任务时进入持续构建模式，Gradle 便会在源代码文件发生修改时自动重新构建项目。 值得注意的是，Gradle 只会监控所指定任务的输入，但如构建脚本等文件的修改不会导致 Gradle 自动重新执行任务。 任务可以通过一定的方式声明自己的输入和输出，详见案例 17.24。 10 使用 Gradle 图形界面执行命令 gradle --gui 即可打开 Gradle 图形界面。注意该命令会一直阻塞直到图形界面退出，因此在 *nix 系统下你可以使用命令 gradle --gui & 来后台执行。 15 构建初始化插件我们可以使用 Gradle 自带的初始化插件在指定文件夹直接生成一个新的 Gradle 项目。 15.1 所执行的任务该初始化插件为 Gradle 添加了如下任务： wrapper：Wrapper 类型的任务，在指定文件夹下生成 Gradle Wrapper init：InitBuild 类型的任务，依赖 wrapper 任务，用于生成 Gradle 项目 15.2 初始化类型在执行 init 任务时，我们需要使用 --type 命令行参数来给出初始化项目的类型。如果没有显式给出类型，Gradle 则会尝试自行推断类型。 支持的类型包括如下。 15.3.1 pompom 类型用于将一个已有的 Maven 项目转换为 Gradle 项目。该类型要求当前文件夹或 -p 命令行参数指定的文件夹下包含有效的 POM 文件以及 Maven 相关配置。如果在文件夹下能够找到有效的 POM 文件，Gradle 也会在未显式指定类型时推断出类型为 pom。 15.3.2 java-libraryjava-library 只能通过显式指定，Gradle 不会自动推断为 java-library 类型。 该类型将自动为生成的项目使用 java 插件并使用 jcenter 依赖库，使用 JUnit 作为测试框架并生成基本的源代码和测试代码文件夹。 除此之外，在使用该类型时还可以通过 --test-framework 参数指定使用除 JUnit 以外的测试框架。支持的框架包括： Spock：gradle init --type java-library --test-framework spock TestNG：gradle init --type java-library --test-framework testng 15.3.3 scala-libraryscala-library 只能通过显式指定，Gradle 不会自动推断为 scala-library 类型。 该类型将自动为生成的项目使用 scala 插件并使用 jcenter 依赖库，使用 Scala Test作为测试框架并生成基本的源代码和测试代码文件夹。除此之外，项目会自动使用 2.10 版本的 Scala 并默认使用 Zinc Scala 编译器。 15.3.4 groovy-librarygroovy-library 只能通过显式指定，Gradle 不会自动推断为 groovy-library 类型。 该类型将自动为生成的项目使用 groovy 插件并使用 jcenter 依赖库，使用 Spock作为测试框架并生成基本的源代码和测试代码文件夹。除此之外，项目会自动使用 2.x 版本的 Groovy。 15.3.5 basic默认的初始化类型，当类型没有被显式给出且无法自动推断为 pom 类型时便会使用该类型。此时 Gradle 会创建一个示例 build.gradle 文件并在其中放入有用的注释和链接。 14 构建脚本入门14.1 项目和任务在 Gradle 中主要包含两个基本概念：项目（Project）和任务（Task）。项目本身的定义并不明确，取决于你想要做些什么：一个项目可以构建什么东西，也可以部署别的什么东西。一个项目可以包含若干个任务，而一个任务则代表着一个构建可以执行的原子逻辑。 14.2 Hello World这节主要给出了一个构建脚本的 Hello World 示例： 12345task hello { doLast { println 'Hello world!' }} 在这段代码中调用了 Task 的 doLast 方法，其作用即把给定的 Groovy 闭包添加到任务的操作列表末尾。同样的还包括一个 doFirst 方法，顾名思义。 在声明了任务以后，我们就可以在脚本内像使用属性那样使用这个任务了： 1234567891011121314task hello { doLast { println 'Hello Earth!' }}hello.doFirst { println 'Hello Venus'}hello.doLast { println &quot;Greetings from the $hello.name task.&quot;}hello &lt;&lt; { println 'Hello Jupiter'} 其中方法 &lt;&lt; 实际上就是 doLast 的别名。 14.4 构建脚本就是代码实际上，Gradle 构建脚本所使用的语言正是 Groovy，因此在构建脚本中我们可以使用任意的 Groovy 代码： 12345task upper &lt;&lt; { String someString = 'mY_nAmE' println &quot;Original: &quot; + someString println &quot;Upper case: &quot; + someString.toUpperCase()} 14.5 任务依赖在声明任务的同时我们也可以声明任务的依赖： 123456task taskX(dependsOn: 'taskY') &lt;&lt; { println 'taskX'}task taskY &lt;&lt; { println 'taskY'} 注意声明依赖的任务的时候，被依赖的任务并不需要提前定义，正如上面的代码那样，尽管 taskX 依赖 taskY，taskY 也可以在 taskX 之后定义。 除此之外我们也可以在完成任务声明后再为任务赋予具体依赖： 1234567task taskX &lt;&lt; { println 'taskX'}task taskY &lt;&lt; { println 'taskY'}taskX.dependsOn taskY 注意，以属性指定 taskY 时需要 taskY 以预先定义，否则可以用字符串的形式来给定 taskY： 12345678task taskX &lt;&lt; { println 'taskX'}taskX.dependsOn 'taskY'task taskY &lt;&lt; { println 'taskY'} 14.6 动态任务Gradle 脚本可以利用 Groovy 的动态特性来动态地创建任务： 1234564.times { counter -&gt; task &quot;task$counter&quot; &lt;&lt; { println &quot;I'm task number $counter&quot; }}task0.dependsOn task2, task3 14.9 额外属性在任务定义内可以使用 ext 属性为任务定义额外属性： 1234567task myTask { ext.myProperty = &quot;myValue&quot;}task printTaskProperties &lt;&lt; { println myTask.myProperty} 有关额外属性的更多内容详见 16.4.2 节。 14.10 使用 Ant 任务多亏了 Groovy 对 Ant 任务的支持，我们同样可以在 Gradle 中使用 Ant 任务来更方便地进行各式各样的文件读写操作： 12345678910task loadfile &lt;&lt; { def files = file('../antLoadfileResources').listFiles().sort() files.each { File file -&gt; if (file.isFile()) { ant.loadfile(srcFile: file, property: file.name) println &quot; *** $file.name ***&quot; println &quot;${ant.properties[file.name]}&quot; } }} 详见 Groovy 的 AntBuilder 教程以及第 19 章。 14.11 使用方法在 Gradle 脚本里也可以声明方法并在其他任务中调用方法： 1234567891011121314151617task checksum &lt;&lt; { fileList('../antLoadfileResources').each {File file -&gt; ant.checksum(file: file, property: &quot;cs_$file.name&quot;) println &quot;$file.name Checksum: ${ant.properties[&quot;cs_$file.name&quot;]}&quot; }}task loadfile &lt;&lt; { fileList('../antLoadfileResources').each {File file -&gt; ant.loadfile(srcFile: file, property: file.name) println &quot;I'm fond of $file.name&quot; }}File[] fileList(String dir) { file(dir).listFiles({file -&gt; file.isFile() } as FileFilter).sort()} 14.12 默认任务可以通过 defaultTasks 方法来指定默认任务： 1defaultTasks 'clean', 'run' 14.13 基于 DAG 的配置在第 20 章可以了解到，Gradle 执行时分为配置阶段和执行阶段，其中任务 DAG 的解析在配置阶段完成，而任务的实际执行则属于执行阶段。因此，部分任务也可以基于 DAG 的信息来改变自己的行为： 123456789101112131415task distribution &lt;&lt; { println &quot;We build the zip with version=$version&quot;}task release(dependsOn: 'distribution') &lt;&lt; { println 'We release now'}gradle.taskGraph.whenReady {taskGraph -&gt; if (taskGraph.hasTask(release)) { version = '1.0' } else { version = '1.0-SNAPSHOT' }} 44 Java 项目构建入门44.1 Java 插件比起 Maven 只能用于构建 JVM 项目，实际上 Gradle 是一个更加 general 的构建工具：在 Gradle 主页上我们也能看到 Gradle 甚至能用于构建 C++ 项目。Gradle 的强大来源于其使用 Groovy 脚本来定义构建逻辑，因此只要你写得出来，Gradle 就做得出来，但前提是你必须在构建脚本里写清楚 Gradle 该怎么做。 但对于同样类型的项目，比如同样是 Java 项目，每次创建一个新项目都要把编译、测试、打包等逻辑写到脚本里是很麻烦的，因此 Gradle 可以使用插件，其中就包括专门用于 Java 项目的 Java 插件。 插件通常通过引入一些预定义的任务来省去程序员编写脚本的功夫。比如说像编译、测试、打包这样十分常见的逻辑，在 Java 插件中就对应着 compile、test、jar 等预定义的任务。 Java 插件本身是基于惯例的，它会为项目默认指定一些配置，例如源代码文件和测试代码文件的位置等。如果项目没有遵循这些惯例也可以通过脚本来进行修改。实际上，由于 Gradle 的 Java 项目构建功能本身就是委托给 Java 插件的，如果实在有必要你甚至可以不使用 Java 插件以寻求最大的定制化。 44.2 基本的 Java 项目在构建文件中加入如下代码： 1apply plugin: 'java' 如此一来，Gradle 就知道这是一个 Java 项目并应用 Java 插件了，你在构建时也就可以使用 Java 插件预定义的任务了。你可以使用 gradle tasks 来查看由 Java 插件添加的任务。 Java 插件对项目的结构做出如下默认配置： 源代码文件位于 src/main/java 测试代码文件位于 src/test/java 所有位于 src/main/resources 的文件都会被作为资源文件复制到构建出的 JAR 文件中 所有位于 src/test/resources 的文件都会在运行测试时被添加到 classpath 中 所有的构建输出文件都会出现在 build 文件夹中，其中构建出的 JAR 文件会位于 build/libs 44.2.1 构建项目主要提到可以用 build 任务来构建 Java 项目。 除此之外还提到如下几个有用的任务： clean：删除 build 文件夹 assemble：编译并将代码打包成 JAR 但不执行任何单元测试。其他插件可能会为该任务添加更多的行为，如使用 War 插件时该任务还会为任务构建 WAR 文件 check：编译并测试代码。其他插件可能会为该任务添加更多的行为，如使用 checkstyle 插件时该任务还会对你的源代码执行代码风格检测 44.2.2 外部依赖同第七章。 44.2.3 项目自定义正如前面所说，Java 插件实际上为项目引入了大量的属性并基于惯例为这些属性赋了默认值。通常来讲这些默认值都足够用于普通的 Java 项目，但你也可以在脚本中改变这些属性值以实现项目定制化。 如如下代码： 12345678sourceCompatibility = 1.7version = '1.0'jar { manifest { attributes 'Implementation-Title': 'Gradle Quickstart', 'Implementation-Version': version }} 你可以通过 gradle properties 来查看项目的所有属性。 除此之外，由 Java 插件预定义的任务和其他在构建脚本中定义的任务没什么不同，你也可以在脚本中访问这些任务并对它们做出设置。具体设置方式详见第 14 章。 44.2.4 发布 JAR 文件同 7.6 节。 44.2.5 创建 Eclipse 项目使用 eclipse 插件： 1apply plugin: 'eclipse' 然后执行 gradle eclipse 命令即可生成 Eclipse 项目文件。详见第 63 章。 44.3 多项目 Java 构建44.3.1 定义多项目构建首先，多项目构建需要在根目录创建一个 settings.gradle 并指定包含的子项目： 1include &quot;shared&quot;, &quot;api&quot;, &quot;services:webservice&quot;, &quot;services:shared&quot; 详见第 24 章。 44.3.2 共用配置对于大多数的多项目构建而言，总有一些配置是各个子项目都相同的。这些配置可以被集中放在根项目的构建文件中，并使用名为“配置嵌入”的方式来将这些配置应用到每一个子项目。 见如下脚本配置： 123456789101112131415161718subprojects { apply plugin: 'java' apply plugin: 'eclipse-wtp' repositories { mavenCentral() } dependencies { testCompile 'junit:junit:4.12' } version = '1.0' jar { manifest.attributes provider: 'gradle' }} 上述代码所使用到的 subprojects 方法会遍历项目中的每一个子项目并应用给定的闭包，如此一来便能将闭包内的配置应用到每一个子项目。 值得注意的是，Java 插件的应用语句被放置在了 subprojects 里面而不是外面，如此一来 Gradle 便不会把根项目当做是一个 Java 项目并到那些预定义的地方寻找源代码文件了。 44.3.3 子项目间的依赖如下述代码所示： 123dependencies { compile project(':shared')} 16 编写构建脚本16.1 Gradle 构建语言Gradle 在 Groovy 的基础上开发出了一套专门用于便捷地进行构建任务的 DSL，因此在 Gradle 构建脚本中我们可以任意地使用 Groovy 语言。除此之外，Gradle 默认假设我们使用 UTF-8 编写脚本。 16.2 Project API对于构建中的每个项目，Gradle 都会为它们各自生成一个 Project 对象并将其与对应项目的构建脚本绑定起来，并在执行脚本时进行如下动作： 凡是对任何没有在该构建脚本中定义的方法进行调用时，该调用都会被委托给对应的 Project 对象 凡是对任何没有在该构建脚本中定义的属性进行访问时，该访问都会被委托给对应的 Project 对象 16.3 Script APIGradle 在执行脚本时实际上会把脚本内容放入到一个实现了 Script 接口的类中进行编译，因此你可以在你的脚本中使用所有由 Script 声明的属性和方法。 16.4 声明变量16.4.1 本地变量同 Groovy，使用 def 关键字声明本地变量。本地变量只能在其所属的作用域内访问。 16.4.2 额外属性我们可以通过部分由 Gradle 定义的类的 ext 属性为该对象添加更多的属性： 1234567891011121314151617181920212223242526apply plugin: &quot;java&quot;ext { springVersion = &quot;3.1.0.RELEASE&quot; emailNotification = &quot;build@master.org&quot;}sourceSets.all { ext.purpose = null }sourceSets { main { purpose = &quot;production&quot; } test { purpose = &quot;test&quot; } plugin { purpose = &quot;production&quot; }}task printProperties &lt;&lt; { println springVersion println emailNotification sourceSets.matching { it.purpose == &quot;production&quot; }.each { println it.name }} 结果如下： 12345&gt; gradle -q printProperties3.1.0.RELEASEbuild@master.orgmainplugin 有关额外属性以及其 API 的更多内容详见 ExtraPropertiesExtension 类的文档。 16.5 配置任意对象可以使用 configure 方法来配置任意对象： 123456def pos = configure(new java.text.FieldPosition(10)) { beginIndex = 1 endIndex = 5}println pos.beginIndexprintln pos.endIndex 16.6 使用其他脚本配置对象对对象的配置甚至还能放入到另一个 .gradle 文件中。 我们可以在 other.gradle 中输入： 123// Set properties.beginIndex = 1endIndex = 5 如下代码即可完成与上一节相同的配置： 12345def pos = new java.text.FieldPosition(10)// Apply the scriptapply from: 'other.gradle', to: posprintln pos.beginIndexprintln pos.endIndex 16.7 Groovy 基础介绍了 Groovy 的一些基本语法特性。可以去看我之前写过的 Groovy 相关的博文。 16.8 默认引入Gradle 脚本本身也会默认引入 Gradle 的包。引入数量较多，详见 16.8 小节。","link":"/gradle-note/"},{"title":"Java TreeMap 源码解析","text":"在本文中，我们将详细解析 java.util.TreeMap 的源代码。本文将首先讲述红黑树及其相关操作的基本原理，并结合 TreeMap 中的相关代码加深印象，继而再对 TreeMap 中的其他代码进行详析。 为提高可读性，本文中将会在保证功能不变的情况下对 TreeMap 的源代码进行一定的修改，包括修改变量名、新增局部变量、省略重复代码等。 2-3-4 树及红黑树首先我们看 java.util.TreeMap 的 JavaDoc 的第一句话： A Red-Black tree based NavigableMap implementation. 由此我们了解到，TreeMap 实现了 java.util.NavigableMap 接口，其本质上的数据结构是红黑树。因此首先我们需要了解什么是红黑树。 红黑树是一种自平衡的二叉搜索树（Self-balancing Binary Search Tree）。阅读过《Algorithms》一书的读者应该了解，红黑树的思想延伸自 2-3-4 树，它们本质上是等价的，红黑树只是在使用统一的二叉树结点的基础上加上结点的颜色信息来替代 2-3-4 树中特有的 3-结点和 4-结点，而红黑树的相关操作实际上也都一一对应着 2-3-4 树的操作。因此，本文将结合讲解 2-3-4 树的相关操作以加深读者对红黑树的理解。除此之外，2-3-4 树实际上就是一种特殊的 B 树，理解 2-3-4 树对读者后续理解 B 树也大有裨益。 通过阅读 2-3-4 树的维基百科可知，2-3-4 树是一种完美平衡的搜索树，除普通的二叉结点，或 2-结点外，还包含了分别有 3 个和 4 个子结点的 3-结点和 4-结点： 2-结点 3-结点 4-结点 而红黑树则是通过为结点间的链接赋予颜色属性来使用二叉结点表示 2-3-4 树。但在计算机语言实现中，难以表示链接（指针）的颜色，因此就将颜色信息储存在了结点中，以表示由父结点指向该结点的链接的颜色。当一个黑色父结点只有一个红色子结点时，这两个结点代表了 2-3-4 树中的一个 3-结点，两个结点的一共 3 个子结点对应 3-结点的 3 个子结点；当一个黑色父结点有两个红色子结点时，这三个节点代表了一个 4-结点，两个子结点的一共 4 个子结点则代表了 4-结点的 4 个子结点。 我们通过阅读红黑树的维基百科可以了解到，红黑树满足如下性质： 每个结点的颜色为红色或黑色 根节点的颜色为黑色 若某个结点的颜色为红色，那么它的两个孩子结点的颜色必为黑色，即不存在两对连续的红色结点 对于任意给定的内部结点，所有从该结点到位于其子树内的叶子结点的路径所经过的黑色结点数相同 其中，性质 1 定义了树结点的颜色属性；性质 2 本身意义并不大 —— 实际上根节点的颜色对红黑树的性质影响极小，该性质完全可以被忽略；而性质 3 和 4 则是确保一颗红黑树为平衡二叉搜索树的有力约束，同时红黑树的性质 4 实际上等价于等价 2-3-4 树的完美平衡性。除此之外，我们可以由性质 4 推得，根节点到最远叶结点的距离不大于由根节点到最近叶结点的距离的两倍，因此一棵红黑树只是在大致上是平衡的。 平衡树操作 —— 查找对于 2-3-4 树而言，其本质上也是一棵搜索树，在遇到 3-结点和 4-结点时只需要通过键的大小选择合适的子结点继续向下查找即可，思想上与一般的二叉搜索树无异。 对于红黑树来说，不考虑其自平衡的特性，其本身也是一棵二叉搜索树，因此红黑树的查找操作与一般二叉搜索树无异。形式化来说，红黑树的查找操作可如下表述： 如果树是空的，则查找未命中（返回 null）；如果被查找的键与根结点的键相等，查找命中（返回对应值）；否则就（递归地）在合适的子树中继续查找：如果被查找的键较小就选择左子树，较大则选择右子树。 TreeMap 的 get(K key) 方法实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public V get(Object key) { Entry&lt;K, V&gt; p = getEntry(key); return (p == null ? null : p.value);}final Entry&lt;K, V&gt; getEntry(Object key) { // Offload comparator-based version for sake of performance if (comparator != null) return getEntryUsingComparator(key); if (key == null) throw new NullPointerException(); @SuppressWarnings(&quot;unchecked&quot;) Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; Entry&lt;K, V&gt; p = root; while (p != null) { int cmp = k.compareTo(p.key); if (cmp &lt; 0) p = p.left; else if (cmp &gt; 0) p = p.right; else return p; } return null;}final Entry&lt;K, V&gt; getEntryUsingComparator(Object key) { @SuppressWarnings(&quot;unchecked&quot;) K k = (K) key; Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) { Entry&lt;K, V&gt; p = root; while (p != null) { int cmp = cpr.compare(k, p.key); if (cmp &lt; 0) p = p.left; else if (cmp &gt; 0) p = p.right; else return p; } } return null;} 除去针对自定义 Comparator 或 Comparable 键的独立实现外，TreeMap#get 方法的实现并无太特别的地方。 平衡树操作 —— 插入自平衡二叉树的自平衡机制在结点数量发生变化时便会被触发，具体来说就是插入结点和删除结点的时候。 2-3-4 树的插入操作可形式化地描述如下： 首先从 2-3-4 树的根节点递归地向下寻找插入的位置： 如果当前结点为一个 4-结点： 暂存位于 4-结点中间的键值，并将其移除，以将该结点变为一个 3-结点 将该 3-结点分裂为两个 2-结点 如果该结点为根节点，那么就用暂存的中间键值创建一个新的 2-结点链接分裂得出的两个 2-结点，并将该新结点作为新的根节点。树的高度增加 1，回到根节点继续递归 否则就将暂存的中间键值插入到父结点（2-结点变 3-结点，3-结点变 4-结点），并由该父结点链接分裂得出的两个 2-结点。树的高度增加 1，回到父节点继续递归 继续查找其区间包含待插入键值的子结点 如果该子结点为叶子结点，便直接将键值插入到该结点中，结束递归 否则，进入对应的子结点并重复递归 大致的过程可以参考这里。 对于红黑树而言，在插入前，我们采用与一般二叉搜索树相同的机制寻找新叶结点插入的位置，并在插入后将其颜色置为红色，即对应 2-3-4 树中将子结点升阶的一般操作（2-结点变 3-结点，3-结点变 4-结点）。值得注意的是，该动作不会使红黑树的性质 4 失效（新插入的红色结点不影响任意路径经过的黑色结点数），但有可能使性质 3 失效（父结点同为红色结点时，出现连续的两个红色结点）。此时为使性质 3 重新成立，具体采用什么动作则需要分情况讨论。 深刻了解红黑树操作的读者可以直接看后面的总结部分。 在详细分析每种情形前，我们先来看看 TreeMap#put 方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public V put(K key, V value) { Entry&lt;K, V&gt; t = root; if (t == null) { // 树为空，将新结点作为根结点 compare(key, key); // type (and possibly null) check root = new Entry&lt;&gt;(key, value, null); size = 1; modCount++; return null; } int cmp; Entry&lt;K, V&gt; parent; Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) { // 使用 Comparator 定位插入位置 do { parent = t; cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); // 在内部结点中找到了相同的键，直接替换结点的值 } while (t != null); } else { // 使用 Comparable#compareTo 方法定位插入位置。效果与上述代码等价 // ... } Entry&lt;K, V&gt; e = new Entry&lt;&gt;(key, value, parent); // 将新结点插入叶结点的对应链接 if (cmp &lt; 0) parent.left = e; else parent.right = e; // 修复红黑树 fixAfterInsertion(e); size++; modCount++; return null;}private void fixAfterInsertion(Entry&lt;K, V&gt; n) { n.color = RED; // 新结点置为红色 while (n != null &amp;&amp; n != root &amp;&amp; n.parent.color == RED) { // 向上递归修复直至遇到黑色结点 if (parentOf(n) == leftOf(parentOf(parentOf(n)))) { // ... } else { // ... } } root.color = BLACK; // 根结点置为黑色} 为方便表述，下文将用叔父结点（Uncle Node）来表示某个结点的父结点的兄弟结点（Sibling Node）。 红黑树插入情形 1：插入空树当插入前红黑树为空时，需将新的结点作为红黑树的根结点。该情形已由 TreeMap#put 方法本身所处理。 红黑树插入情形 2：父结点为黑色如果插入了新结点后，新结点的父结点为黑色，对应于 2-3-4 树中将新键值插入到了 2-结点使其成为 3-结点的情形。这实际上不会破坏 2-3-4 树的平衡性，新的结点不是 4-结点也不需要进行分裂，也没有违反红黑树的性质 3。此外，性质 4 也没有违反，因为新插入的结点为红色结点，不会影响根结点到任意叶子结点所经过的黑色结点的数量。 TreeMap#fixAfterInsertion 方法的内部 while 循环会先判断当前扫描结点的父结点颜色是否为红色。当父结点颜色为黑色时，控制流不会进入 while 循环体。 红黑树插入情形 3：父结点与叔父结点均为红色结点由于父结点与新插入的叶子结点同为红色，性质 3 不再成立，此时便需要启动修复。实际上，由于修复过程是自底向上递归进行的，是否进入该情形与当前结点 $N$ 的颜色无关，但 $N$ 实际上却是只会是红色的。这种情形对应于 2-3-4 树中遇到 4-结点的情形（由 $P$、$U$、$G$ 共同组成的 4-结点），因此此时我们需要对 4-结点进行分裂，并把中间键值（$G$）插入到上一级结点中。 实际上，我们只需要将父结点和叔父结点的颜色反转（置为黑色），并将它们的父结点（新叶子结点的爷爷结点）置为红色即可使性质 3 和性质 4 在该子树内重新成立。如下图所示： 若如图中所示，修复后，爷爷结点 $G$ 变为红色，代表插入到了上一层的结点中，而 $P$ 和 $U$ 变为黑色，代表成为了分裂后的 2-结点，而红色的 $N$ 结点也代表了新键值插入 2-结点产生 3-结点的过程。若爷爷结点 $G$ 为红黑树的根结点，那么即修复成功。若 $G$ 不是根结点，那么我们只能确保性质 3 和 4 在以 $G$ 为根的子树内成立，在 $G$ 往上的树中并不一定成立，因此我们需要继续从 $G$ 开始递归地向上修复。 TreeMap#fixAfterInsertion 方法中的相关代码如下： 12345678910111213141516while (n != null &amp;&amp; n != root &amp;&amp; n.parent.color == RED) { if (parentOf(n) == leftOf(parentOf(parentOf(n)))) { // g = parentOf(parentOf(n)) Entry&lt;K, V&gt; u = rightOf(parentOf(parentOf(n))); if (colorOf(u) == RED) { setColor(parentOf(n), BLACK); // p = parentOf(n) setColor(u, BLACK); setColor(parentOf(parentOf(n)), RED); n = parentOf(parentOf(n)); } else { // ... } } else { // 与上半部分代码相对称 }} 子树的左旋转与右旋转对接下来的两种不平衡情形的修复涉及到了对子树的旋转操作。旋转操作会保持二叉搜索树性质的情况下反转指定父子结点的父子关系，即原本的子结点升级为父结点，父结点降级为子结点，而两个结点原本的其他子结点及子树则基于一定的规则重新分配。 以左旋转为例，父结点 P 与其右子结点 Q 一同逆时针旋转，原本的父结点 P 将变为右子结点 Q 的左子结点。除此之外，Q 原本的左子结点 B 将成为 P 的左子结点。如下图所示： 右旋转则为左旋转的逆过程。 在 TreeMap 中，左旋转与右旋转分别对应于方法 rotateLeft 和 rotateRight，代码如下： 123456789101112131415161718192021222324252627282930313233private void rotateLeft(Entry&lt;K, V&gt; p) { if (p != null) { Entry&lt;K, V&gt; r = p.right; p.right = r.left; if (r.left != null) r.left.parent = p; r.parent = p.parent; if (p.parent == null) root = r; else if (p.parent.left == p) p.parent.left = r; else p.parent.right = r; r.left = p; p.parent = r; }}private void rotateRight(Entry&lt;K, V&gt; p) { if (p != null) { Entry&lt;K, V&gt; l = p.left; p.left = l.right; if (l.right != null) l.right.parent = p; l.parent = p.parent; if (p.parent == null) root = l; else if (p.parent.right == p) p.parent.right = l; else p.parent.left = l; l.right = p; p.parent = l; }} 红黑树插入情形 4：叔父结点为黑色结点，当前结点与其父结点及爷爷结点间形成左左结构在该情形下，父结点为红色结点，但叔父结点为黑色结点。除此之外，当前结点为父结点的左子结点，而父结点同为爷爷结点的左子结点，即这三代结点形成了左左结构。 此时，我们需要将父结点与爷爷结点的颜色互换，并以爷爷结点为锚点执行右旋转操作。如下图所示： 对于 2-3-4 树而言，原本的两个连续的红色结点无法代表 2-3-4 树的结点，但在经过旋转和变色后，$N$、$P$、$G$ 则重新构成了一个 4-结点，因此我们仍需以上升后的 $N$ 结点为起点向上递归修复，触发下一次的 4-结点分裂。 若三代结点间形成的是右右结构，我们只需要轴对称地执行左旋转操作即可。 TreeMap#fixAfterInsertion 方法中的相关代码如下： 1234567891011121314151617while (n != null &amp;&amp; n != root &amp;&amp; n.parent.color == RED) { if (parentOf(n) == leftOf(parentOf(parentOf(n)))) { Entry&lt;K, V&gt; u = rightOf(parentOf(parentOf(n))); if (colorOf(u) == RED) { // ... } else { if (n == rightOf(parentOf(n))) { // ... } setColor(parentOf(n), BLACK); setColor(parentOf(parentOf(n)), RED); rotateRight(parentOf(parentOf(n))); } } else { // 与上半部分轴对称 }} 红黑树插入情形 5：叔父结点为黑色结点，当前结点与其父结点及爷爷结点间形成左右结构在该情形下，父结点为红色结点，但叔父结点为黑色结点。除此之外，当前结点为父结点的右子结点，而父结点则为爷爷结点的左子结点，即这三代结点形成了左右结构。 此时，我们需要以父结点为锚点执行左旋转操作。如下图所示： 实际上这项操作并未使得子树得以修复，而是使得当前的结构进入了情形 4，因此我们需要再次执行情形 4 对应的修复动作。同理，原本的两个连续的红色结点无法代表 2-3-4 树的结点，因此我们需要进入情形 4 再执行对应修复，以重新得到一个合法的红黑树。 若三代结点间形成的是右左结构，我们只需要轴对称地执行右旋转即可。 TreeMap#fixAfterInsertion 方法中的相关代码如下： 12345678910111213141516171819while (n != null &amp;&amp; n != root &amp;&amp; n.parent.color == RED) { if (parentOf(n) == leftOf(parentOf(parentOf(n)))) { Entry&lt;K, V&gt; u = rightOf(parentOf(parentOf(n))); if (colorOf(u) == RED) { // ... } else { if (n == rightOf(parentOf(n))) { // 进入情形 5 n = parentOf(n); rotateLeft(n); } // 进入情形 4，继续修复 setColor(parentOf(n), BLACK); setColor(parentOf(parentOf(n)), RED); rotateRight(parentOf(parentOf(n))); } } else { // 与上半部分轴对称 }} 总结红黑树插入修复的总结如下： # 情形 处理 示意图 2 $P$ 为黑色结点 结束修复 3 $P$ 和 $U$ 均为红色结点 将 $P$、$U$、$G$ 的颜色反转。 $G$ 的颜色变为红色，故从 $G$ 开始继续往上执行相同的修复过程 4 $U$ 为黑色结点，$G$、$P$、$N$ 形成左左结构 $P$、$G$ 颜色互换，并以 $G$ 进行右旋转 5 $U$ 为黑色结点，$G$、$P$、$N$ 形成左右结构 以 $P$ 进行左旋转，从 $P$ 开始继续修复，进入情形 4 TreeMap#fixAfterInsertion 方法的总结如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344private void fixAfterInsertion(Entry&lt;K, V&gt; n) { n.color = RED; // 新结点置为红色 while (n != null &amp;&amp; n != root &amp;&amp; n.parent.color == RED) { Entry&lt;K, V&gt; p = parentOf(n); Entry&lt;K, V&gt; g = parentOf(p); if (p == leftOf(g)) { Entry&lt;K, V&gt; u = rightOf(g); if (colorOf(u) == RED) { // 进入情形 3，进行颜色反转 setColor(p, BLACK); setColor(u, BLACK); setColor(g, RED); // 递进至爷爷结点，即新出现的红色结点，继续向上修复 n = g; } else { if (n == rightOf(p)) { // 进入情形 5，以父结点进行左旋转 n = p; rotateLeft(n); } setColor(p, BLACK); // 进入情形 4。设置颜色后，以爷爷结点进行右旋转 setColor(g, RED); rotateRight(g); } } else { // 与上半部分代码轴对称 Entry&lt;K, V&gt; u = leftOf(g); if (colorOf(u) == RED) { // 进入情形 3 setColor(p, BLACK); setColor(u, BLACK); setColor(g, RED); n = g; } else { if (n == leftOf(p)) { // 进入情形 5，以父结点进行右旋转 n = p; rotateRight(n); } setColor(p, BLACK); // 进入情形 4。设置颜色后，以爷爷结点进行左旋转 setColor(g, RED); rotateLeft(g); } } } root.color = BLACK // 将根结点置为黑色} 平衡树操作 —— 删除与插入操作同理，在删除前，我们采用与一般二叉搜索树相同的方法查找需要删除的元素。与之不同的是，我们不会直接执行原地删除操作，因为待删除的结点有可能是内部结点，而新插入的结点只可能是叶子结点，因此要完成删除操作还需要一些特殊的预处理。 当删除内部结点时，我们采取与一般二叉搜索树相同的方法，先将其与其后继结点（Successor Node）互换（颜色保持不变），再删除该后继结点。后继结点通常指大于该结点的最小结点或小于该结点的最大结点。对于一个同时有着左右子结点的结点来说，其后继结点即为位于其右子树最左下角的结点或位于其左子树最右下角的结点。因此在完成该预处理后，我们只需要考虑待删除结点只有一个子结点或没有子结点（叶子结点）两种情况。 对于一般的二叉搜索树而言，删除有一个子结点的内部结点只需要让其父结点的对应链接直接指向该子结点即可。而删除叶子结点的操作则更是简单了。 首先我们考虑 2-3-4 树的删除操作。形式化的描述如下： 查找待删除的键值 如果键值所处结点不是叶子结点，那就继续向下递归寻找其后续结点，同时在下沉的过程中对结点进行调整，以确保所找到的后续结点不是一个 2-结点 如果键值所处的结点是一个 2-叶子结点，那就对结点进行相同的调整 在下降的过程中对沿途所有非根结点的 2-结点 $N$ 作如下调整： 如果父结点 $P$ 和兄弟结点 $S$ 均为 2-结点，那就将 $N$、$P$、$S$ 组合成一个 4-结点，树的高度减少 1。实际上，这种情况只有在父结点 $P$ 同为根结点时才会发生，因为其他 2-结点在下降的过程中早已被转换 如果有一个位于该结点左侧或右侧的兄弟 3-结点或 4-结点 $S$（即包含多于一个的键值），那么就与该兄弟结点进行旋转操作： 将兄弟结点 $S$ 距离该结点 $N$ 最近的键值上升到两个结点的父结点 $P$ 中 父结点 $P$ 原有的键值下降到该 2-结点 $N$ 中以形成一个 3-结点 原本属于上升至父结点的键值的子结点现在成为 $N$ 的新的子结点 如果父结点 $P$ 为 3-结点或 4-结点且所有兄弟结点均为 2-结点，那么就将 $N$、$P$ 以及其最近的兄弟结点 $S$ 执行混合操作： 利用 $N$ 的键值、 $S$ 的键值以及 $P$ 中位于 $N$ 和 $S$ 链接交合处的键值（一共三个键值）组成一个 4-结点 将 $S$ 原本的子结点变为该结点的子结点 如此一来，待删除键值所处的叶子结点便不是 2-结点（包含多于一个键值），可以安全地将其移除并对该叶子结点进行降阶，同时不影响 2-3-4 树的平衡性。该过程结合《Algorithms》一书的图 3.3.26 应该会更好理解。（见右侧） 但对于红黑树而言，如果被移除的结点是一个黑色结点则有可能使得性质 3 和 4 不再成立，因此需要在此时对红黑树进行修复。 我们先对可能发生的情况进行讨论： 待删除的结点为红色结点：这种情况对应于 2-3-4 树中的一般结点降阶过程。那么由性质 3 可知，它的父结点和子结点必为黑色结点，而从红黑树内部移除一个红色结点不会改变根结点到任意叶子结点所经过黑色结点的数量，因此性质 4 也不会变化，在这种情况下我们不需要进行任何修复； 待删除结点为黑色结点，其子结点为红色结点：这种情况同样对应于 2-3-4 树中的一般结点降阶过程。但由它的红色子结点会顶替它原有的位置，这会导致红黑树的性质 4 不成立（通过该结点的所有路径所经过的黑色结点数减少 1），此时我们只需要将该子结点置为黑色即可使性质 4 重新成立。 TreeMap#remove 方法的相关代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public V remove(Object key) { Entry&lt;K, V&gt; p = getEntry(key); // 查找待删除的结点 if (p == null) // 结点不存在，返回 return null; V oldValue = p.value; deleteEntry(p); // 删除结点 return oldValue;}private void deleteEntry(Entry&lt;K, V&gt; p) { modCount++; size--; if (p.left != null &amp;&amp; p.right != null) { // 待删除结点是有两个子结点的内部结点 Entry&lt;K, V&gt; s = successor(p); // 与后继结点替换 p.key = s.key; p.value = s.value; p = s; // 开始删除该后继结点 } // 到这里，p 所指向的结点只可能有 1 个或 0 个子结点 Entry&lt;K, V&gt; replacement = (p.left != null ? p.left : p.right); // 获取其子结点作为替换结点 if (replacement != null) { // 替换结点不为空，即 p 有一个子结点 replacement.parent = p.parent; // 用替换结点替换 p if (p.parent == null) // p 为根结点 root = replacement; else if (p == p.parent.left) p.parent.left = replacement; else p.parent.right = replacement; p.left = p.right = p.parent = null; if (p.color == BLACK) // 若被删除的结点为黑色结点，执行自平衡修复 fixAfterDeletion(replacement); } else if (p.parent == null) { // p 为叶子结点，且 p 为根结点，即此时红黑树只有一个结点 root = null; } else { // p 为叶子结点 if (p.color == BLACK) // 若被删除的结点为黑色结点，执行自平衡修复 fixAfterDeletion(p); if (p.parent != null) { // 移除 p if (p == p.parent.left) p.parent.left = null; else if (p == p.parent.right) p.parent.right = null; p.parent = null; } }} 我们可以看一下 TreeMap#fixAfterDeletion 方法的循环结束条件： 123456private void fixAfterDeletion(Entry&lt;K, V&gt; n) { while (n != root &amp;&amp; colorOf(n) == BLACK) { // 当 n 为根结点或红色结点时结束循环 // ... } setColor(n, BLACK); // 对于上面提到的第二种情形，红色子结点会在这里被置为黑色} 接下来我们就需要分情况讨论待删除结点及其子结点同为黑色结点的情况了，即在 2-3-4 树中删除 2-结点的情况。出于方便，接下来我们将作为修复起点的结点称为 $N$，其父结点为 $P$，兄弟结点为 $S$，并有 $S$ 结点的左右子结点分别为 $S_L$ 和 $S_R$。 红黑树删除情形 1：兄弟结点 S 为红色结点在这种情况下，我们需要以父结点 $P$ 为锚点进行左旋转，让兄弟结点 $S$ 变为当前结点 $N$ 的爷爷结点，再将 $P$ 置为红色，$S$ 置为黑色即可。如下图所示： TreeMap#fixAfterDeletion 方法的相关代码如下： 1234567891011121314151617181920212223242526private void fixAfterDeletion(Entry&lt;K, V&gt; n) { Entry&lt;K, V&gt; p = parentOf(n); while (n != root &amp;&amp; colorOf(n) == BLACK) { if (n == leftOf(p)) { Entry&lt;K, V&gt; s = rightOf(p); Entry&lt;K, V&gt; s_l = leftOf(s); Entry&lt;K, V&gt; s_r = rightOf(s); if (colorOf(s) == RED) { // 进入情形 1 setColor(s, BLACK); setColor(p, RED); rotateLeft(p); s = rightOf(p); s_l = leftOf(s); s_r = rightOf(s); } // ... } else { // 与上半部分代码轴对称 // ... } } setColor(n, BLACK);} 此时我们并未完成修复，因为由于黑色结点被删除，经过 $N$ 的路径仍然比经过 $S_L$ 的路径少一个黑色结点，因此我们仍然需要以 $N$ 为起点进行修复，但此时则进入了其他情形（取决于原 $S_L$ 的颜色）。 红黑树删除情形 2：兄弟结点及其子结点均为黑色结点此时，由于被删除结点的关系，经过结点 $N$ 的路径少了一个黑色结点。此时我们需要将兄弟结点 $S$ 置为红色，如下图所示： 这对应于在 2-3-4 树中将两个 2-结点合并为一个 3-结点的过程。即便如此，我们也只是确保了性质 4 在以 $P$ 为根的子树内成立，我们仍需要继续以 $P$ 为起点向上修复。 TreeMap#fixAfterDeletion 方法中的相关代码如下： 123456789101112131415161718192021222324252627private void fixAfterDeletion(Entry&lt;K, V&gt; n) { Entry&lt;K, V&gt; p = parentOf(n); while (n != root &amp;&amp; colorOf(n) == BLACK) { if (n == leftOf(p)) { Entry&lt;K, V&gt; s = rightOf(p); Entry&lt;K, V&gt; s_l = leftOf(s); Entry&lt;K, V&gt; s_r = rightOf(s); if (colorOf(s) == RED) { // ... } if (colorOf(s_l) == BLACK &amp;&amp; colorOf(s_r) == BLACK) { // 进入情形 2 setColor(s, RED); // 兄弟结点置为红色 n = p; // 继续从父结点开始向上修复 } else { // ... } } else { // 与上半部分代码轴对称 // ... } } setColor(n, BLACK);} 红黑树删除情形 3：兄弟结点及其右子结点为黑色结点，其左子结点为红色结点此时，我们以 $S$ 为锚点执行右旋转，并将 $S_L$ 与 $S$ 的颜色互换，继而进入情形 4。如下图所示： TreeMap#fixAfterDeletion 方法中的相关代码如下： 123456789101112131415161718192021222324252627282930313233private void fixAfterDeletion(Entry&lt;K, V&gt; n) { Entry&lt;K, V&gt; p = parentOf(n); while (n != root &amp;&amp; colorOf(n) == BLACK) { if (n == leftOf(p)) { Entry&lt;K, V&gt; s = rightOf(p); Entry&lt;K, V&gt; s_l = leftOf(s); Entry&lt;K, V&gt; s_r = rightOf(s); if (colorOf(s) == RED) { // ... } if (colorOf(s_l) == BLACK &amp;&amp; colorOf(s_r) == BLACK) { // ... } else { if (colorOf(s_r) == BLACK) { // 进入情形 3 setColor(s_l, BLACK); setColor(s, RED); rotateRight(s); s = rightOf(p); s_l = leftOf(s); s_r = rightOf(s); } // ... } } else { // 与上半部分代码轴对称 // ... } } setColor(n, BLACK);} 红黑树删除情形 4：兄弟结点及其左子结点为黑色结点，其右子结点为红色结点在这种情况下，我们在结点 $P$ 上做左旋转，并互换 $P$ 和 $S$ 的颜色，再将 $S_R$ 置为黑色。 如此一来，由于 $N$ 新增了一个新的黑色父结点 $P$，原本经过 $N$ 的路径的黑色结点数得到了恢复，同时由于 $S_R$ 的颜色变为了黑色，经过 $S_R$ 的路径的黑色结点数也没有变化。 1234567891011121314151617181920212223242526272829303132private void fixAfterDeletion(Entry&lt;K, V&gt; n) { Entry&lt;K, V&gt; p = parentOf(n); while (n != root &amp;&amp; colorOf(n) == BLACK) { if (n == leftOf(p)) { Entry&lt;K, V&gt; s = rightOf(p); Entry&lt;K, V&gt; s_l = leftOf(s); Entry&lt;K, V&gt; s_r = rightOf(s); if (colorOf(s) == RED) { // ... } if (colorOf(s_l) == BLACK &amp;&amp; colorOf(s_r) == BLACK) { // ... } else { if (colorOf(s_r) == BLACK) { // ... } setColor(s, colorOf(p)); setColor(p, BLACK); setColor(s_r, BLACK); rotateLeft(p); n = root; // 结束循环 } } else { // 与上半部分代码轴对称 // ... } } setColor(n, BLACK);} 总结我们定义待删除的结点为 $D$、其唯一子结点为 $N$、父结点为 $P$、爷爷结点为 $G$、兄弟结点为 $S$，那么红黑树删除修复可总结如下： # 情形 处理 示意图 $D$ 为红色结点 无需修复 $D$ 为黑色结点，$N$ 为红色结点 $N$ 置为黑色，结束修复 1 $D$、$N$ 为黑色结点，$S$ 为红色结点 以 $P$ 进行左旋转，$P$、$S$ 进行颜色互换，使 $N$ 的父结点为红色结点 TreeMap#fixAfterDeletion 方法的总结如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970private void fixAfterDeletion(Entry&lt;K, V&gt; n) { Entry&lt;K, V&gt; p = parentOf(n); while (n != root &amp;&amp; colorOf(n) == BLACK) { if (n == leftOf(p)) { Entry&lt;K, V&gt; s = rightOf(p); Entry&lt;K, V&gt; s_l = leftOf(s); Entry&lt;K, V&gt; s_r = rightOf(s); if (colorOf(s) == RED) { // 进入情形 1，以 P 进行左旋转 setColor(s, BLACK); setColor(p, RED); rotateLeft(p); s = rightOf(p); s_l = leftOf(s); s_r = rightOf(s); } if (colorOf(s_l) == BLACK &amp;&amp; colorOf(s_r) == BLACK) { // 进入情形 2，将 S 置为红色，向上递归 setColor(s, RED); n = p; } else { if (colorOf(s_r) == BLACK) { // 进入情形 3，以 S 进行右旋转 setColor(s_l, BLACK); setColor(s, RED); rotateRight(s); s = rightOf(p); } setColor(s, colorOf(p)); // 进入情形 4，以 P 进行左旋转，结束修复 setColor(p, BLACK); setColor(s_r, BLACK); rotateLeft(p); n = root; } } else { // 与上半部分代码轴对称 Entry&lt;K, V&gt; s = leftOf(p); Entry&lt;K, V&gt; s_l = leftOf(s); Entry&lt;K, V&gt; s_r = rightOf(s); if (colorOf(s) == RED) { // 进入情形 1，以 P 进行右旋转 setColor(s, BLACK); setColor(p, RED); rotateRight(p); s = leftOf(p); s_l = leftOf(s); s_r = rightOf(s); } if (colorOf(s_r) == BLACK &amp;&amp; colorOf(s_l) == BLACK) { // 进入情形 2，将 S 置为红色，向上递归 setColor(s, RED); n = p; } else { if (colorOf(s_l) == BLACK) { // 进入情形 3，以 S 进行左旋转 setColor(s_r, BLACK); setColor(s, RED); rotateLeft(s); s = leftOf(p); } setColor(s, colorOf(p)); // 进入情形 4，以 P 进行右旋转，结束修复 setColor(p, BLACK); setColor(s_l, BLACK); rotateRight(p); n = root; } } } setColor(n, BLACK);} 没理解为什么这些东西要这么做？没关系，我也不理解，可能写这些代码的人自己也不理解 =。= 实际上这些 private 方法大多数都有 /** From CLR */ 的注释，在 TreeMap.java 中也能找到这么一段注释： 123456789/** * Balancing operations. * * Implementations of rebalancings during insertion and deletion are * slightly different than the CLR version. Rather than using dummy * nilnodes, we use a set of accessors that deal properly with null. They * are used to avoid messiness surrounding nullness checks in the main * algorithms. */ 因此，此 CLR 指的应该就是 Common Language Runtime，这些自平衡代码也很有可能是从 C# 那边“借”来的。详见这里。不管怎么说，红黑树都是很复杂的数据结构，如果你不能完全记忆这些操作那就罢了，没什么必要。","link":"/java_collection_treemap/"},{"title":"Raft 总结","text":"这篇文章是本人按照 MIT 6.824 的课程安排阅读《In Search of an Understandable Consensus Algorithm》一文以及相关课程资料并总结而来。 这篇论文详细介绍了斯坦福大学研究人员为解决 Paxos 难度过高难以理解而开发出的一个名为 Raft 的分布式共识算法。接下来我会结合课程给出的资料给大家总结论文中的主要内容，然后会给出课程所给出的论文 FAQ 的部分翻译。 背景在上一篇论文阅读中我们已经了解到，为了令进程实现高可用，我们可以对进程进行备份，而实现进程的主从备份有两种方法： State Transfer（状态转移）：主服务器将状态的所有变化都传输给备份服务器 Replicated State Machine（备份状态机）：将需要备份的服务器视为一个确定性状态机 —— 主备以相同的状态启动，以相同顺序导入相同的输入，最后它们就会进入相同的状态、给出相同的输出 其中 Replicated State Machine 是较为常用的主从备份实现方式。常见的 Replicated State Machine 架构如下： 客户端向服务发起请求，执行指定操作 共识模块将该操作以日志的形式备份到其他备份实例上 当日志安全备份后，指定操作被应用于上层状态机 服务返回操作结果至客户端 由此，我们很容易得出结论：在 Replicated State Machine 中，分布式共识算法的职责就是按照固定的顺序将指定的日志内容备份到集群的其他实例上。包括我们在上一篇论文阅读中提到的 VMWare FT 协议、广为人知的 Paxos 协议以及这次我们即将学习的 Raft 协议，它们完成的工作都是如此。 在 Raft 协议出现（2014 年）之前，Paxos 协议几乎成了分布式共识算法的唯一标准：有着大量以 Paxos 为基础开发的正在实际使用中的分布式共识算法，也有着大量与 Paxos 相关的文献，MIT 6.824 也是直到 2016 年才从 Paxos 改为教学 Raft。即便如此，Paxos 算法的名声也不算好，它的复杂程度广为人知。为此，斯坦福大学的研究人员研究了很多方法来简化 Paxos，最终主要通过问题拆分、状态空间降维等方式完成了简化，由此诞生出 Raft 算法。 好了，说了这么多，我们就来看看 Raft 到底有多简单吧。 Raft 性质与集群交互在行文上，Raft 的论文首先在图 2 和图 3 中给出了 Raft 算法组成的简单描述以及 Raft 所能为系统提供的性质。首先我们先来说说 Raft 所提供的性质： Election Safety（选举安全）：在任意给定的 Term 中，至多一个节点会被选举为 Leader Leader Append-Only（Leader 只追加）：Leader 绝不会覆写或删除其所记录的日志，只会追加日志 Log Matching（日志匹配）：若两份日志在给定 Term 及给定 index 值处有相同的记录，那么两份日志在该位置及之前的所有内容完全一致 Leader Completeness（Leader 完整性）：若给定日志记录在某一个 Term 中已经被提交（后续会解释何为“提交”），那么后续所有 Term 的 Leader 都将包含该日志记录 State Machine Safety（状态机安全性）：如果一个服务器在给定 index 值处将某个日志记录应用于其上层状态机，那么其他服务器在该 index 值处都只会应用相同的日志记录 一个 Raft 集群由若干个节点组成。节点可能处于以下三种角色的其中之一：Leader、Follower 或 Candidate，职责分别如下： Leader 负责从客户端处接收新的日志记录，备份到其他服务器上，并在日志安全备份后通知其他服务器将该日志记录应用到位于其上层的状态机上 Follower 总是处于被动状态，接收来自 Leader 和 Candidate 的请求，而自身不会发出任何请求 Candidate 会在 Leader 选举时负责投票选出 Leader 在采用 Leader-Follower 架构的语境下，Raft 将其需要解决的共识问题拆分为了以下 3 个问题： Leader 选举：已有 Leader 失效后需要选举出一个新的 Leader 日志备份：Leader 从客户端处接收日志记录，备份到其他服务器上 安全性：如果某个服务器为其上层状态机应用了某个日志记录，那么其他服务器在该 index 值处则不能应用其他不同的日志记录 Raft 算法在运行时会把时间分为任意长度的 Term，如文中图 5 所示： 每个 Term 的开头都会包含一次 Leader 选举，在选举中胜出的节点会担当该 Term 的 Leader。 Term 由单调递增的 Term ID 所标识，每个节点都会在内存中保存当前 Term 的 ID。每次节点间发生通信时，它们都会发出自己所保存的 Term ID；当节点从其他节点处接收到比自己保存的 Term ID 更大的 Term ID 值时，它便会更新自己的 Term ID 并进入 Follower 状态。在 Raft 中，节点间通信由 RPC 实现，主要有 RequestVote 和 AppendEntries 两个 RPC API，其中前者由处于选举阶段的 Candidate 发出，而后者由 Leader 发出。 整个集群在运行时会持有如下状态信息： 所有节点都会持有的持久化状态信息（在响应 RPC 前会先将更新写入到持久存储）： currentTerm：当前 Term ID（初值为 0） votedFor: 该 Term 中已接收到来自该节点的选票的 Candidate ID log[]: 日志记录。第一个日志记录的 index 值为 1 所有节点都会持有的易失性状态信息： commitIndex: 最后一个已提交日志记录的 index（初值为 0） lastApplied: 最后一个已应用至上层状态机的日志记录的 index（初值为 0） Leader 才会持有的易失性状态信息（会在每次选举完成后初始化）： nextIndex[]: 每个节点即将为其发送的下一个日志记录的 index（初值均为 Leader 最新日志记录 index 值 + 1） matchIndex[]: 每个节点上已备份的最后一条日志记录的 index（初值均为 0） 在 Raft 集群中，节点间的交互主要由两种 RPC 调用构成。 首先是用于日志备份的 AppendEntries： AppendEntries RPC：由 Leader 进行调用，用于将日志记录备份至 Follower，同时还会被用来作为心跳信息 参数： term: Leader 的 Term ID leaderId: Leader 的 ID prevLogIndex: 在正在备份的日志记录之前的日志记录的 index 值 prevLogTerm: 在正在备份的日志记录之前的日志记录的 Term ID entries[]: 正在备份的日志记录 leaderCommmit: Leader 已经提交的最后一条日志记录的 index 值 返回值： term: 接收方的当前 Term ID success: 当 Follower 能够在自己的日志中找到 index 值和 Term ID 与 prevLogIndex 和 prevLogTerm 相同的记录时为 true 接收方在接收到该 RPC 后会进行以下操作： 若 term &lt; currentTerm，返回 false 若日志中不包含index 值和 Term ID 与 prevLogIndex 和 prevLogTerm 相同的记录，返回 false 如果日志中存在与正在备份的日志记录相冲突的记录（有相同的 index 值但 Term ID 不同），删除该记录以及之后的所有记录 在保存的日志后追加新的日志记录 若 leaderCommit &gt; commitIndex，令 commitIndex 等于 leaderCommit 和最后一个新日志记录的 index 值之间的最小值 而后是用于 Leader 选举的 RequestVote： RequestVote RPC：由 Candidate 调起以拉取选票 参数： term：Candidate 的 Term ID candidateId: Candidate 的 ID lastLogIndex: Candidate 所持有的最后一条日志记录的 index lastLogTerm: Candidate 所持有的最后一条日志记录的 Term ID 返回值： term：接收方的 Term ID voteGranted：接收方是否同意给出选票 接收方在接收到该 RPC 后会进行以下操作： 若 term &lt; currentTerm，返回 false 若 votedFor == null 且给定的日志记录信息可得出对方的日志和自己的相同甚至更新，返回 true 最后，Raft 集群的节点还需要遵循以下规则： 对于所有节点： 若 commitIndex &gt; lastApplied，则对 lastApplied 加 1，并将 log[lastApplied] 应用至上层状态机 若 RPC 请求或相应内容中携带的 term &gt; currentTerm，则令 currentTerm = term，且 Leader 降级为 Follower 对于 Follower： 负责响应 Candidate 和 Leader 的 RPC 如果在 Election Timeout 之前没能收到来自当前 Leader 的 AppendEntries RPC 或将选票投给其他 Candidate，则进入 Candidate 角色 对于 Candidate： 在进入 Candidate 角色时，发起 Leader 选举： currentTerm 加 1 将选票投给自己 重置 Election Timeout 计时器 发送 RequestVote RPC 至其他所有节点 如果接收到来自其他大多数节点的选票，则进入 Leader 角色 若接收到来自其他 Leader 的 AppendEntries RPC，则进入 Follower 角色 若再次 Election Timeout，那么重新发起选举 对于 Leader： 在空闲时周期地向 Follower 发起空白的 AppendEntries RPC（作为心跳信息），以避免 Follower 发起选举 若从客户端处接收到新的命令，则将该命令追加到所存储的日志中，并在顺利将该命令应用至上层状态机后返回响应 如果最新一条日志记录的 index 值大于等于某个 Follower 的 nextIndex 值，则通过 AppendEntries RPC 发送在该 nextIndex 值之后的所有日志记录： 如果备份成功，那么就更新该 Follower 对应的 nextIndex 和 matchIndex 值 否则，对 nextIndex 减 1 并重试 如果存在一个值 N，使得 N &gt; commitIndex，且大多数的 matchIndex[i] &gt;= N，且 log[N].term == currentTerm，令 commitIndex = N 接下来我们将分章节介绍 Raft 的主要实现以及各种约束的主要考虑。 Leader 选举在初次启动时，节点首先会进入 Follower 角色。只要它能够一直接收到来自其他 Leader 节点发来的 RPC 请求，它就会一直处于 Follower 状态。如果接收不到来自 Leader 的通信，Follower 会等待一个称为 Election Timeout（选举超时）的超时时间，然后便会开始发起新一轮选举。 Follower 发起选举时会对自己存储的 Term ID 进行自增，并进入 Candidate 状态。随后，它会将自己的一票投给自己，并向其他节点并行地发出 RequestVote RPC 请求。其他节点在接收到该类 RPC 请求时，会以先到先得的原则投出自己在该 Term 中的一票。 当 Candidate 在某个 Term 接收到来自集群中大多数节点发来的投票时，它便会成为 Leader，然后它便会向其他节点进行通信，确保其他节点知悉它是 Leader 而不会发起又一轮投票。每个节点在指定 Term 内只会投出一票，而只有接收到大多数节点发来的投票才能成为 Leader 的性质确保了在任意 Term 内都至多会有一个 Leader。由此我们实现了前面提及的 Eleaction Safety 性质。 Candidate 在投票过程中也有可能收到来自其他 Leader 的 AppendEntries RPC 调用，这意味着有其他节点成为了该 Term 的 Leader。如果该 RPC 中携带的 Term ID 大于等于 Candidate 当前保存的 Term ID，那么 Candidate 便会认可其为 Leader，并进入 Follower 状态，否则它会拒绝该 RPC 并继续保持其 Candidate 身份。 除了上述两种情况以外，选举也有可能发生平局的情况：若干节点在短时间内同时发起选举，导致集群中没有任何一个节点能够收到来自集群大多数节点的投票。此时，节点同样会在等待 Election Timeout 后发起新一轮的选举，但如果不加入额外的应对机制，这样的情况有可能持续发生。为此，Raft 为 Election Timeout 的取值引入了随机机制：节点在进入新的 Term 时，会在一个固定的区间内（如 150~300ms）随机选取自己在该 Term 所使用的 Election Timeout。通过随机化来错开各个节点进入 Candidate 状态的时机便能有效避免这种情况的重复发生。 日志备份在选举出一个 Leader 后，Leader 便能够开始响应来自客户端的请求了。客户端请求由需要状态机执行的命令所组成：Leader 会将接收到的命令以日志记录的形式追加到自己的记录里，并通过 AppendEntries RPC 备份到其他节点上；当日志记录被安全备份后，Leader 就会将该命令应用于位于自己上层的状态机，并向客户端返回响应；无论 Leader 已响应客户端与否，Leader 都会不断重试 AppendEntries RPC 调用，直到所有节点都持有该日志记录的备份。 日志由若干日志记录组成：每条记录中都包含一个需要在状态机上执行的命令，以及其对应的 index 值；除外，日志记录还会记录自己所属的 Term ID。 当某个日志记录顺利备份到集群大多数节点上后，Leader 便会认为该日志记录“已提交”（Committed），即该日志记录已可被安全的应用到上层状态机上。Raft 保证一个日志记录一旦被提交，那么它最终就会被所有仍可用的状态机所应用。除外，一条日志记录的提交也意味着位于其之前的所有日志记录也进入“已提交”状态。Leader 会保存其已知的最新的已提交日志的 index 值，并在每次进行 AppendEntries RPC 调用时附带该信息；Follower 在接收到该信息后即可将对应的日志记录应用在位于其上层的状态机上。 在运行时，Raft 能为系统提供如下两点性质，这两点性质共同构成了论文图 3 中提到的 Log Matching 性质： 对于两份日志中给定的 index 处，如果该处两个日志记录的 Term ID 相同，那么它们存储的状态机命令相同 如果两份日志中给定 index 处的日志记录有相同的 Term ID 值，那么位于它们之前的日志记录完全相同 第一条性质很容易得出，考虑到 Leader 在一个 Term 中只会在一个 index 处创建一条日志记录，而且日志的位置不会发生改变。为了提供上述第二个性质，Leader 在进行 AppendEntries RPC 调用时会同时携带在其自身的日志存储中位于该新日志记录之前的日志记录的 index 值及 Term ID；如果 Follower 在自己的日志存储中没有找到这条日志记录，那么 Follower 就会拒绝这条新记录。由此，每一次 AppendEntries RPC 调用的成功返回都意味着 Leader 可以确定该 Follower 存储的日志直到该 index 处均与自己所存储的日志相同。 AppendEntries RPC 的日志一致性检查是必要的，因为 Leader 的崩溃会导致新 Leader 存储的日志可能和 Follower 不一致。 考虑上图（即文中的图 7），对于给定的 Leader 日志，Follower 有可能缺失部分日志（a、b 情形）、有可能包含某些未提交的日志（c、d 情形）、或是两种情况同时发生（e、f 情形）。 对于不一致的 Follower 日志，Raft 会强制要求 Follower 与 Leader 的日志保持一致。为此，Leader 会尝试确定它与各个 Follower 所能相互统一的最后一条日志记录的 index 值，然后就会将 Follower 在该 index 之后的所有日志删除，再将自身存储的日志记录备份到 Follower 上。具体而言： Leader 会为每个 Follower 维持一个 nextIndex 变量，代表 Leader 即将通过 AppendEntries RPC 调用发往该 Follower 的日志的 index 值 在刚刚被选举为一个 Leader 时，Leader 会将每个 Follower 的 nextIndex 置为其所保存的最新日志记录的 index 之后 当有 Follower 的日志与 Leader 不一致时，Leader 的 AppendEntries RPC 调用会失败，Leader 便对该 Follower 的 nextIndex 值减 1 并重试，直到 AppendEntries 成功 Follower 接收到合法的 AppendEntries 后，便会移除其在该位置上及以后存储的日志记录，并追加上新的日志记录 如此，在 AppendEntries 调用成功后，Follower 便会在该 Term 接下来的时间里与 Leader 保持一致 由此，我们实现了前面提及的 Leader Append-Only 和 Log Matching 性质。 Leader 选举约束就上述所提及的 Leader 选举及日志备份规则，实际上是不足以确保所有状态机都能按照相同的顺序执行相同的命令的。例如，在集群运行的过程中，某个 Follower 可能会失效，而 Leader 继续在集群中提交日志记录；当这个 Follower 恢复后，有可能会被选举为 Leader，而它实际上缺少了一些已经提交的日志记录。 其他的基于 Leader 架构的共识算法都会保证 Leader 最终会持有所有已提交的日志记录。一些算法（如 Viewstamped Replication）允许节点在不持有所有已提交日志记录的情况下被选举为 Leader，并通过其他机制将缺失的日志记录发送至新 Leader。而这种机制实际上会为算法引入额外的复杂度。为了简化算法，Raft 限制了日志记录只会从 Leader 流向 Follower，同时 Leader 绝不会覆写它所保存的日志。 在这样的前提下，要提供相同的保证，Raft 就需要限制哪些 Candidate 可以成为 Leader。前面提到，Candidate 为了成为 Leader 需要获得集群内大多数节点的选票，而一个日志记录被提交同样要求它已经被备份到集群内的大多数节点上，那么如果一个 Candidate 能够成为 Leader，投票给它的节点中必然存在节点保存有所有已提交的日志记录。Candidate 在发送 RequestVote RPC 调用进行拉票时，它还会附带上自己的日志中最后一条记录的 index 值和 Term ID 值：其他节点在接收到后会与自己的日志进行比较，如果发现对方的日志落后于自己的日志（首先由 Term ID 决定大小，在 Term ID 相同时由 index 决定大小），就会拒绝这次 RPC 调用。如此一来，Raft 就能确保被选举为 Leader 的节点必然包含所有已经提交的日志。 来自旧 Term 的日志记录如上文所述，Leader 在备份当前 Term 的日志记录时，在成功备份至集群大多数节点上后 Leader 即可认为该日志记录已提交。但如果 Leader 在日志记录备份至大多数节点之前就崩溃了，后续的 Leader 会尝试继续备份该日志。然而，此时的 Leader 即使在将该日志备份至大多数节点上后都无法立刻得出该日志已提交的结论。 考虑上图这种情形。在时间点 (a) 时，S1 是 Leader，并把 (TermID=2, index=2) 的日志记录备份到了 S2 上。到了时间点 (b) 时，S1 崩溃，S5 收到 S3、S4、S5 的选票，被选为 Leader，并从客户端处接收到日志记录 (TermID=3, index=2)。在时间点 (c) 时，S5 崩溃，S1 重启，被选举为 Leader，并继续将先前没有备份的日志记录 (TermID=2, index=2) 备份到其他节点上。即便此时 S1 顺利把该日志记录备份到集群大多数节点上，它仍然不能认为该日志记录已被安全提交。考虑此时 S1 崩溃，S5 将可以收到来自 S2、S3、S4、S5 的选票，成为 Leader（其最后一个日志记录的 Term ID 是 3，大于 2），进入情形 (d)：此时 S5 会继续把日志记录 (TermID=3, index=2) 备份到其他节点上，覆盖掉原本已经备份至大多数节点的日志记录 (TermID=2, index=2)。然而，如果在时间点 (c) S1 成为 Leader 后，同样将当前 Term 的最新日志记录 (TermID=4, index=3) 备份出去并提交，就会进入情形 (e)，此时 S5 便无法再被选举为 Leader。因此，解决该问题的关键在于在备份旧 Term 的日志时也要把当前 Term 最新的日志一并分发出去。 由此，Raft 只会在备份当前 Term 的日志记录时才会通过计数的方式来判断该日志记录是否已被提交；一旦该日志记录完成提交，根据前面提及的 Log Matching 性质，Leader 就能得出之前的日志记录也已被提交。由此，我们便实现了前面提及的 Leader Completeness 性质。文中 5.4.3 节有完整的证明过程，感兴趣的读者可自行查阅。 证得前面 4 条性质后，最后一条 State Machine Safety 性质也可证得：当节点将日志记录应用于其上层状态机时，该日志记录及其之前的所有日志记录必然已经提交。某些节点执行命令的进度可能落后，我们考虑所有节点目前已执行日志记录的 index 值的最小值：Log Completeness 性质保证了未来的所有 Leader 都会持有该日志记录，因此在之后的 Term 中其他节点应用位于该 index 处的日志记录时，该日志记录保存的必然是相同的命令。由此，上层状态机只要按照 Raft 日志记录的 index 值顺序执行命令即可安全完成状态备份。 时序要求为了提供合理的可用性，集群仍需满足一定的时序要求，具体如下： $$broadcastTime \\ll electionTimeout \\ll MTBF$$ 其中 $broadcastTime$ 即一个节点并发地发送 RPC 请求至集群中其他节点并接收请求的平均耗时；$electionTimeout$ 即节点的选举超时时间；$MTBF$ 即单个节点每次失效的平均间隔时间（Mean Time Between Failures）。 上述的不等式要求，$broadcastTime$ 要小于 $electionTimeout$ 一个数量级，以确保正常 Leader 心跳间隔不会导致 Follower 超时并发起选举；同时考虑到 $electionTimeout$ 会随机选出，该不等式还能确保 Leader 选举时平局局面不会频繁出现。除外，$electionTimeout$ 也应比 $MTBF$ 小几个数量级，考虑到系统会在 Leader 失效时停止服务，而这样的情况不应当频繁出现。 在这个不等式中，$broadcastTime$ 及 $MTBF$ 由集群架构所决定，$electionTimeout$ 则可由运维人员自行配置。 Candidate 与 Follower 失效目前来讲我们讨论都是 Leader 失效的问题。对于 Candidate 和 Follower 而言，它们分别是 RequestVote 和 AppendEntries RPC 调用的接收方：当 Candidate 或 Follower 崩溃后，RPC 调用会失败；Raft RPC 失败时会不断重试 RPC，直至 RPC 成功；除外，RPC 调用也有可能已经生效，但接收方在响应前就已失效，为此 Raft 保证 RPC 的幂等性，在节点重启后收到重复的 RPC 调用也不会有所影响。 集群成员变更直到目前为止，我们的讨论都假设集群的成员配置是一成不变的，然而这在系统的正常运维中是不常见的：系统总是可能需要做出变更，例如移除一些节点或增加一些节点。 当然，集群可以被全部关闭后，调整配置文件，再全部重启，这样也能完成集群配置变更，但这样会导致系统出现一段时间的不可用。而 Raft 则引入了额外的机制来允许集群在运行中变更自己的成员配置。 在进行配置变更时，直接从旧配置切换至新配置是不可行的，源于不同的节点不可能原子地完成配置切换，而这之间可能会有一些时间间隙使得集群存在两个不同的“大多数”。 如上图所示，集群逐渐地从旧配置切换至新配置，那么在箭头标记的位置就出现了两个不同的“大多数”：S1、S2 构成 $C_{old}$ 的大多数，S3、S4、S5 构成 $C_{new}$ 的大多数。在这一时间间隔内，处于两个配置的节点可能会选出各自的 Leader，引入 Split-Brain 问题。问题的关键在于，在这段时间间隔中，$C_{old}$ 和 $C_{new}$ 都能够独立地做出决定。 为了解决这个问题，Raft 采用二阶段的方式来完成配置切换：在 $C_{old}$ 与 $C_{new}$ 之间，引入一个被称为 Joint Consensus 的特殊配置 $C_{old,new}$ 作为迁移状态。该配置有如下性质： 日志记录会被备份到 $C_{old}$ 及 $C_{new}$ 的节点上 两份配置中的任意机器都能成为 Leader 选举或提交日志记录要求得到来自 $C_{old}$ 和 $C_{new}$ 的两个不同的“大多数”的同意 上图显示了配置切换的时序，其中可以看到不存在 $C_{old}$ 和 $C_{new}$ 都能独立作出决定的时间段。 切换时，Leader 会创建特殊的配置切换日志，并利用先前提到的日志备份机制通知其他节点进行配置切换。对于这种特殊的配置切换日志，节点在接收到时就会立刻切换配置，不会等待日志提交，因此 Leader 会首先进入 $C_{old,new}$ 配置，同时运用这份配置来判断配置切换的日志是否成功提交。如此，如果 Leader 在完成提交这份日志之前崩溃，新的 Leader 只会处于 $C_{old}$ 或 $C_{old,new}$ 配置，如此一来在该日志记录完成提交前，$C_{new}$ 便无法独立做出决定。 在 $C_{old,new}$ 的配置变更日志完成提交后，$C_{old}$ 便也不能独立做出决定了，且 Leader Completeness 性质保证了此时选举出的 Leader 必然处于 $C_{old,new}$ 配置。此时，Leader 就能开始重复上述过程，切换到 $C_{new}$ 配置。在 $C_{new}$ 配置日志完成提交后，这个过程中被移出集群的节点就能顺利关闭了。 除此以外，我们仍有三个问题需要去解决。 首先，配置变更可能会引入新的节点，这些节点不包含之前的日志记录，完成日志备份可能会需要较长的时间，而这段时间可能导致集群无法提交日志，引入一段时间的服务不可用。为此，Raft 在节点变更配置之前还引入了一个额外的阶段：此时节点会以不投票成员的形式加入集群，开始备份日志，Leader 在计算“大多数”时也不会考虑它们；等到它们完成备份后，它们就能回到正常状态，完成配置切换。 此外，集群的 Leader 有可能不属于 $C_{new}$。在这种情况下，Leader 在完成 $C_{new}$ 的配置变更日志提交后才能变更自己的配置并关闭。也就是说，在 Leader 提交 $C_{new}$ 的日志时，那段时间里它会需要管理一个不包含自己的集群：它会把日志记录备份出去，但不会把自己算入“大多数”。直到 $C_{new}$ 日志完成提交，$C_{new}$ 才能够独立做出决定，才能够在原 Leader 降级后在 $C_{new}$ 集群中选出新的 Leader；在那之前，来自 $C_{old}$ 的节点有可能被选为 Leader。 最后，那些从集群中被移除出去的节点可能会在配置切换完成后干扰新集群的运行。这些节点不会再接收到 Leader 的心跳，于是它们就会超时并发起选举。这时它们会发起 RequestVote RPC 调用，其中包含新的 Term ID，而这可能导致新的 Leader 自动降级为 Follower，导致服务不可用。最终新集群会选出一个新的 Leader，但被移除的节点依旧不会接收到心跳信息，它们会再次超时，再次发起选举，如此循环往复。 为解决此问题，节点在其“确信” Leader 仍存活时会拒绝 RequestVote RPC 调用：如果距离节点上一次接收到 Leader 心跳信息过去的时间小于 Election Timeout 的最小值，那么节点便会“确信” Leader 仍然存活。考虑前面提到的时序要求，这确实能够在大多数情况下避免该问题。 日志压缩随着 Raft 集群的不断运行，节点上的日志体积会不断增大，这会逐渐占用节点的磁盘资源，此外过长的日志也会延长节点重放日志的耗时，引入服务可用性问题。为此，集群需要对过往的日志进行压缩。 快照是进行日志压缩最简便的方案。在进行快照时，状态机当前的完整状态会被写入到持久存储中，而后就能够安全地把直到该时间点以前的日志记录移除了。完成快照后，Raft 也会在快照中记录其所覆盖到的最新日志记录的 index 和 Term ID，以便后面的日志记录能够被继续追加。为了兼容前面提到的集群成员配置变更，快照同样需要记录下当前的集群成员配置。 对于 Raft 来说，每个节点会独立地生成快照。相比起只由 Leader 生成快照，这样的设计避免了 Leader 频繁向其他 Follower 传输快照而占用网络带宽，况且 Follower 也持有着足以独立生成快照的数据。 尽管如此，当某个 Follower 落后太多或是集群加入了新节点时，Leader 仍然会需要将自己持有的快照传输给 Follower。为此，Raft 提供了专门的 InstallSnapshot RPC 接口。 InstallSnapshot RPC: 由 Leader 进行调用，用于将组成快照的文件块发给指定 Follower。Leader 总会按照顺序发送文件块。 参数： term: Leader 当前的 Term ID leaderId: Leader 的 ID lastIncludedIndex: 该快照所覆盖的最后一个日志记录的 index 值 lastIncludedTerm: 该快照所覆盖的最后一个日志记录的 Term ID offset: 当前发送的文件块在整个快照文件中的偏移值 data[]: 文件块的内容 done: 该文件块是否为最后一个文件块 返回值： term: Follower 的当前 Term ID。Leader 可根据该值判断是否要降级为 Follower。 Follower 在接收到该 RPC 调用后会进行以下操作： 若 term &lt; currentTerm，return 若 offset == 0，创建快照文件 在快照文件的指定 offset 处写入 data[] 若 done == false，返回响应，并继续等待其他文件块 移除已有的或正在生成的在该快照之前的快照 如果有一个日志记录的 Term ID 及 index 值与该快照所包含的最后一个日志记录相同，那么便保留该日志记录之后的日志记录，并返回响应 移除被该快照覆盖的所有日志记录 使用该快照的内容重置上层状态机，并载入快照所携带的集群成员配置 客户端交互最后，我们再来聊聊客户端如何与 Raft 集群进行交互。 首先，客户端需要能够得知目前 Raft 集群的 Leader 是谁。在一开始，客户端会与集群中任意的一个节点进行通信：如果该节点不是 Leader，那么它会把上一次接收到的 AppendEntries RPC 调用中携带的 Leader ID 返回给客户端；如果客户端无法连接至该节点，那么客户端就会再次随机选取一个节点进行重试。 Raft 的目标之一是为上层状态机提供日志记录的 exactly-once 语义，但如果 Leader 在完成日志提交后、向客户端返回响应之前崩溃，客户端就会重试发送该日志记录。为此，客户端需要为自己的每一次通信赋予独有的序列号，而上层状态机则需要为每个客户端记录其上一次通信所携带的序列号以及对应的响应内容，如此一来当收到重复的调用时状态机便可在不执行该命令的情况下返回响应。 对于客户端的只读请求，Raft 集群可以在不对日志进行任何写入的情况下返回响应。然而，这有可能让客户端读取到过时的数据，源于当前与客户端通信的 “Leader” 可能已经不是集群的实际 Leader，而它自己并不知情。 为了解决此问题，Raft 必须提供两个保证。首先，Leader 持有关于哪个日志记录已经成功提交的最新信息。基于前面提到的 Leader Completeness 性质，节点在成为 Leader 后会立刻添加一个空白的 no-op 日志记录；此外，Leader 还需要知道自己是否已经需要降级，为此 Leader 在处理只读请求前需要先与集群大多数节点完成心跳通信，以确保自己仍是集群的实际 Leader。 结语至此，本文已对 Raft 论文的内容进行了完整的总结。总体而言，Raft 的论文为 Raft 提供了很详实的介绍，论文各处的 API Specification 也为他人实现 Raft 算法提供了很好的基础。Raft 算法也存在着一些在论文中也没有提及的细节及优化方式，有机会的话我会在新的博文中介绍这部分的内容，敬请期待。","link":"/raft/"},{"title":"Spark Catalyst 进阶：Parser 词素","text":"在之前的 SparkSQL Catalyst 源码解析中，我大致的讲解了 SparkSQL 的执行流程，用户输入的 SQL 语句如何一步一步地变为 Logical Plan 再变为 Physical Plan，再执行成为结果 RDD。上一个系列旨在抛砖引玉，该流程中的每个重要部件如 Parser 、 Analyzer 、Optimizer 、 Planner 等仅仅讲解了它们是如何管理和运行一些列的 rule，但并未仔细讲解每一个 rule 的功能。 接下来的内容旨在更深入地研究这些被上一个系列所遗漏的细节实现，同时还会在之后考虑解析 SparkSQL 的 UDF 注册以及 cache 表功能。 那么作为进阶篇的第一篇，我们就先从 SparkSQL Parser 开始。 Parsers之前我们提到，在我们调用 SQLContext#sql 方法时，SQLContext 实际上以我们传入的 SQL 字符串为参数调用了 parseSql 方法来获取对应的 LogicalPlan，而 parseSql 方法则调用了 ddlParser 的 parse 方法。除此之外我们还了解到，SparkSQL 有着不止一个 parser，每个 parser 之间是 fallback 的关系，在上一个 parser 解析完成后，剩下的无法解析的内容再移交给下一个 parser。除了最顶层的 ddlParser 以外，依次往下还有一个 SparkSQLParser 和一个会根据设置而改变的 SQL 方言 Parser。 我们不妨先去了解一下默认环境下这个 SQL 方言 Parser 是什么： 123456789101112131415161718192021// SQLContext.scala// 通过作为 SparkSQLParser 的构造参数将 SQL 方言 Parser 注册为 fallback parserprotected[sql] val sqlParser = new SparkSQLParser(getSQLDialect().parse(_))protected[sql] def getSQLDialect(): ParserDialect = { try { val clazz = Utils.classForName(dialectClassName) clazz.newInstance().asInstanceOf[ParserDialect] // 通过反射机制实例化了一个 ParserDialect 子类实例 } catch { // ... }}protected[sql] def dialectClassName = if (conf.dialect == &quot;sql&quot;) { classOf[DefaultParserDialect].getCanonicalName // 看来默认情况下的方言 Parser 就是这个 DefaultParserDialect} else { conf.dialect} 那么我们就看看这个 DefaultParserDialect 的源代码： 123456789private[spark] class DefaultParserDialect extends ParserDialect { @transient protected val sqlParser = new SqlParser override def parse(sqlText: String): LogicalPlan = { sqlParser.parse(sqlText) // 也就是说，真正的 parse 工作由 SqlParser 完成 }} 那我们就去看看 SqlParser： 1234567891011121314151617181920212223// 它和 SparkSQLParser、DDLParser 一样继承于 AbstractSparkSQLParserclass SqlParser extends AbstractSparkSQLParser with DataTypeParser { // 与 AbstractSparkSQLParser#parse 几乎完全一致，差别仅在于 phrase 的参数变成了 projection def parseExpression(input: String): Expression = { // Initialize the Keywords. initLexical phrase(projection)(new lexical.Scanner(input)) match { case Success(plan, _) =&gt; plan case failureOrError =&gt; sys.error(failureOrError.toString) } } // 一系列的 keyword，由 initLexical 方法注册为词素 // 与 DDLParser 和 SparkSQLParser 都不同，这里我们找到了剩下的那些 SQL 常见字 protected val ABS = Keyword(&quot;ABS&quot;) protected val ALL = Keyword(&quot;ALL&quot;) // ... protected val WHERE = Keyword(&quot;WHERE&quot;) protected val WITH = Keyword(&quot;WITH&quot;) // ...} OK，由此一来我们搞懂了最后一个 Parser 到底是什么了。它们之间的关系是这样的： 看起来相当复杂，要想完整理解，我们不仅需要看 Spark 的源代码，甚至还要学习 Scala 自带的分词器。不过，我们先不着急着去学习它们是怎么分词的。现在编译器的开发工作已经不像以前那样一个一个代码码过去了，否则光是实现一个状态转换机便是极其复杂的事。快捷的分词和语义分析器代码通常是由软件自动生成的。学习过编译原理的同学自然不会陌生，这种软件就叫 Flex。我们通过特殊的格式，将分词逻辑写在 flex 文件里，告诉它我们希望符合什么模式的词或表达式变成什么样的实例，Flex 便能根据我们的 flex 文件自动生成分词器。 Spark 也是类似。在 Spark 的类中，我们看得最多的就是 Keyword。这些对象包含了每一个 SQL 保留字，和我们编写 flex 文件是需要提供的信息内容是一样的。但一个完整的 flex 文件还需要有其它的信息，但我们不着急，毕竟已经找到一个起步点了。我们先来看看 SqlParser： 12345678910111213141516171819202122232425262728class SqlParser extends AbstractSparkSQLParser with DataTypeParser { // ... // 一堆 SQL 关键字... protected val ABS = Keyword(&quot;ABS&quot;) protected val ALL = Keyword(&quot;ALL&quot;) // ... protected val WHERE = Keyword(&quot;WHERE&quot;) protected val WITH = Keyword(&quot;WITH&quot;) // ... protected lazy val start: Parser[LogicalPlan] = start1 | insert | cte // 返回的是一个 Parser[LogicalPlan]，而且 start 这个变量是作为 phrase 参数在 AbstractSparkSQLParser#parse 里使用的 // 越来越接近了，我们接着往下看 protected lazy val start1: Parser[LogicalPlan] = (select | (&quot;(&quot; ~&gt; select &lt;~ &quot;)&quot;)) * ( UNION ~ ALL ^^^ { (q1: LogicalPlan, q2: LogicalPlan) =&gt; Union(q1, q2) } | INTERSECT ^^^ { (q1: LogicalPlan, q2: LogicalPlan) =&gt; Intersect(q1, q2) } | EXCEPT ^^^ { (q1: LogicalPlan, q2: LogicalPlan) =&gt; Except(q1, q2)} | UNION ~ DISTINCT.? ^^^ { (q1: LogicalPlan, q2: LogicalPlan) =&gt; Distinct(Union(q1, q2)) } ) // 我倒，这些运算符是什么鬼！ // ...} 好，在继续往下看之前，我们需要先了解这些奇怪的运算符到底是什么意思。但我们先看看那个 start1 变量。这种语句格式，很明显是在利用这些关键字表达某些语句模式，并且给定了当符合这些模式时将应用什么方法来将其对应为相应的 LogicalPlan。那么我们现在要做的，就是学习这些运算符的含义，这样我们就能理解这些模式了。 我们先看一下 Keyword 类： 123456789// AbstractSparkSQLParser.scala// 通过隐式调用 Keyword 的 parser 方法来将其变为 Parser 实例protected implicit def asParser(k: Keyword): Parser[String] = k.parserprotected case class Keyword(str: String) { def normalize: String = lexical.normalizeKeyword(str) def parser: Parser[String] = normalize} 这下我们就能理解了，AbstractSparkSQLParser 通过一个隐式转换把 Keyword 转换为了 Parser，这些奇怪的运算符实际上是 Parser 的方法。这些方法实际上就在 Parsers.scala 里，通过查阅 ScalaDoc 就能了解到这些运算符的功能大致如下： 运算符 作用 ~ p ~ q succeeds if p succeeds and q succeeds on the input left over by p.Return a Parser that – on success – returns a ~ (like a Pair, but easier to pattern match on) that contains the result of p and that of q. The resulting parser fails if either p or q fails. ~&gt; p ~&gt; q succeeds if p succeeds and q succeeds on the input left over by p.Return a Parser that – on success – returns the result of q. &lt;~ p &lt;~ q succeeds if p succeeds and q succeeds on the input left over by p.Return a Parser that – on success – returns the result of p. | p | q succeeds if p succeeds or q succeeds.Return a Parser that returns the result of the first parser to succeed (out of p and q) The resulting parser succeeds if (and only if) - p succeeds, ‘’or’’ - if p fails allowing back-tracking and q succeeds. ^^ p ^^ f succeeds if p succeeds; it returns f applied to the result of p. ^^^ p ^^^ v succeeds if p succeeds; discards its result, and returns v instead. * Returns a parser that repeatedly parses what this parser parses. ? Returns a parser that optionally parses what this parser parses. 语文不好，就不翻译了，这种等级的英语应该不算难懂。 DDLParser在了解过这些运算符的意思以后，我们就可以轻松愉快地阅读 Spark 定义的分词规则了。我们先从 DDLParser 开始： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113private[sql] class DDLParser(parseQuery: String =&gt; LogicalPlan) extends AbstractSparkSQLParser with DataTypeParser with Logging { // ... // 关键字 protected val CREATE = Keyword(&quot;CREATE&quot;) protected val TEMPORARY = Keyword(&quot;TEMPORARY&quot;) protected val TABLE = Keyword(&quot;TABLE&quot;) protected val IF = Keyword(&quot;IF&quot;) protected val NOT = Keyword(&quot;NOT&quot;) protected val EXISTS = Keyword(&quot;EXISTS&quot;) protected val USING = Keyword(&quot;USING&quot;) protected val OPTIONS = Keyword(&quot;OPTIONS&quot;) protected val DESCRIBE = Keyword(&quot;DESCRIBE&quot;) protected val EXTENDED = Keyword(&quot;EXTENDED&quot;) protected val AS = Keyword(&quot;AS&quot;) protected val COMMENT = Keyword(&quot;COMMENT&quot;) protected val REFRESH = Keyword(&quot;REFRESH&quot;) protected def start: Parser[LogicalPlan] = ddl protected lazy val ddl: Parser[LogicalPlan] = createTable | describeTable | refreshTable // 说明 DDLParser 可以 parse 的语句模式就是上面三种。我们接下来分别看看它们各自是什么模式 // 匹配建表 SQL 语句中的一个列。形如 `name String` 、 `age INT COMMENT &quot;user's age&quot;` protected lazy val column: Parser[StructField] = // ident 匹配一个由字符组成的 identifier // 列名 列类型 可能存在的列 COMMENT 生成该列对应的 StructField ident ~ dataType ~ (COMMENT ~&gt; stringLit).? ^^ { case columnName ~ typ ~ cm =&gt; val meta = cm match { case Some(comment) =&gt; new MetadataBuilder().putString(COMMENT.str.toLowerCase, comment).build() case None =&gt; Metadata.empty } StructField(columnName, typ, nullable = true, meta) } // 匹配建表语句中的模式定义部分，形如 `(name STRING, age INT COMMENT &quot;user's age&quot;)` // ( 由 `,` 间隔的 column 们 ) protected lazy val tableCols: Parser[Seq[StructField]] = &quot;(&quot; ~&gt; repsep(column, &quot;,&quot;) &lt;~ &quot;)&quot; protected lazy val createTable: Parser[LogicalPlan] = // 返回是否 temp 返回是否 NOT 表名 (CREATE ~&gt; TEMPORARY.? &lt;~ TABLE) ~ (IF ~&gt; NOT &lt;~ EXISTS).? ~ ident ~ tableCols.? ~ (USING ~&gt; className) ~ (OPTIONS ~&gt; options).? ~ (AS ~&gt; restInput).? ^^ { case temp ~ allowExisting ~ tableName ~ columns ~ provider ~ opts ~ query =&gt; if (temp.isDefined &amp;&amp; allowExisting.isDefined) { throw new DDLException( &quot;a CREATE TEMPORARY TABLE statement does not allow IF NOT EXISTS clause.&quot;) } val options = opts.getOrElse(Map.empty[String, String]) if (query.isDefined) { if (columns.isDefined) { throw new DDLException( &quot;a CREATE TABLE AS SELECT statement does not allow column definitions.&quot;) } // When IF NOT EXISTS clause appears in the query, the save mode will be ignore. val mode = if (allowExisting.isDefined) { SaveMode.Ignore } else if (temp.isDefined) { SaveMode.Overwrite } else { SaveMode.ErrorIfExists } val queryPlan = parseQuery(query.get) // 使用 AS+SELECT 语句返回的结果建表 CreateTableUsingAsSelect(tableName, provider, temp.isDefined, Array.empty[String], mode, options, queryPlan) } else { val userSpecifiedSchema = columns.flatMap(fields =&gt; Some(StructType(fields))) // 直接建表 CreateTableUsing( tableName, userSpecifiedSchema, provider, temp.isDefined, options, allowExisting.isDefined, managedIfNoPath = false) } } // 至此我们知道了两个 LogicalPlan 子类，CreateTableUsingAsSelect 和 CreateTableUsing protected lazy val describeTable: Parser[LogicalPlan] = // DESCRIBE [EXTENDED] [dataBase.] table (DESCRIBE ~&gt; opt(EXTENDED)) ~ (ident &lt;~ &quot;.&quot;).? ~ ident ^^ { case e ~ db ~ tbl =&gt; val tblIdentifier = db match { case Some(dbName) =&gt; Seq(dbName, tbl) case None =&gt; Seq(tbl) } DescribeCommand(UnresolvedRelation(tblIdentifier, None), e.isDefined) } // 又一个 LogicalPlan 子类，DescribeCommand，左子是[db.]tbl 的 UnresolvedRelation，右子是 ` 是否 Extended` protected lazy val refreshTable: Parser[LogicalPlan] = REFRESH TABLE [db.] table REFRESH ~&gt; TABLE ~&gt; (ident &lt;~ &quot;.&quot;).? ~ ident ^^ { case maybeDatabaseName ~ tableName =&gt; RefreshTable(maybeDatabaseName.getOrElse(&quot;default&quot;), tableName) } // RefreshTable，左子为数据库名，默认为 `default`，右子为表名} OK，至此 DDLParser 支持的三种语句我们就解析完了，它们分别是 CREATE 、 DESCRIBE 和 REFRESH 语句。它无法解析的语句将交由它的 fallback 解析方法来解析。那好，我们 fallback ！ SparkSQLParser1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private[sql] class SparkSQLParser(fallback: String =&gt; LogicalPlan) extends AbstractSparkSQLParser { // ... // 关键字 protected val AS = Keyword(&quot;AS&quot;) protected val CACHE = Keyword(&quot;CACHE&quot;) protected val CLEAR = Keyword(&quot;CLEAR&quot;) protected val IN = Keyword(&quot;IN&quot;) protected val LAZY = Keyword(&quot;LAZY&quot;) protected val SET = Keyword(&quot;SET&quot;) protected val SHOW = Keyword(&quot;SHOW&quot;) protected val TABLE = Keyword(&quot;TABLE&quot;) protected val TABLES = Keyword(&quot;TABLES&quot;) protected val UNCACHE = Keyword(&quot;UNCACHE&quot;) override protected lazy val start: Parser[LogicalPlan] = cache | uncache | set | show | others // 一共五个：CACHE、UNCACHE、SET、SHOW，还有一个 others，也许包含不止一种操作 private lazy val cache: Parser[LogicalPlan] = CACHE ~&gt; LAZY.? ~ (TABLE ~&gt; ident) ~ (AS ~&gt; restInput).? ^^ { case isLazy ~ tableName ~ plan =&gt; CacheTableCommand(tableName, plan.map(fallback), isLazy.isDefined) } // CacheTableCommand(表名, 查询用的 Logical Plan, 是否 lazy) private lazy val uncache: Parser[LogicalPlan] = ( UNCACHE ~ TABLE ~&gt; ident ^^ { case tableName =&gt; UncacheTableCommand(tableName) } | CLEAR ~ CACHE ^^^ ClearCacheCommand ) // UncacheTableCommand(表名)、ClearCacheCommand 清空 cache private lazy val set: Parser[LogicalPlan] = SET ~&gt; restInput ^^ { case input =&gt; SetCommandParser(input) } // SetCommandParser(输入)，该类就定义在 SparkSQLParsers 里，会尝试从输入中拆分出键值对，并返回 SetCommand(key, value) private lazy val show: Parser[LogicalPlan] = SHOW ~&gt; TABLES ~ (IN ~&gt; ident).? ^^ { case _ ~ dbName =&gt; ShowTablesCommand(dbName) } // ShowTablesCommand(数据库名) // 剩余的其他所有输入 protected lazy val wholeInput: Parser[String] = new Parser[String] { def apply(in: Input): ParseResult[String] = Success(in.source.toString, in.drop(in.source.length())) } private lazy val others: Parser[LogicalPlan] = wholeInput ^^ { case input =&gt; fallback(input) } // 这下我们就明白了，CACHE、UNCACHE、SET、SHOW 四种模式无法匹配的语句将通过 others 直接分配给 fallback } 至此我们又收集到了五种 LogicalPlan：CacheTableCommand、UncacheTableCommand、ClearCacheCommand、SetCommand、ShowTablesCommand。那么我们继续 fallback ！ SqlParser123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385class SqlParser extends AbstractSparkSQLParser with DataTypeParser { def parseExpression(input: String): Expression = { // Initialize the Keywords. initLexical phrase(projection)(new lexical.Scanner(input)) match { case Success(plan, _) =&gt; plan case failureOrError =&gt; sys.error(failureOrError.toString) } } // 各种 SQL 关键字 protected val ABS = Keyword(&quot;ABS&quot;) protected val ALL = Keyword(&quot;ALL&quot;) protected val AND = Keyword(&quot;AND&quot;) protected val APPROXIMATE = Keyword(&quot;APPROXIMATE&quot;) protected val AS = Keyword(&quot;AS&quot;) protected val ASC = Keyword(&quot;ASC&quot;) protected val AVG = Keyword(&quot;AVG&quot;) protected val BETWEEN = Keyword(&quot;BETWEEN&quot;) protected val BY = Keyword(&quot;BY&quot;) protected val CASE = Keyword(&quot;CASE&quot;) protected val CAST = Keyword(&quot;CAST&quot;) protected val COALESCE = Keyword(&quot;COALESCE&quot;) protected val COUNT = Keyword(&quot;COUNT&quot;) protected val DESC = Keyword(&quot;DESC&quot;) protected val DISTINCT = Keyword(&quot;DISTINCT&quot;) protected val ELSE = Keyword(&quot;ELSE&quot;) protected val END = Keyword(&quot;END&quot;) protected val EXCEPT = Keyword(&quot;EXCEPT&quot;) protected val FALSE = Keyword(&quot;FALSE&quot;) protected val FIRST = Keyword(&quot;FIRST&quot;) protected val FROM = Keyword(&quot;FROM&quot;) protected val FULL = Keyword(&quot;FULL&quot;) protected val GROUP = Keyword(&quot;GROUP&quot;) protected val HAVING = Keyword(&quot;HAVING&quot;) protected val IF = Keyword(&quot;IF&quot;) protected val IN = Keyword(&quot;IN&quot;) protected val INNER = Keyword(&quot;INNER&quot;) protected val INSERT = Keyword(&quot;INSERT&quot;) protected val INTERSECT = Keyword(&quot;INTERSECT&quot;) protected val INTO = Keyword(&quot;INTO&quot;) protected val IS = Keyword(&quot;IS&quot;) protected val JOIN = Keyword(&quot;JOIN&quot;) protected val LAST = Keyword(&quot;LAST&quot;) protected val LEFT = Keyword(&quot;LEFT&quot;) protected val LIKE = Keyword(&quot;LIKE&quot;) protected val LIMIT = Keyword(&quot;LIMIT&quot;) protected val LOWER = Keyword(&quot;LOWER&quot;) protected val MAX = Keyword(&quot;MAX&quot;) protected val MIN = Keyword(&quot;MIN&quot;) protected val NOT = Keyword(&quot;NOT&quot;) protected val NULL = Keyword(&quot;NULL&quot;) protected val ON = Keyword(&quot;ON&quot;) protected val OR = Keyword(&quot;OR&quot;) protected val ORDER = Keyword(&quot;ORDER&quot;) protected val SORT = Keyword(&quot;SORT&quot;) protected val OUTER = Keyword(&quot;OUTER&quot;) protected val OVERWRITE = Keyword(&quot;OVERWRITE&quot;) protected val REGEXP = Keyword(&quot;REGEXP&quot;) protected val RIGHT = Keyword(&quot;RIGHT&quot;) protected val RLIKE = Keyword(&quot;RLIKE&quot;) protected val SELECT = Keyword(&quot;SELECT&quot;) protected val SEMI = Keyword(&quot;SEMI&quot;) protected val SQRT = Keyword(&quot;SQRT&quot;) protected val SUBSTR = Keyword(&quot;SUBSTR&quot;) protected val SUBSTRING = Keyword(&quot;SUBSTRING&quot;) protected val SUM = Keyword(&quot;SUM&quot;) protected val TABLE = Keyword(&quot;TABLE&quot;) protected val THEN = Keyword(&quot;THEN&quot;) protected val TRUE = Keyword(&quot;TRUE&quot;) protected val UNION = Keyword(&quot;UNION&quot;) protected val UPPER = Keyword(&quot;UPPER&quot;) protected val WHEN = Keyword(&quot;WHEN&quot;) protected val WHERE = Keyword(&quot;WHERE&quot;) protected val WITH = Keyword(&quot;WITH&quot;) protected def assignAliases(exprs: Seq[Expression]): Seq[NamedExpression] = { exprs.zipWithIndex.map { case (ne: NamedExpression, _) =&gt; ne case (e, i) =&gt; Alias(e, s&quot;c$i&quot;)() } } protected lazy val start: Parser[LogicalPlan] = start1 | insert | cte // 这里的 start1 和 cte 很有可能包含了不止一个操作 // 用于解析表达式（Expression）的 Parser 们 // 最基本的表达式词素，包括 `*` 、 `table.*` protected lazy val baseExpression: Parser[Expression] = ( &quot;*&quot; ^^^ UnresolvedStar(None) | ident &lt;~ &quot;.&quot; ~ &quot;*&quot; ^^ { case tableName =&gt; UnresolvedStar(Option(tableName)) } | primary ) // 根据表达式运算符的优先级开始逐渐建树 // 首先是乘除、取余与位运算 protected lazy val productExpression: Parser[Expression] = baseExpression * ( &quot;*&quot; ^^^ { (e1: Expression, e2: Expression) =&gt; Multiply(e1, e2) } | &quot;/&quot; ^^^ { (e1: Expression, e2: Expression) =&gt; Divide(e1, e2) } | &quot;%&quot; ^^^ { (e1: Expression, e2: Expression) =&gt; Remainder(e1, e2) } | &quot;&amp;&quot; ^^^ { (e1: Expression, e2: Expression) =&gt; BitwiseAnd(e1, e2) } | &quot;|&quot; ^^^ { (e1: Expression, e2: Expression) =&gt; BitwiseOr(e1, e2) } | &quot;^&quot; ^^^ { (e1: Expression, e2: Expression) =&gt; BitwiseXor(e1, e2) } ) // 然后是加减 protected lazy val termExpression: Parser[Expression] = productExpression * ( &quot;+&quot; ^^^ { (e1: Expression, e2: Expression) =&gt; Add(e1, e2) } | &quot;-&quot; ^^^ { (e1: Expression, e2: Expression) =&gt; Subtract(e1, e2) } ) // 然后是比较符 protected lazy val comparisonExpression: Parser[Expression] = ( termExpression ~ (&quot;=&quot; ~&gt; termExpression) ^^ { case e1 ~ e2 =&gt; EqualTo(e1, e2) } | termExpression ~ (&quot;&lt;&quot; ~&gt; termExpression) ^^ { case e1 ~ e2 =&gt; LessThan(e1, e2) } | termExpression ~ (&quot;&lt;=&quot; ~&gt; termExpression) ^^ { case e1 ~ e2 =&gt; LessThanOrEqual(e1, e2) } | termExpression ~ (&quot;&gt;&quot; ~&gt; termExpression) ^^ { case e1 ~ e2 =&gt; GreaterThan(e1, e2) } | termExpression ~ (&quot;&gt;=&quot; ~&gt; termExpression) ^^ { case e1 ~ e2 =&gt; GreaterThanOrEqual(e1, e2) } | termExpression ~ (&quot;!=&quot; ~&gt; termExpression) ^^ { case e1 ~ e2 =&gt; Not(EqualTo(e1, e2)) } | termExpression ~ (&quot;&lt;&gt;&quot; ~&gt; termExpression) ^^ { case e1 ~ e2 =&gt; Not(EqualTo(e1, e2)) } | termExpression ~ (&quot;&lt;=&gt;&quot; ~&gt; termExpression) ^^ { case e1 ~ e2 =&gt; EqualNullSafe(e1, e2) } | termExpression ~ NOT.? ~ (BETWEEN ~&gt; termExpression) ~ (AND ~&gt; termExpression) ^^ { case e ~ not ~ el ~ eu =&gt; val betweenExpr: Expression = And(GreaterThanOrEqual(e, el), LessThanOrEqual(e, eu)) not.fold(betweenExpr)(f =&gt; Not(betweenExpr)) } | termExpression ~ (RLIKE ~&gt; termExpression) ^^ { case e1 ~ e2 =&gt; RLike(e1, e2) } | termExpression ~ (REGEXP ~&gt; termExpression) ^^ { case e1 ~ e2 =&gt; RLike(e1, e2) } | termExpression ~ (LIKE ~&gt; termExpression) ^^ { case e1 ~ e2 =&gt; Like(e1, e2) } | termExpression ~ (NOT ~ LIKE ~&gt; termExpression) ^^ { case e1 ~ e2 =&gt; Not(Like(e1, e2)) } | termExpression ~ (IN ~ &quot;(&quot; ~&gt; rep1sep(termExpression, &quot;,&quot;)) &lt;~ &quot;)&quot; ^^ { case e1 ~ e2 =&gt; In(e1, e2) } | termExpression ~ (NOT ~ IN ~ &quot;(&quot; ~&gt; rep1sep(termExpression, &quot;,&quot;)) &lt;~ &quot;)&quot; ^^ { case e1 ~ e2 =&gt; Not(In(e1, e2)) } | termExpression &lt;~ IS ~ NULL ^^ { case e =&gt; IsNull(e) } | termExpression &lt;~ IS ~ NOT ~ NULL ^^ { case e =&gt; IsNotNull(e) } | NOT ~&gt; termExpression ^^ {e =&gt; Not(e)} | termExpression ) // 然后是 AND 关系 protected lazy val andExpression: Parser[Expression] = comparisonExpression * (AND ^^^ { (e1: Expression, e2: Expression) =&gt; And(e1, e2) }) // OR 关系 protected lazy val orExpression: Parser[Expression] = andExpression * (OR ^^^ { (e1: Expression, e2: Expression) =&gt; Or(e1, e2) }) // 基本表达式解析树的顶层 protected lazy val expression: Parser[Expression] = orExpression // 函数 protected lazy val function: Parser[Expression] = ( SUM ~&gt; &quot;(&quot; ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; Sum(exp) } | SUM ~&gt; &quot;(&quot; ~&gt; DISTINCT ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; SumDistinct(exp) } | COUNT ~ &quot;(&quot; ~&gt; &quot;*&quot; &lt;~ &quot;)&quot; ^^ { case _ =&gt; Count(Literal(1)) } | COUNT ~ &quot;(&quot; ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; Count(exp) } | COUNT ~&gt; &quot;(&quot; ~&gt; DISTINCT ~&gt; repsep(expression, &quot;,&quot;) &lt;~ &quot;)&quot; ^^ { case exps =&gt; CountDistinct(exps) } | APPROXIMATE ~ COUNT ~ &quot;(&quot; ~ DISTINCT ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; ApproxCountDistinct(exp) } | APPROXIMATE ~&gt; &quot;(&quot; ~&gt; floatLit ~ &quot;)&quot; ~ COUNT ~ &quot;(&quot; ~ DISTINCT ~ expression &lt;~ &quot;)&quot; ^^ { case s ~ _ ~ _ ~ _ ~ _ ~ e =&gt; ApproxCountDistinct(e, s.toDouble) } | FIRST ~ &quot;(&quot; ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; First(exp) } | LAST ~ &quot;(&quot; ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; Last(exp) } | AVG ~ &quot;(&quot; ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; Average(exp) } | MIN ~ &quot;(&quot; ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; Min(exp) } | MAX ~ &quot;(&quot; ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; Max(exp) } | UPPER ~ &quot;(&quot; ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; Upper(exp) } | LOWER ~ &quot;(&quot; ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; Lower(exp) } | IF ~ &quot;(&quot; ~&gt; expression ~ (&quot;,&quot; ~&gt; expression) ~ (&quot;,&quot; ~&gt; expression) &lt;~ &quot;)&quot; ^^ { case c ~ t ~ f =&gt; If(c, t, f) } | CASE ~&gt; expression.? ~ rep1(WHEN ~&gt; expression ~ (THEN ~&gt; expression)) ~ (ELSE ~&gt; expression).? &lt;~ END ^^ { case casePart ~ altPart ~ elsePart =&gt; val branches = altPart.flatMap { case whenExpr ~ thenExpr =&gt; Seq(whenExpr, thenExpr) } ++ elsePart casePart.map(CaseKeyWhen(_, branches)).getOrElse(CaseWhen(branches)) } | (SUBSTR | SUBSTRING) ~ &quot;(&quot; ~&gt; expression ~ (&quot;,&quot; ~&gt; expression) &lt;~ &quot;)&quot; ^^ { case s ~ p =&gt; Substring(s, p, Literal(Integer.MAX_VALUE)) } | (SUBSTR | SUBSTRING) ~ &quot;(&quot; ~&gt; expression ~ (&quot;,&quot; ~&gt; expression) ~ (&quot;,&quot; ~&gt; expression) &lt;~ &quot;)&quot; ^^ { case s ~ p ~ l =&gt; Substring(s, p, l) } | COALESCE ~ &quot;(&quot; ~&gt; repsep(expression, &quot;,&quot;) &lt;~ &quot;)&quot; ^^ { case exprs =&gt; Coalesce(exprs) } | SQRT ~ &quot;(&quot; ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; Sqrt(exp) } | ABS ~ &quot;(&quot; ~&gt; expression &lt;~ &quot;)&quot; ^^ { case exp =&gt; Abs(exp) } | ident ~ (&quot;(&quot; ~&gt; repsep(expression, &quot;,&quot;)) &lt;~ &quot;)&quot; ^^ { case udfName ~ exprs =&gt; UnresolvedFunction(udfName, exprs) } ) // CAST 关键字的类型转换 protected lazy val cast: Parser[Expression] = CAST ~ &quot;(&quot; ~&gt; expression ~ (AS ~&gt; dataType) &lt;~ &quot;)&quot; ^^ { case exp ~ t =&gt; Cast(exp, t) } // 解析常量的入口，可以看到包括数字、布尔常量、字符串常量和 NULL protected lazy val literal: Parser[Literal] = ( numericLiteral | booleanLiteral | stringLit ^^ {case s =&gt; Literal.create(s, StringType) } | NULL ^^^ Literal.create(null, NullType) ) // 解析布尔常量 protected lazy val booleanLiteral: Parser[Literal] = ( TRUE ^^^ Literal.create(true, BooleanType) | FALSE ^^^ Literal.create(false, BooleanType) ) // 解析数字常量 protected lazy val numericLiteral: Parser[Literal] = signedNumericLiteral | unsignedNumericLiteral // 解析正负符号 protected lazy val sign: Parser[String] = &quot;+&quot; | &quot;-&quot; // 解析有符号数字常量 protected lazy val signedNumericLiteral: Parser[Literal] = ( sign ~ numericLit ^^ { case s ~ l =&gt; Literal(toNarrowestIntegerType(s + l)) } | sign ~ floatLit ^^ { case s ~ f =&gt; Literal((s + f).toDouble) } ) // 解析无符号数字常量 protected lazy val unsignedNumericLiteral: Parser[Literal] = ( numericLit ^^ { n =&gt; Literal(toNarrowestIntegerType(n)) } | floatLit ^^ { f =&gt; Literal(f.toDouble) } ) private def toNarrowestIntegerType(value: String): Any = { val bigIntValue = BigDecimal(value) bigIntValue match { case v if bigIntValue.isValidInt =&gt; v.toIntExact case v if bigIntValue.isValidLong =&gt; v.toLongExact case v =&gt; v.underlying() } } // 解析浮点数 protected lazy val floatLit: Parser[String] = ( &quot;.&quot; ~&gt; unsignedNumericLiteral ^^ { u =&gt; &quot;0.&quot; + u } | elem(&quot;decimal&quot;, _.isInstanceOf[lexical.FloatLit]) ^^ (_.chars) ) protected lazy val signedPrimary: Parser[Expression] = sign ~ primary ^^ { case s ~ e =&gt; if (s == &quot;-&quot;) UnaryMinus(e) else e} protected lazy val primary: PackratParser[Expression] = ( literal | expression ~ (&quot;[&quot; ~&gt; expression &lt;~ &quot;]&quot;) ^^ { case base ~ ordinal =&gt; UnresolvedExtractValue(base, ordinal) } | (expression &lt;~ &quot;.&quot;) ~ ident ^^ { case base ~ fieldName =&gt; UnresolvedExtractValue(base, Literal(fieldName)) } | cast | &quot;(&quot; ~&gt; expression &lt;~ &quot;)&quot; | function | dotExpressionHeader | ident ^^ {case i =&gt; UnresolvedAttribute.quoted(i)} | signedPrimary | &quot;~&quot; ~&gt; expression ^^ BitwiseNot ) // a.b[.c ...] protected lazy val dotExpressionHeader: Parser[Expression] = (ident &lt;~ &quot;.&quot;) ~ ident ~ rep(&quot;.&quot; ~&gt; ident) ^^ { case i1 ~ i2 ~ rest =&gt; UnresolvedAttribute(Seq(i1, i2) ++ rest) } // 投影表达式的顶层 protected lazy val projection: Parser[Expression] = expression ~ (AS.? ~&gt; ident.?) ^^ { case e ~ a =&gt; a.fold(e)(Alias(e, _)()) } // 用于 FROM 的 relation 们 protected lazy val relations: Parser[LogicalPlan] = // rep1 相当于正则表达式的{1,}或+ ( relation ~ rep1(&quot;,&quot; ~&gt; relation) ^^ { case r1 ~ joins =&gt; joins.foldLeft(r1) { case(lhs, r) =&gt; Join(lhs, r, Inner, None) } } | relation ) // 解析单个 relation protected lazy val relation: Parser[LogicalPlan] = joinedRelation | relationFactor protected lazy val relationFactor: Parser[LogicalPlan] = ( rep1sep(ident, &quot;.&quot;) ~ (opt(AS) ~&gt; opt(ident)) ^^ { case tableIdent ~ alias =&gt; UnresolvedRelation(tableIdent, alias) } | (&quot;(&quot; ~&gt; start &lt;~ &quot;)&quot;) ~ (AS.? ~&gt; ident) ^^ { case s ~ a =&gt; Subquery(a, s) } ) protected lazy val joinedRelation: Parser[LogicalPlan] = relationFactor ~ rep1(joinType.? ~ (JOIN ~&gt; relationFactor) ~ joinConditions.?) ^^ { case r1 ~ joins =&gt; joins.foldLeft(r1) { case (lhs, jt ~ rhs ~ cond) =&gt; Join(lhs, rhs, joinType = jt.getOrElse(Inner), cond) } } // 条件 JOIN protected lazy val joinConditions: Parser[Expression] = ON ~&gt; expression // JOIN 类型 protected lazy val joinType: Parser[JoinType] = ( INNER ^^^ Inner | LEFT ~ SEMI ^^^ LeftSemi | LEFT ~ OUTER.? ^^^ LeftOuter | RIGHT ~ OUTER.? ^^^ RightOuter | FULL ~ OUTER.? ^^^ FullOuter ) // 排序类型 protected lazy val sortType: Parser[LogicalPlan =&gt; LogicalPlan] = ( ORDER ~ BY ~&gt; ordering ^^ { case o =&gt; l: LogicalPlan =&gt; Sort(o, true, l) } | SORT ~ BY ~&gt; ordering ^^ { case o =&gt; l: LogicalPlan =&gt; Sort(o, false, l) } ) protected lazy val ordering: Parser[Seq[SortOrder]] = ( rep1sep(expression ~ direction.? , &quot;,&quot;) ^^ { case exps =&gt; exps.map(pair =&gt; SortOrder(pair._1, pair._2.getOrElse(Ascending))) } ) protected lazy val direction: Parser[SortDirection] = ( ASC ^^^ Ascending | DESC ^^^ Descending ) // SELECT 语句 protected lazy val select: Parser[LogicalPlan] = SELECT ~&gt; DISTINCT.? ~ repsep(projection, &quot;,&quot;) ~ (FROM ~&gt; relations).? ~ (WHERE ~&gt; expression).? ~ (GROUP ~ BY ~&gt; rep1sep(expression, &quot;,&quot;)).? ~ (HAVING ~&gt; expression).? ~ sortType.? ~ (LIMIT ~&gt; expression).? ^^ { case d ~ p ~ r ~ f ~ g ~ h ~ o ~ l =&gt; val base = r.getOrElse(OneRowRelation) val withFilter = f.map(Filter(_, base)).getOrElse(base) val withProjection = g .map(Aggregate(_, assignAliases(p), withFilter)) .getOrElse(Project(assignAliases(p), withFilter)) val withDistinct = d.map(_ =&gt; Distinct(withProjection)).getOrElse(withProjection) val withHaving = h.map(Filter(_, withDistinct)).getOrElse(withDistinct) val withOrder = o.map(_(withHaving)).getOrElse(withHaving) val withLimit = l.map(Limit(_, withOrder)).getOrElse(withOrder) withLimit } // 通过 UNION、INTERSET、EXCEPT 等串联 SELECT 语句 protected lazy val start1: Parser[LogicalPlan] = (select | (&quot;(&quot; ~&gt; select $lt;~ &quot;)&quot;)) * ( UNION ~ ALL ^^^ { (q1: LogicalPlan, q2: LogicalPlan) =&gt; Union(q1, q2) } | INTERSECT ^^^ { (q1: LogicalPlan, q2: LogicalPlan) =&gt; Intersect(q1, q2) } | EXCEPT ^^^ { (q1: LogicalPlan, q2: LogicalPlan) =&gt; Except(q1, q2)} | UNION ~ DISTINCT.? ^^^ { (q1: LogicalPlan, q2: LogicalPlan) =&gt; Distinct(Union(q1, q2)) } ) // INSERT 语句 protected lazy val insert: Parser[LogicalPlan] = INSERT ~&gt; (OVERWRITE ^^^ true | INTO ^^^ false) ~ (TABLE ~&gt; relation) ~ select ^^ { case o ~ r ~ s =&gt; InsertIntoTable(r, Map.empty[String, Option[String]], s, o, false) } protected lazy val cte: Parser[LogicalPlan] = WITH ~&gt; rep1sep(ident ~ ( AS ~ &quot;(&quot; ~&gt; start1 &lt;~ &quot;)&quot;), &quot;,&quot;) ~ (start1 | insert) ^^ { case r ~ s =&gt; With(s, r.map({case n ~ s =&gt; (n, Subquery(n, s))}).toMap) }} 总结下来，SQL 语句与 LogicalPlan 实现类之间的完整对应关系是这样的： DDLParser： 实现类 SQL 语句 CreateTableUsingAsSelect CREATE [TEMPORARY] TABLE [IF NOT EXISTS] &lt;Table&gt; [(&lt;Columns&gt;])] USING &lt;ClassName&gt;] [OPTIONS &lt;Options&gt;] AS ... CreateTableUsing CREATE [TEMPORARY] TABLE [IF NOT EXISTS] &lt;Table&gt; [(&lt;Columns&gt;])] USING &lt;ClassName&gt; [OPTIONS &lt;Options&gt;] DescribeCommand DESCRIBE [EXTENDED] [&lt;Database&gt;.]&lt;Table&gt; RefreshTable REFRESH TABLE [&lt;Database&gt;.]&lt;Table&gt; SparkSQLParser： 实现类 SQL 语句 CacheTableCommand CACHE [LAZY] TABLE &lt;Table&gt; [AS ...] UncacheTableCommand UNCACHE TABLE &lt;Table&gt; ClearCacheCommand CLEAR CACHE SetCommand SET &lt;key&gt;=&lt;value&gt; ShowTablesCommand SHOW TABLES [IN &lt;Database&gt;] 总结在本篇文章中，我们通过更加仔细地阅读 DDLParser 、 SparkSQLParser 和 SqlParser 的源代码，彻底理解了 SQL 语句与抽象语法树结点之间的一一对应关系。相信这些 case class 树节点大家阅读起来应该是十分轻松愉快的一件事，这里就不赘述了。 下一次，我们将脱离 SparkSQL 常规执行工作流，开始探究 SparkSQL 的 CacheManager，敬请期待。","link":"/sparksql_catalyst_source_6/"},{"title":"Groovy 教程 - GDK","text":"这是一篇译文，读者可前往 Groovy Getting Started - The Groovy Development Kit 阅读原文。 1 I/O Groovy 为 I/O 提供了大量的便捷方法。尽管你仍然可以在 Groovy 中使用标准的 Java 代码，但 Groovy 提供了更多方便的途径来处理文件、流等。 具体来说，你应该了解一下添加至如下类的方法： java.io.File：http://docs.groovy-lang.org/latest/html/groovy-jdk/java/io/File.html java.io.InputStream：http://docs.groovy-lang.org/latest/html/groovy-jdk/java/io/InputStream.html java.io.OutputStream：http://docs.groovy-lang.org/latest/html/groovy-jdk/java/io/OutputStream.html java.io.Reader：http://docs.groovy-lang.org/latest/html/groovy-jdk/java/io/Reader.html java.io.Writer：http://docs.groovy-lang.org/latest/html/groovy-jdk/java/io/Writer.html java.nio.file.Path：http://docs.groovy-lang.org/latest/html/groovy-jdk/java/nio/file/Path.html 接下来的内容将重点介绍如何使用上述便捷方法但并不会对所有的这些方法进行完整描述，具体请查阅 GDK API。 1.1 读取文件 在第一个例子中，我们先来看看如何在 Groovy 中打印一个文本文件中的内容： 123new File(baseDir, 'haiku.txt').eachLine { line -&gt; println line} eachLine 方法是由 Groovy 自动添加到 File 类中的新方法而且有很多的变体，例如如果你想要知道文件的行号，你可以使用如下这个变体： 123new File(baseDir, 'haiku.txt').eachLine { line, nb -&gt; println &quot;Line $nb: $line&quot;} 如果 eachLine 的方法体抛出了异常的话，eachLine 方法会确保所有相关资源都被正确地关闭。这一点对于所有由 Groovy 添加的 I/O 方法来说都是相同的。 例如在某些时候你更想使用 Reader，但依然想利用上 Groovy 的自动资源管理。在下面的例子中，即使抛出了异常，所使用的 Reader 依然会被关闭： 12345678def count = 0, MAXSIZE = 3new File(baseDir,&quot;haiku.txt&quot;).withReader { reader -&gt; while (reader.readLine()) { if (++count &gt; MAXSIZE) { throw new RuntimeException('Haiku should only have 3 verses') } }} 如果你需要将一个文本文件的每一行内容放入到一个列表中，你可以这样做： 1def list = new File(baseDir, 'haiku.txt').collect {it} 或者你也可以使用 as 操作符将文本文件每一行的内容放入到一个数组中： 1def array = new File(baseDir, 'haiku.txt') as String[] 你有试过把文件的内容读入到一个 byte[] 中吗？这么做需要写多少代码呢？Groovy 则使得这么做变得十分简单： 1byte[] contents = file.bytes I/O 功能并不局限于文件读写。实际上，很大一部分操作依赖于输入输出流，因此 Groovy 为它们添加了大量的便捷方法，正如你在它们的文档中看到的那样。 例如，你很容易就能够从一个 File 中获取一个 InputStream： 123def is = new File(baseDir,'haiku.txt').newInputStream()// 做一些事情 ...is.close() 然而，正如你所看到的那样，这样做会需要你自己关闭这个 InputStream。实际上，在 Groovy 中使用 withInputStream 方法来处理资源管理是更好的选择： 123new File(baseDir,'haiku.txt').withInputStream { stream -&gt; // 做一些事情 ...} 1.2 写入文件 当然了，在某些情况下你可能会想要往文件中写入内容而不是读取内容。其中一种做法是使用 Writer： 12345new File(baseDir,'haiku.txt').withWriter('utf-8') { writer -&gt; writer.writeLine 'Into the ancient pond' writer.writeLine 'A frog jumps' writer.writeLine 'Water’s sound!'} 但对于这么简单的功能，使用 &lt;&lt; 运算符也许也足够了： 123new File(baseDir,'haiku.txt') &lt;&lt; '''Into the ancient pondA frog jumpsWater’s sound!''' 当然了，我们并不总是只需要处理文本内容，所以你也可以使用 Writer 或者像如下示例那样直接写入字节： 1file.bytes = [66,22,11] 当然，你也可以直接处理输出流。例如，你可以像这个样子来创建一个能写入到文件的输出流： 123def os = new File(baseDir,'data.bin').newOutputStream()// 做一些事情 ...os.close() 然而，正如你所见，这么做需要你自己关闭该输出流。同样，使用 withOutputStream 方法是更好的做法，因为它能处理抛出的异常并最终能在任何情况下关闭输出流： 123new File(baseDir,'data.bin').withOutputStream { stream -&gt; // 做一些事情 ...} 1.3 遍历文件树 在编写脚本的时候我们经常会需要遍历文件树来找到某些特定的文件并进行一些处理。Groovy 为此提供了多种不同的方法。例如你可以对文件夹中的所有文件执行指定的操作： 123456dir.eachFile { file -&gt; // 注1 println file.name}dir.eachFileMatch(~/.*\\.txt/) { file -&gt; // 注2 println file.name} &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;对文件夹中的所有文件执行给定的闭包代码&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;对文件夹中所有匹配指定模式的文件执行给定的闭包代码&lt;/td&gt; &lt;/tr&gt; 有时你还需要处理更深的文件层次，这时候你就需要使用 eachFileRecurse 了： 1234567dir.eachFileRecurse { file -&gt; // 注1 println file.name}dir.eachFileRecurse(FileType.FILES) { file -&gt; // 注2 println file.name} &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;从该目录开始递归地查找所有文件或目录并执行指定的闭包代码&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;从该目录开始递归地查找所有文件并执行指定的闭包代码&lt;/td&gt; &lt;/tr&gt; 对于更复杂的遍历操作你可以使用 traverse 方法，这需要你要返回特殊的标识位来指示如何进行遍历： 123456789dir.traverse { file -&gt; if (file.directory &amp;&amp; file.name=='bin') { FileVisitResult.TERMINATE // 注1 } else { println file.name FileVisitResult.CONTINUE // 注2 }} 1 如果该文件为一个文件夹且名称为 `bin` 则停止遍历 2 否则打印文件的名称并继续遍历 1.4 数据与对象 在 Java 中，通过 java.io.DataOutputStream 和 java.io.DataInputStream 类来对数据进行序列化和反序列化并不少见，而 Groovy 则让这个过程变得更为简单。例如，你可以使用如下代码来将数据序列化到文件中并读取： 12345678910111213boolean b = trueString message = 'Hello from Groovy'// 将数据序列化至一个文件file.withDataOutputStream { out -&gt; out.writeBoolean(b) out.writeUTF(message)}// ...// 然后重新读取回来file.withDataInputStream { input -&gt; assert input.readBoolean() == b assert input.readUTF() == message} 同样的，如果你想要序列化的数据实现了 Serializable 接口，你还可以像如下代码那样使用 ObjectOutputStream： 123456789101112Person p = new Person(name:'Bob', age:76)// 将数据序列化至一个文件file.withObjectOutputStream { out -&gt; out.writeObject(p)}// ...// 然后读取回来file.withObjectInputStream { input -&gt; def p2 = input.readObject() assert p2.name == p.name assert p2.age == p.age} 1.5 执行外部进程 在上面的章节中我们看到了 Groovy 处理文件、Reader 和输入输出流有多简便。然而，在诸如系统管理或者 DevOps 这样的领域中，我们则需要 Groovy 脚本能够与外部进程进行通信。 Groovy 提供了一种十分简单的方法来执行命令行进程，只要把命令行写作一个简单的 String 对象然后调用其 execute() 方法即可。例如，在一个 *nix 机器上（或者一个安装了合适的 *nix 命令行环境的 Windows 机器上），你可以这样做： 12def process = &quot;ls -l&quot;.execute() // 注1println &quot;Found text ${process.text}&quot; // 注2 1 在一个外部进程中执行 `ls` 命令 2 消耗命令的输出并将其作为文本进行读取 execute() 方法会返回一个 java.lang.Process 对象，借由此我们可以对标准输入/标准输出/错误输出流进行处理，或者检查进程退出时的退出值。 例如，这里我们执行与上例相同的命令，但我们将逐行地处理其输出流： 1234def process = &quot;ls -l&quot;.execute() // 注1process.in.eachLine { line -&gt; // 注2 println line // 注3} 1 在一个外部进程中执行 `ls` 命令 2 对于该进程的输入流中的每一行内容 3 输出该内容 值得注意的是 in 代表的输入流对应着命令的标准输出，而你可以通过 out 代表的输出流向进程的标准输入写入数据。 注意，有不少命令实际上是 Shell 的内置功能，需要一些特殊的处理。所以如果你想要在一个 Windows 机器上列出一个文件夹内的所有文件，然后这样写的话： 12def process = &quot;dir&quot;.execute()println &quot;${process.text}&quot; 你会得到一个 IOException，内容如下：Cannot run program &quot;dir&quot;: CreateProcess error=2, The system cannot find the file specified. 这是因为 dir 实际上是 Windows Shell（cmd.exe）的一个内置功能，不能被当做一个单纯的可执行文件来运行。因此，你应该这样写： 12def process = &quot;cmd /c dir&quot;.execute()println &quot;${process.text}&quot; 除此之外，由于这个功能实际上是通过 java.lang.Process 实现的，因此我们也应该考虑到这个类的一些不足之处。具体来说，它的 JavaDoc 是这么说的： 因为有些平台只为标准输入和输出流提供了很有限的缓存空间，写入输入流和读取输出流发生错误时可能会导致子进程发生阻塞，甚至发生死锁。 正是因为这个原因，Groovy 提供了一些额外的便捷方法来更好地处理外部进程的输入输出流。 通过如下代码你可以消耗掉进程的所有输出（包括错误流输出）： 123def p = &quot;rm -f foo.tmp&quot;.execute([], tmpDir)p.consumeProcessOutput()p.waitFor() consumeProcessOutput 方法还包括其他一些变体可以利用 StringBuffer、InputStream、OutputStream 等，详见 java.lang.Process 的 GDK API。 除此之外，还有一个 pipeTo 方法（对应于 | 操作符且可进行重载）可以将一个进程的输出流内容转移到另一个进程的输入流中。 如下为使用该方法的案例。 1234567891011proc1 = 'ls'.execute()proc2 = 'tr -d o'.execute()proc3 = 'tr -d e'.execute()proc4 = 'tr -d i'.execute()proc1 | proc2 | proc3 | proc4proc4.waitFor()if (proc4.exitValue()) { println proc4.err.text} else { println proc4.text} 消耗错误流输出： 1234567891011121314def sout = new StringBuilder()def serr = new StringBuilder()proc2 = 'tr -d o'.execute()proc3 = 'tr -d e'.execute()proc4 = 'tr -d i'.execute()proc4.consumeProcessOutput(sout, serr)proc2 | proc3 | proc4[proc2, proc3].each { it.consumeProcessErrorStream(serr) }proc2.withWriter { writer -&gt; writer &lt;&lt; 'testfile.groovy'}proc4.waitForOrKill(1000)println &quot;Standard output: $sout&quot;println &quot;Standard error: $serr&quot; 2 集合 Groovy 为各种不同的集合类型提供了原生的语言支持，包括列表、映射和范围。这些集合类大多数都基于 Java 原本的集合类型，同时加上了 GDK 特有的方法。 2.1 列表2.1.1 列表字面量 你可以像如下代码那样创建列表。注意 [] 是空列表表达式。 123456789def list = [5, 6, 7, 8]assert list.get(2) == 7assert list[2] == 7assert list instanceof java.util.Listdef emptyList = []assert emptyList.size() == 0emptyList.add(5)assert emptyList.size() == 1 每一个列表表达式都会创建一个 java.util.List 实现类。 当然，列表也可以用于创建另一个列表： 123456789def list1 = ['a', 'b', 'c']// 创建一个包含 `list1` 元素的新列表def list2 = new ArrayList&lt;String&gt;(list1)assert list2 == list1 // == checks that each corresponding element is the same// clone() can also be calleddef list3 = list1.clone()assert list3 == list1 列表实际上就是对象的有序集合： 12345678910111213141516171819202122232425262728def list = [5, 6, 7, 8]assert list.size() == 4assert list.getClass() == ArrayList // 具体使用的列表实现类assert list[2] == 7 // 元素索引值从 0 开始assert list.getAt(2) == 7 // 下标运算符 [] 的等价方法assert list.get(2) == 7 // 另一个可用的方法list[2] = 9assert list == [5, 6, 9, 8,] // 在尾部再加一个逗号也是可以的list.putAt(2, 10) // 使用 [] 修改元素值的等价方法assert list == [5, 6, 10, 8]assert list.set(2, 11) == 10 // 可用的另一个修改元素的方法，返回旧的元素值assert list == [5, 6, 11, 8]assert ['a', 1, 'a', 'a', 2.5, 2.5f, 2.5d, 'hello', 7g, null, 9 as byte]// 可以包含重复或不同类型的元素assert [1, 2, 3, 4, 5][-1] == 5 // 使用负索引值从列表末尾开始访问元素assert [1, 2, 3, 4, 5][-2] == 4assert [1, 2, 3, 4, 5].getAt(-2) == 4 // getAt() 同样接受负索引值try { [1, 2, 3, 4, 5].get(-2) // 但 get() 不接受负索引值 assert false} catch (e) { assert e instanceof ArrayIndexOutOfBoundsException} 2.1.2 将列表作为布尔表达式列表可以被估作一个 boolean 值： 1234assert ![] // 空白列表会被视作 `false` 值// 其他所有列表，无论其内容，都会被视作 `true`assert [1] &amp;&amp; ['a'] &amp;&amp; [0] &amp;&amp; [0.0] &amp;&amp; [false] &amp;&amp; [null] 2.1.3 遍历列表通常我们可以通过调用 each 或 eachWithIndex 方法来遍历列表的所有元素并给定处理元素的代码： 123456[1, 2, 3].each { println &quot;Item: $it&quot; // `it` 是一个隐式参数，指代元素的索引值}['a', 'b', 'c'].eachWithIndex { it, i -&gt; // `it` 为当前元素，而 `i` 为当前索引值 println &quot;$i: $it&quot;} 除了遍历列表，有时我们还需要对一个列表的元素进行转换进而构建出另一个新的列表。这个操作，又被称为映射，在 Groovy 中可通过 collect 方法完成： 123456789assert [1, 2, 3].collect { it * 2 } == [2, 4, 6]// `collect` 的另一种快捷写法assert [1, 2, 3]*.multiply(2) == [1, 2, 3].collect { it.multiply(2) }def list = [0]// 可以通过给 `collect` 方法给定的列表放入新产生的元素assert [1, 2, 3].collect(list) { it * 2 } == [0, 2, 4, 6]assert list == [0, 2, 4, 6] 2.1.4 过滤和查找1234567891011121314151617181920212223242526272829303132333435assert [1, 2, 3].find { it &gt; 1 } == 2 // 查找第一个匹配给定条件的元素assert [1, 2, 3].findAll { it &gt; 1 } == [2, 3] // 查找所有匹配给定条件的元素assert ['a', 'b', 'c', 'd', 'e'].findIndexOf { // 查找第一个匹配给定条件的元素的索引值 it in ['c', 'e', 'g']} == 2assert ['a', 'b', 'c', 'd', 'c'].indexOf('c') == 2 // 返回索引值assert ['a', 'b', 'c', 'd', 'c'].indexOf('z') == -1 // 返回的索引值为 -1 意味着元素未找到assert ['a', 'b', 'c', 'd', 'c'].lastIndexOf('c') == 4assert [1, 2, 3].every { it &lt; 5 } // 如果所有元素都满足给定的条件则返回 trueassert ![1, 2, 3].every { it &lt; 3 }assert [1, 2, 3].any { it &gt; 2 } // 如果存在满足给定条件的元素则返回 trueassert ![1, 2, 3].any { it &gt; 3 }assert [1, 2, 3, 4, 5, 6].sum() == 21 // 使用元素的 plus() 方法来返回元素的总和值assert ['a', 'b', 'c', 'd', 'e'].sum { it == 'a' ? 1 : it == 'b' ? 2 : it == 'c' ? 3 : it == 'd' ? 4 : it == 'e' ? 5 : 0 // 使用自定义的值来求和} == 15assert ['a', 'b', 'c', 'd', 'e'].sum { ((char) it) - ((char) 'a') } == 10assert ['a', 'b', 'c', 'd', 'e'].sum() == 'abcde'assert [['a', 'b'], ['c', 'd']].sum() == ['a', 'b', 'c', 'd']// 可以提供一个给定的初始值assert [].sum(1000) == 1000assert [1, 2, 3].sum(1000) == 1006assert [1, 2, 3].join('-') == '1-2-3' // 字符串拼接assert [1, 2, 3].inject('counting: ') { str, item -&gt; str + item // 归约操作} == 'counting: 123'assert [1, 2, 3].inject(0) { count, item -&gt; count + item} == 6 Groovy 还提供了在集合中查找最大值和最小值的方法： 1234567891011def list = [9, 4, 2, 10, 5]assert list.max() == 10assert list.min() == 2// 我们还可以比较包括字符在内的可比较的对象assert ['x', 'y', 'a', 'z'].min() == 'a'// 我们还可以通过闭包来给定排序的行为def list2 = ['abc', 'z', 'xyzuvw', 'Hello', '321']assert list2.max { it.size() } == 'xyzuvw'assert list2.min { it.size() } == 'z' 除了闭包，你还可以使用 Comparator 来定义大小比较规则： 1234567891011121314Comparator mc = { a, b -&gt; a == b ? 0 : (a &lt; b ? -1 : 1) }def list = [7, 4, 9, -6, -1, 11, 2, 3, -9, 5, -13]assert list.max(mc) == 11assert list.min(mc) == -13Comparator mc2 = { a, b -&gt; a == b ? 0 : (Math.abs(a) &lt; Math.abs(b)) ? -1 : 1 }assert list.max(mc2) == -13assert list.min(mc2) == -1assert list.max { a, b -&gt; a.equals(b) ? 0 : Math.abs(a) &lt; Math.abs(b) ? -1 : 1 } == -13assert list.min { a, b -&gt; a.equals(b) ? 0 : Math.abs(a) &lt; Math.abs(b) ? -1 : 1 } == -1 2.1.5 添加和移除元素我们可以使用 [] 来创建一个新的空列表并用 &lt;&lt; 来向其中追加元素： 1234567891011121314151617def list = []assert list.emptylist &lt;&lt; 5assert list.size() == 1list &lt;&lt; 7 &lt;&lt; 'i' &lt;&lt; 11assert list == [5, 7, 'i', 11]list &lt;&lt; ['m', 'o']assert list == [5, 7, 'i', 11, ['m', 'o']]// &lt;&lt; 调用链的第一个对象为目标列表assert ([1, 2] &lt;&lt; 3 &lt;&lt; [4, 5] &lt;&lt; 6) == [1, 2, 3, [4, 5], 6]// 使用 leftShift 方法等价于使用 &lt;&lt; 方法assert ([1, 2, 3] &lt;&lt; 4) == ([1, 2, 3].leftShift(4)) 除此之外很有很多种向列表中添加元素的方式： 1234567891011121314151617181920212223242526272829assert [1, 2] + 3 + [4, 5] + 6 == [1, 2, 3, 4, 5, 6]// 等价于调用 `plus` 方法assert [1, 2].plus(3).plus([4, 5]).plus(6) == [1, 2, 3, 4, 5, 6]def a = [1, 2, 3]a += 4 // 创建一个新的列表并赋值给 `a`a += [5, 6]assert a == [1, 2, 3, 4, 5, 6]assert [1, *[222, 333], 456] == [1, 222, 333, 456]assert [*[1, 2, 3]] == [1, 2, 3]assert [1, [2, 3, [4, 5], 6], 7, [8, 9]].flatten() == [1, 2, 3, 4, 5, 6, 7, 8, 9]def list = [1, 2]list.add(3)list.addAll([5, 4])assert list == [1, 2, 3, 5, 4]list = [1, 2]list.add(1, 3) // 在索引值 1 之前添加元素 3assert list == [1, 3, 2]list.addAll(2, [5, 4]) // 在索引值 2 之前添加元素 5 和 4assert list == [1, 3, 5, 4, 2]list = ['a', 'b', 'z', 'e', 'u', 'v', 'g']list[8] = 'x' // [] 操作符在有需要的时候会扩充列表// 并在需要的位置插入 null 值assert list == ['a', 'b', 'z', 'e', 'u', 'v', 'g', null, 'x'] 然而，值得注意的是，对列表使用 + 运算符并不会改变原列表。比起 &lt;&lt;，它会产生出一个新的列表，很多时候这可能不是你想要的效果进而带来一些性能上的问题。 GDK 同样包含一些可以让你很方便地从列表中移除元素的方法： 12345678assert ['a','b','c','b','b'] - 'c' == ['a','b','b','b']assert ['a','b','c','b','b'] - 'b' == ['a','c']assert ['a','b','c','b','b'] - ['b','c'] == ['a']def list = [1,2,3,4,3,2,1]list -= 3 // 通过从原本的列表中移除 `3` 来创建一个新的列表assert list == [1,2,4,2,1]assert ( list -= [2,4] ) == [1,1] 同样，我们还可以通过给定元素的索引值来移除元素，而这种情况则会改变原本的列表： 123def list = [1,2,3,4,5,6,2,2,1]assert list.removeAt(2) == 3 // 移除第三个元素并返回assert list == [1,2,4,5,6,2,2,1] 如果你只是想移除列表中第一个拥有给定值的元素而不是移除所有元素，你可以使用 remove 方法： 123456def list= ['a','b','c','b','b']assert list.remove('c') // 移除 'c'。由于成功找到元素并移除，因此返回 `true`assert list.remove('b') // 移除第一个 'b'，返回 `true`assert ! list.remove('z') // 由于找不到元素 'z'，返回 `false`assert list == ['a','b','b'] 通过 clear 方法可以移除列表中的所有元素： 123def list= ['a',2,'c',4]list.clear()assert list == [] 2.1.6 集合操作GDK 还提供了可以更好地进行集合操作的方法： 12345678910111213assert 'a' in ['a','b','c'] // 如果该列表包含给定的元素则返回 trueassert ['a','b','c'].contains('a') // 等价于 Java 的 `contains` 方法assert [1,3,4].containsAll([1,4]) // `containsAll` 方法会检查列表是否包含所有给定的元素assert [1,2,3,3,3,3,4,5].count(3) == 4 // 计算列表中拥有给定值的元素个数assert [1,2,3,3,3,3,4,5].count { it%2==0 // 计算列表中满足给定条件的元素个数} == 2assert [1,2,4,6,8,10,12].intersect([1,3,6,9,12]) == [1,6,12]assert [1,2,3].disjoint( [4,6,9] )assert ![1,2,3].disjoint( [2,4,6] ) 2.1.7 排序 使用集合时通常需要对其进行排序。Groovy 同样提供了多种排序列表的方式，可以使用闭包或是提供 Comparator，正如如下例子所示： 123456789101112131415161718192021222324assert [6, 3, 9, 2, 7, 1, 5].sort() == [1, 2, 3, 5, 6, 7, 9]def list = ['abc', 'z', 'xyzuvw', 'Hello', '321']assert list.sort { it.size()} == ['z', 'abc', '321', 'Hello', 'xyzuvw']def list2 = [7, 4, -6, -1, 11, 2, 3, -9, 5, -13]assert list2.sort { a, b -&gt; a == b ? 0 : Math.abs(a) &lt; Math.abs(b) ? -1 : 1 } == [-1, 2, 3, 4, 5, -6, 7, -9, 11, -13]Comparator mc = { a, b -&gt; a == b ? 0 : Math.abs(a) &lt; Math.abs(b) ? -1 : 1 }// 只可用于 JDK 8+// list2.sort(mc)// assert list2 == [-1, 2, 3, 4, 5, -6, 7, -9, 11, -13]def list3 = [6, -3, 9, 2, -7, 1, 5]Collections.sort(list3)assert list3 == [-7, -3, 1, 2, 5, 6, 9]Collections.sort(list3, mc)assert list3 == [1, 2, -3, 5, 6, -7, 9] 2.1.8 复制元素 GDK 还利用了运算符重载的功能为列表提供了复制元素的方法： 123456assert [1, 2, 3] * 3 == [1, 2, 3, 1, 2, 3, 1, 2, 3]assert [1, 2, 3].multiply(2) == [1, 2, 3, 1, 2, 3]assert Collections.nCopies(3, 'b') == ['b', 'b', 'b']// JDK 定义的 `nCopies` 则有着不同的语义assert Collections.nCopies(2, [1, 2]) == [[1, 2], [1, 2]] // 不是 [1,2,1,2] 2.2 映射2.2.1 映射字面量 在 Groovy 中，映射（又被称为联合数组）可使用映射字面量语法 [:] 创建： 123456789101112def map = [name: 'Gromit', likes: 'cheese', id: 1234]assert map.get('name') == 'Gromit'assert map.get('id') == 1234assert map['name'] == 'Gromit'assert map['id'] == 1234assert map instanceof java.util.Mapdef emptyMap = [:]assert emptyMap.size() == 0emptyMap.put(&quot;foo&quot;, 5)assert emptyMap.size() == 1assert emptyMap.get(&quot;foo&quot;) == 5 映射的键默认为字符串：[a:1] 等价于 ['a':1]。你有可能会没能意识到这种语句的含义，如果你定义了一个叫 a 的变量并且你想将它的值作为映射的键的话。如果你想要这样做的话，你应该像下面的例子那样为键加上括号来进行转义： 1234567def a = 'Bob'def ages = [a: 43]assert ages['Bob'] == null // 无法找到 `Bob`assert ages['a'] == 43 // 因为 `a` 是一个字面量ages = [(a): 43] // 现在我们通过为 `a` 加上括号来进行转义assert ages['Bob'] == 43 // 这样我们就能找到关联的值了 除了映射字面量，你还可以获取一个映射的拷贝： 123456789def map = [ simple : 123, complex: [a: 1, b: 2]]def map2 = map.clone()assert map2.get('simple') == map.get('simple')assert map2.get('complex') == map.get('complex')map2.get('complex').put('c', 3)assert map.get('complex').get('c') == 3 正如上面的例子所示，所得的映射只是原映射的浅拷贝。 2.2.2 映射属性访问语句 映射同样可以作为 Bean 使用，因此你也可以使用属性访问语句来访问映射，只要映射的键是字符串而且也是合法的 Groovy 标识符： 123456789def map = [name: 'Gromit', likes: 'cheese', id: 1234]assert map.name == 'Gromit' // 可用于替换 map.get('name')assert map.id == 1234def emptyMap = [:]assert emptyMap.size() == 0emptyMap.foo = 5assert emptyMap.size() == 1assert emptyMap.foo == 5 注意，按这种规则的话，map.foo 会导致 Groovy 从映射 map 中查找 foo。这意味着如果映射 map 不包含键 class 的话，map.class 会返回 null。如果你只是想要获取映射的 Class 对象，你只能直接使用 getClass() 方法 ： 1234567891011121314151617def map = [name: 'Gromit', likes: 'cheese', id: 1234]assert map.class == nullassert map.get('class') == nullassert map.getClass() == LinkedHashMap // 也许这才是你想要的map = [1 : 'a', (true) : 'p', (false): 'q', (null) : 'x', 'null' : 'z']assert map.containsKey(1) // `1` 并不是合法的标识符，所以只能这样写assert map.true == nullassert map.false == nullassert map.get(true) == 'p'assert map.get(false) == 'q'assert map.null == 'z'assert map.get(null) == 'x' 2.2.3 遍历映射正如之前那样，GDK 同样为映射提供了 each 和 eachWithIndex 方法来进行遍历。值得注意的是通过映射字面量表达式创建的映射是有序的，也就是说如果你尝试遍历映射，映射中的键值对将总是以其被添加到映射中的顺序被遍历。 12345678910111213141516171819202122232425def map = [ Bob : 42, Alice: 54, Max : 33]// `entry` 是映射中的一个键值对map.each { entry -&gt; println &quot;Name: $entry.key Age: $entry.value&quot;}// `entry` 是映射中的一个键值对，`i` 则为其索引值map.eachWithIndex { entry, i -&gt; println &quot;$i - Name: $entry.key Age: $entry.value&quot;}// 除此之外，你也可以直接访问被遍历的键和值map.each { key, value -&gt; println &quot;Name: $key Age: $value&quot;}// 还能直接访问键、值以及索引值 `i`map.eachWithIndex { key, value, i -&gt; println &quot;$i - Name: $key Age: $value&quot;} 2.2.4 添加和删除元素 可以通过 put 方法、putAll 方法或下标运算符来将一个元素添加到映射中： 12345678def defaults = [1: 'a', 2: 'b', 3: 'c', 4: 'd']def overrides = [2: 'z', 5: 'x', 13: 'x']def result = new LinkedHashMap(defaults)result.put(15, 't')result[17] = 'u'result.putAll(overrides)assert result == [1: 'a', 2: 'z', 3: 'c', 4: 'd', 5: 'x', 13: 'x', 15: 't', 17: 'u'] 调用 clear 方法可以移除映射中的所有元素： 1234def m = [1:'a', 2:'b']assert m.get(1) == 'a'm.clear()assert m == [:] 由映射字面量语法产生的映射依赖于键的 equals 和 hashCode 方法，因此你不应使用那些 hashCode 会发生变化的对象作为键，否则你很有可能无法获取到其关联的值。 除此之外值得注意的是，你不应使用 GString 作为映射的键，因为 GString 的哈希码和内容与其相同的 String 的哈希码是不同的： 12345def key = 'some key'def map = [:]def gstringKey = &quot;${key.toUpperCase()}&quot;map.put(gstringKey,'value')assert map.get('SOME KEY') == null 2.1.5 键、值与键值对 我们可以在一个视图中读取映射的键、值和键值对： 12345678910def map = [1:'a', 2:'b', 3:'c']def entries = map.entrySet()entries.each { entry -&gt; assert entry.key in [1,2,3] assert entry.value in ['a','b','c']}def keys = map.keySet()assert keys == [1,2,3] as Set 通过该试图来修改映射（修改其键或值或键值对）都是不可取的，因为这样的操作是否能顺利执行直接取决于其背后被修改的映射的类型。具体来说，Groovy 所使用的来自 JDK 的集合类并不保证映射可以安全地通过其 keySet、entrySet 或 values 视图进行修改。 2.1.6 过滤与查找 GDK 也为映射提供了与列表类似的过滤、查找和收集方法： 12345678910111213141516171819202122232425262728293031323334353637def people = [ 1: [name:'Bob', age: 32, gender: 'M'], 2: [name:'Johnny', age: 36, gender: 'M'], 3: [name:'Claire', age: 21, gender: 'F'], 4: [name:'Amy', age: 54, gender:'F']]def bob = people.find { it.value.name == 'Bob' } // 查找单一键值对def females = people.findAll { it.value.gender == 'F' }// 上述两个方法均返回键值对，但你可以使用 `collect` 方法来获取其域def ageOfBob = bob.value.agedef agesOfFemales = females.collect { it.value.age}assert ageOfBob == 32assert agesOfFemales == [21,54]// 你还可以使用键值对作为闭包的参数def agesOfMales = people.findAll { id, person -&gt; person.gender == 'M'}.collect { id, person -&gt; person.age}assert agesOfMales == [32, 36]// 如果所有键值对均满足给定的条件则 `every` 方法返回 true assert people.every { id, person -&gt; person.age &gt; 18}// 如果存在键值对满足给定的条件则 `any` 方法返回 trueassert people.any { id, person -&gt; person.age == 54} 2.1.7 分组 我们可以通过给定一个条件来让列表中的元素各自分组形成一个列表： 12345678910111213141516171819assert ['a', 7, 'b', [2, 3]].groupBy { it.class} == [(String) : ['a', 'b'], (Integer) : [7], (ArrayList): [[2, 3]]]assert [ [name: 'Clark', city: 'London'], [name: 'Sharma', city: 'London'], [name: 'Maradona', city: 'LA'], [name: 'Zhang', city: 'HK'], [name: 'Ali', city: 'HK'], [name: 'Liu', city: 'HK'],].groupBy { it.city } == [ London: [[name: 'Clark', city: 'London'], [name: 'Sharma', city: 'London']], LA : [[name: 'Maradona', city: 'LA']], HK : [[name: 'Zhang', city: 'HK'], [name: 'Ali', city: 'HK'], [name: 'Liu', city: 'HK']],] 2.3 区间 你可以使用区间（Range）来创建一个由连续值组成的列表。区间可以被直接用作 List 因为 Range 扩展了 java.util.List。 使用 .. 记号定义的区间是一个闭区间（也就是说该列表包含了起始值和终止值）。 使用 ..&lt; 记号定义的区间则是一个半开区间：它包含起始值但不包含终止值。 12345678910111213141516171819202122// 闭区间def range = 5..8assert range.size() == 4assert range.get(2) == 7assert range[2] == 7assert range instanceof java.util.Listassert range.contains(5)assert range.contains(8)// 半开区间range = 5..&lt;8assert range.size() == 3assert range.get(2) == 7assert range[2] == 7assert range instanceof java.util.Listassert range.contains(5)assert !range.contains(8)// 可以在不使用集体索引值的情况下获取区间的端点值range = 1..10assert range.from == 1assert range.to == 10 值得注意的是，int 类型区间的实现方式十分高效，实际上就是一个只包含了起始值和终止值的 Java 对象。 区间可以被用作任何实现了 java.lang.Comparable 接口用于进行大小比较，同时又有方法 next() 和 previous() 用于返回其上一个和下一个值的 Java 对象。例如，你可以创建一个由 String 元素组成的区间： 123456789// 闭区间def range = 'a'..'d'assert range.size() == 4assert range.get(2) == 'c'assert range[2] == 'c'assert range instanceof java.util.Listassert range.contains('a')assert range.contains('d')assert !range.contains('e') 你可以使用经典的 for 循环来迭代区间： 123for (i in 1..10) { println &quot;Hello ${i}&quot;} 但你也可以通过使用 each 方法来更 Groovy 地迭代区间： 123(1..10).each { i -&gt; println &quot;Hello ${i}&quot;} 区间还可用于 switch 语句： 12345switch (years) { case 1..10: interestRate = 0.076; break; case 11..25: interestRate = 0.052; break; default: interestRate = 0.037;} 2.4 集合类的语法增强2.4.1 GPath 支持多亏了列表和映射都支持属性访问语法，在 Groovy 中我们可以使用语法糖来更好地应对嵌套集合，如下例所示： 123456789def listOfMaps = [['a': 11, 'b': 12], ['a': 21, 'b': 22]]assert listOfMaps.a == [11, 21] // GPath 语法assert listOfMaps*.a == [11, 21] // 延伸点语法listOfMaps = [['a': 11, 'b': 12], ['a': 21, 'b': 22], null]assert listOfMaps*.a == [11, 21, null] // 可以很好地应对 null 值assert listOfMaps*.a == listOfMaps.collect { it?.a } // 判等语法// 但这时候只会收集非 null 值assert listOfMaps.a == [11,21] 2.4.2 延伸运算符 延伸运算符可用于将一个集合“内联”到另一个集合之中。这个语法糖主要为了使我们不需要调用 putAll 方法并能写出更简短的代码： 123456789101112131415assert [ 'z': 900, *: ['a': 100, 'b': 200], 'a': 300] == ['a': 300, 'b': 200, 'z': 900]// 用于映射定义的延伸映射语法assert [*: [3: 3, *: [5: 5]], 7: 7] == [3: 3, 5: 5, 7: 7]def f = { [1: 'u', 2: 'v', 3: 'w'] }assert [*: f(), 10: 'zz'] == [1: 'u', 10: 'zz', 2: 'v', 3: 'w']// 用于方法实参的延伸映射语法f = { map -&gt; map.c }assert f(*: ['a': 10, 'b': 20, 'c': 30], 'e': 50) == 30f = { m, i, j, k -&gt; [m, i, j, k] }// 使用延伸映射语法给定方法的具名参数和不具名参数assert f('e': 100, *[4, 5], *: ['a': 10, 'b': 20, 'c': 30], 6) == [[&quot;e&quot;: 100, &quot;b&quot;: 20, &quot;c&quot;: 30, &quot;a&quot;: 10], 4, 5, 6] 2.4.3 *. 运算符 星点运算符可用于调用集合中所有元素的某个方法或属性： 12345678assert [1, 3, 5] == ['a', 'few', 'words']*.size()class Person { String name int age}def persons = [new Person(name:'Hugo', age:17), new Person(name:'Sandra',age:19)]assert [17, 19] == persons*.age 2.4.4 使用下标运算符进行分割 你可以使用下标运算符根据索引值来访问列表、元素和映射的元素。有趣的是在这种情况下，字符串也会被视作特殊的集合： 123456789101112def text = 'nice cheese gromit!'def x = text[2]assert x == 'c'assert x.class == Stringdef sub = text[5..10]assert sub == 'cheese'def list = [10, 11, 12, 13]def answer = list[2,3]assert answer == [12,13] 值得注意的是你可以使用区间来获取集合中的一小部分： 123list = 100..200sub = list[1, 3, 20..25, 33]assert sub == [101, 103, 120, 121, 122, 123, 124, 125, 133] 对于那些可变的集合，下标运算符可用于更新集合的值： 123list = ['a','x','x','d']list[1..2] = ['b','c']assert list == ['a','b','c','d'] 除此之外，你还可以使用负索引值来更好地从集合末尾开始提取元素： 123456text = &quot;nice cheese gromit!&quot;x = text[-1]assert x == &quot;!&quot;def name = text[-7..-2]assert name == &quot;gromit&quot; 最后，如果你使用的是一个反向区间（起始值大于终止值），那么所得的结果也是反向的： 123text = &quot;nice cheese gromit!&quot;name = text[3..1]assert name == &quot;eci&quot; 2.5 新添加的集合方法 除了列表、映射和区间以外，Groovy 还为其他集合或更普通的 Iterable 类提供了更多的用于过滤、收集、分组、计数等方法。 有关这方面的内容，我们希望你能仔细阅读 GDK 的 API 文档。具体来说： 在这里可以找到 Iterable 的新方法 在这里可以找到 Iterator 的新方法 在这里可以找到 Collection 的新方法 在这里可以找到 List 的新方法 在这里可以找到 Map 的新方法 3 其他好用的功能3.1 ConfigSlurper ConfigSlurper 是可用于读取以 Groovy 脚本形式编写的配置文件的功能类。正如 Java 的 *.properties 文件那样，ConfigSlurper 也可以使用点号语法进行访问，除此之外它还能用闭包括号来给定配置值以及任意的对象类型： 1234567891011def config = new ConfigSlurper().parse(''' app.date = new Date() // 注1 app.age = 42 app { // 注2 name = &quot;Test${42}&quot; }''')assert config.app.date instanceof Dateassert config.app.age == 42assert config.app.name == 'Test42' 使用点号语法 使用闭包括号语法替代点号语法 正如我们在上一个例子中所见到的那样，parse 方法可用于获取一个 groovy.util.ConfigObject 实例。ConfigObject 是一种特殊的 java.util.Map 实现类，它要么返回具体的配置值要么返回一个新的 ConfigObject，但绝不会返回 null。 1234567def config = new ConfigSlurper().parse(''' app.date = new Date() app.age = 42 app.name = &quot;Test${42}&quot;''')assert config.test != null // 注1 我们并未给出 `config.test`，但在被调用时仍然返回了一个 `ConfigObject` 如果点号本身需要作为配置变量的名称的话，可以使用单引号或双引号对其进行转义： 12345def config = new ConfigSlurper().parse(''' app.&quot;person.age&quot; = 42''')assert config.app.&quot;person.age&quot; == 42 除此之外，ConfigSlurper 还支持不同的环境。environments 方法可被用于处理一个包含若干个配置小节的 Closure 实例。假设我们想要为开发环境创建一些特别的配置值。那么在创建 ConfigSlurper 实例时我们可以使用 ConfigSlurper(String) 构造器来给定目标环境： 1234567891011121314151617def config = new ConfigSlurper('development').parse(''' environments { development { app.port = 8080 } test { app.port = 8082 } production { app.port = 80 } }''')assert config.app.port == 8080 ConfigSlurper 支持的环境并不只局限于几个具体的环境名，它取决于 ConfigSlurper 的客户端代码支持的环境并能基于此进行解析。 environments 方法本身是内置的，但你同样可以通过 registerConditionalBlock 来注册除了 environments 以外的方法名： 1234567891011121314def slurper = new ConfigSlurper()slurper.registerConditionalBlock('myProject', 'developers') // 注1def config = slurper.parse(''' sendMail = true myProject { developers { sendMail = false } }''')assert !config.sendMail 在注册了新的代码块以后，ConfigSlurper 就能进行解析了 在与 Java 进行整合时，我们可以使用 toProperties 方法将 ConfigObject 转换成一个 java.util.Properties，然后再将其存储至一个 *.properties 文本文件中。但要注意的是在转换成新的 Properties 实例的时候所有配置值都会被转换为 String 实例。 12345678910111213def config = new ConfigSlurper().parse(''' app.date = new Date() app.age = 42 app { name = &quot;Test${42}&quot; }''')def properties = config.toProperties()assert properties.&quot;app.date&quot; instanceof Stringassert properties.&quot;app.age&quot; == '42'assert properties.&quot;app.name&quot; == 'Test42' 3.2 Expando Expando 类可用于创建一个可动态扩展的对象。尽管它的名字看起来很像，但实际上它并没有利用 ExpandoMetaClass 来实现。每个 Expando 对象都代表一个独立的、可动态构造的实例，这些实例可在运行时用属性或方法进行扩展。 1234def expando = new Expando()expando.name = 'John'assert expando.name == 'John' 当将一个闭包代码块注册为动态属性时则比较特殊：在完成注册后可以像调用方法那样对其进行调用： 123456def expando = new Expando()expando.toString = { -&gt; 'John' }expando.say = { String s -&gt; &quot;John says: ${s}&quot; }assert expando as String == 'John'assert expando.say('Hi') == 'John says: Hi' 3.3 可观察的列表、映射和集 Groovy 还提供了可观察的列表、映射和集。这些集合在添加、移除或修改元素时都会触发 java.beans.PropertyChangeEvent 事件。值得注意的是一个 PropertiChangeEvent 并不只用于告诉监听器发生了特定的事件，它还包含了包括属性名以及属性修改前后的值等内容。 根据所发生的修改的类型，可观察的集合甚至可以一次触发多个不同类型的 PropertyChangeEvent 事件。例如，向一个可观察的列表中添加一个元素会触发 ObservableList.ElementAddedEvent 事件： 1234567891011121314151617181920def event // 注1def listener = { if (it instanceof ObservableList.ElementEvent) { // 注2 event = it }} as PropertyChangeListenerdef observable = [1, 2, 3] as ObservableList // 注3observable.addPropertyChangeListener(listener) // 注4observable.add 42 // 注5assert event instanceof ObservableList.ElementAddedEventdef elementAddedEvent = event as ObservableList.ElementAddedEventassert elementAddedEvent.changeType == ObservableList.ChangeType.ADDEDassert elementAddedEvent.index == 3assert elementAddedEvent.oldValue == nullassert elementAddedEvent.newValue == 42 声明一个 PropertyChangeEventListener 用于捕获触发的事件 ObservableList.ElementEvent 及其子类都会使该监听器起作用 注册监听器 用给定的列表创建一个 ObservableList 触发一个 ObservableList.ElementAddedEvent 事件 注意，添加元素实际上会触发两个事件。第一个事件即为 ObservableList.ElementAddedEvent，而第二个实为一个 PropertyChangeEvent，用于告诉监听器列表的大小属性发生了变化。 ObservableList.ElementClearedEvent 则是另一种比较有意思的事件。当列表中的复数元素被移除，例如被调用了 clear() 方法时，它会包含所有被从列表中移除的元素： 123456789101112131415161718def eventdef listener = { if (it instanceof ObservableList.ElementEvent) { event = it }} as PropertyChangeListenerdef observable = [1, 2, 3] as ObservableListobservable.addPropertyChangeListener(listener)observable.clear()assert event instanceof ObservableList.ElementClearedEventdef elementClearedEvent = event as ObservableList.ElementClearedEventassert elementClearedEvent.values == [1, 2, 3]assert observable.size() == 0 为更好地了解所有支持的事件类型，读者可以参考所使用的可观察集合的 JavaDoc 文档或源代码。 ObservableMap 和 ObservableSet 同样包含了在这节中我们所看到的 ObservableList 所包含的功能。","link":"/groovy-dev-kit/"},{"title":"Groovy 语言规范 - 第一章：语法","text":"这是一篇译文，读者可前往 The Apache Groovy programming language - Syntax 阅读原文。 本章将讲述 Groovy 语言的语法。Groovy 语言的文法由 Java 语言的文法衍生而来，但同时也通过特定的语法结构对其进行了加强，同时也进行了一定的简化。 1 注释1.1 单行注释 单行注释由 // 起始，可存在于一行中的任意位置。从 // 开始到行末之间的字符均被视作注释的内容。 12// 这是一个独占一行的单行注释println &quot;hello&quot; // 该注释延伸至行末 1.2 多行注释 多行注释由 /* 起始，可起始于一行中的任意位置。从 /* 开始到第一个遇到的 */ 之间的包括换行符在内的所有字符均被视作注释的内容。由此，多行注释可被放在语句的末尾或是语句之间。 12345/* 一个独占了两行的 多行注释 */println &quot;hello&quot; /* 一个从语句末尾开始的 多行注释 */println 1 /* 一 */ + 2 /* 二 */ 1.3 GroovyDoc 注释 与多行注释类似，GroovyDoc 注释也可以包括多行，但其由 /** 起始并由 */ 终止。从第二行开始，GroovyDoc 注释中的每一行都可以选择以星号 * 起始。这种注释可以与如下几种语言元素相关联： 类型定义（类、接口、枚举类型、注解）； 域和属性的定义； 方法定义 尽管即使 GroovyDoc 注释没能和这些语言元素关联在一起编译器也不会有任何反应，但将 GroovyDoc 注释放在这些元素的正上方是更好的做法。 1234567891011121314151617/** * 类的描述 */class Person { /** 人的名字 */ String name /** * 为给定的人返回一段问候语 * * @param otherPerson 需要问候的人 * @return 一段问候语 */ String greet(String otherPerson) { &quot;Hello ${otherPerson}&quot; }} GroovyDoc 使用了和 Java 的 JavaDoc 相同的语法约定，因此你也可以使用和 JavaDoc 相同的文档注释标签。 1.4 Shebang 行 除了一般的单行注释，还有一种被 Unix 系统称为 Shebang 行（译者注：She 和 Bang 分别对应于 # 和 ! 符号） 的注释，它们使得在你安装了 Groovy 并将 groovy 命令放置在 PATH 中后能够从命令行中运行脚本。 12#!/usr/bin/env groovyprintln &quot;Hello from the shebang line&quot; # 符号必须为脚本文件的第一个字符，任何缩进都会产生编译错误。 2 关键词如下表格给出了 Groovy 语言中的所有关键词： 表 1：关键词 as assert break case catch class const continue def default do else enum extends false finally for goto if implements import in instanceof interface new null package return super switch this throw throws trait true try while （译者注：比 Java 多了 def、in 和 trait） 3 标识符3.1 普通标识符标识符可由字母、美金符号或下滑钱开头，不能以数字开头。 所谓“字母”包括如下范围内的字符： a 到 z（ASCII 小写字母） A 到 Z（ASCII 大写字母） \\u00C0 到 \\u00D6 \\u00D8 到 \\u00F6 \\u00F8 到 \\u00FF \\u0100 到 \\uFFFE 标识符接下来的字符可以包括字母和数字。 如下为部分合法标识符： 1234def namedef item3def with_underscoredef $dollarStart 如下为部分非法标识符： 123def 3tierdef a+bdef a#b 当跟在一个句点（.）后时，所有关键字均为合法标识符： 12345foo.asfoo.assertfoo.breakfoo.casefoo.catch 3.2 带引号的标识符带引号的标识符可出现在句点表达式的句点之后。例如，person.name 表达式中的 name 即可被引号包裹，写作 person.&quot;name&quot; 或 person.'name'。如果某些标识符中包含 Java 语言规范不允许但在 Groovy 中被引号包裹时允许存在的字符时，这样的写法就十分有用了。这样的字符包括破折号、空格和感叹号。 1234567def map = [:]map.&quot;an identifier with a space and double quotes&quot; = &quot;ALLOWED&quot;map.'with-dash-signs-and-single-quotes' = &quot;ALLOWED&quot;assert map.&quot;an identifier with a space and double quotes&quot; == &quot;ALLOWED&quot;assert map.'with-dash-signs-and-single-quotes' == &quot;ALLOWED&quot; 在后面讲述字符串的章节中我们还能了解到，Groovy 提供了好几种不同的字符串字面量，而所有的这些字符串都可以被放在句点后作为带引号的标识符： 123456map.'single quote'map.&quot;double quote&quot;map.'''triple single quote'''map.&quot;&quot;&quot;triple double quote&quot;&quot;&quot;map./slashy string/map.$/dollar slashy string/$ 值得注意的是，当使用 Groovy 的 GString（插值字符串）作为带引号的标识符时是和使用普通字符串有所区别的：插值字符串中的值将会被填充，而后再以插值的结果作为标识符进行处理： 1234def firstname = &quot;Homer&quot;map.&quot;Simson-${firstname}&quot; = &quot;Homer Simson&quot;assert map.'Simson-Homer' == &quot;Homer Simson&quot; 4 字符串 文本字面量被表现为名为字符串的一串字符。在 Groovy 中你可以实例化 java.lang.String 对象以及 GString 对象（groovy.lang.GString），而后者在其他某些编程语言中又被称为插值字符串。 4.1 带单引号的字符串 带单引号的字符串（Single Quoted String）为一组由单引号包围的字符： 1'a single quoted string' 带单引号的字符串实际上即为普通的 java.lang.String 且不支持插值操作。 4.2 字符串拼接 所有 Groovy 字符串可由 + 运算符进行拼接： 1assert 'ab' == 'a' + 'b' 4.3 带三重单引号的字符串 带三重单引号的字符串（Triple Single Quoted String）是一串由三组单引号包围的字符： 1'''a triple single quoted string''' 带三重单引号的字符串实际上即为普通的 java.lang.String 且不支持插值操作。 带三重单引号的字符串可包含多行。你无须将字符串分成若干块并利用字符串拼接或转义的换行符即可使字符串的内容横跨若干行。 123def aMultilineString = '''line oneline twoline three''' 如果你的代码中包含缩进，例如在类的方法体中时，你的字符串中也会包含缩进所使用的空白字符。GDK 支持使用 String#stripIndent() 方法来移除这些缩进，也可以使用 String#stripMargin() 方法通过给定的分隔符来移除字符串前面的字符。 当你创建如下字符串时： 12345def startingAndEndingWithANewline = '''line oneline twoline three''' 你会注意到所创建的字符串的第一个字符为换行符。你可以通过添加一个反斜杠将换行符进行转义以将该换行符从字符串中移除： 1234567def strippedFirstNewline = '''\\line oneline twoline three'''assert !strippedFirstNewline.startsWith('\\n') 4.3.1 对特殊字符进行转义 你可以对单引号进行转义以免该单引号中断了字符串字面量： 1'an escaped single quote: \\' needs a backslash' 你也可以通过输入两个连续的反斜杠来对转义符进行转义： 1'an escaped escape character: \\\\ needs a double backslash' 某些特殊字符同样使用了反斜杠来作为转义符： 转义序列 字符 '\\t' 制表符 '\\b' 退格符 '\\n' 换行符 '\\r' 回车符 '\\f' 进纸符 '\\\\' 反斜杠 '\\'' 单引号（用于带单引号的字符串和带三重单引号的字符串） '\\\"' 双引号（用于带双引号的字符串和带三重双引号的字符串） 4.3.2 Unicode 转义序列 要输入那些不存在于你的键盘上的字符，你可以使用 Unicode 转义序列，序列由一个反斜杠，紧接着一个 'u' 再跟着 4 个十六进制数字组成。 例如，欧元货币符号可以这样输入： 1'The Euro currency symbol: \\u20AC' 4.4 带双引号的字符串 带双引号的字符串为一组由双引号包围的字符： 1&quot;a double quoted string&quot; 当不包含插值表达式时，带双引号的字符串将产生一个 java.lang.String 实例，否则将产生一个 groovy.lang.GString 实例。 你可以使用反斜杠对双引号进行转义： 1&quot;A double quote: \\&quot;&quot; 4.4.1 字符串插值 除了带单引号的字符串和带三重单引号的字符串，任何 Groovy 表达式都可以被放入到任意的字符串字面量中。插值操作即通过对给定字符串进行处理并用所得值替换字面量中的占位符的操作。占位符表达式可用 ${} 包围，对于句点表达式也可以仅用 $ 起始。当 GString 被传递至一个以 String 为参数的方法时，占位符中的表达式将会运算求值，最终调用所得值的 toString() 方法获得其字符串表示。 这里，我们创建了一个字符串，其中包含了一个引用了局部变量的占位符： 1234def name = 'Guillaume' // a plain stringdef greeting = &quot;Hello ${name}&quot;assert greeting.toString() == 'Hello Guillaume' 但实际上在占位符中使用任何 Groovy 表达式都是合法的。在下面的例子中我们可以看到可以使用代数表达式： 12def sum = &quot;The sum of 2 and 3 equals ${2 + 3}&quot;assert sum.toString() == 'The sum of 2 and 3 equals 5' 实际上除了表达式以外，${} 占位符中同样可以放入语句，但语句的值为 null，因此如果要在占位符中放入多个语句的话，最后一个语句则应该返回一个比较有意义的用于替换占位符的结果。例如，&quot;The sum of 1 and 2 is equal to ${def a = 1; def b = 2; a + b}&quot; 实际上是可以产生出想要的结果的，但在 GString 占位符中使用简单的表达式依然是更好的做法。 除了 ${} 占位符，我们还可以使用以 $ 符号起始的句点表达式： 12def person = [name: 'Guillaume', age: 36]assert &quot;$person.name is $person.age years old&quot; == 'Guillaume is 36 years old' 但只有形如 a.b 或 a.b.c 等的句点表达式可以使用这种写法，如方法调用、闭包等带括号或带代数运算符的表达式是不能这样写的。假设我们定义了如下的数字变量： 1def number = 3.14 如下语句将会抛出一个 groovy.lang.MissingPropertyException 因为 Groovy 以为你想要访问该变量的 toString 属性，而数字变量本身不包含这样一个属性： 123shouldFail(MissingPropertyException) { println &quot;$number.toString()&quot;} 你可能会以为 &quot;$number.toString()&quot; 会被解析器理解为 &quot;${number.toString}()&quot;。 如果你想要对 GString 中的 $ 或 ${} 占位符进行转义使其不要触发插值操作，你只需要使用一个反斜杠符号对 $ 符号转义即可： 1assert '${name}' == &quot;\\${name}&quot; 4.4.2 使用闭包表达式进行插值 目前来讲，我们了解到我们可以向 ${} 占位符中插入任意的表达式，但如果要插入闭包表达式的话则需要使用一些特殊的符号。当占位符中包含一个箭头符号时，${→}，该表达式实际上是一个闭包表达式 —— 你可以将其想象成一个前面带着一个美金符号的闭包： 12345def sParameterLessClosure = &quot;1 + 2 == ${-&gt; 3}&quot; // 注1assert sParameterLessClosure == '1 + 2 == 3'def sOneParamClosure = &quot;1 + 2 == ${ w -&gt; w &lt;&lt; 3}&quot; // 注2 assert sOneParamClosure == '1 + 2 == 3' 1 该闭包为无参数闭包，无需传入任何实参 2 该闭包需要传入一个 java.io.StringWriter 参数，你可以使用","link":"/groovy-specification-1/"},{"title":"Spark Catalyst 进阶：Join","text":"在之前的文章中，我们已经了解了 SparkSQL 把 SQL 语句变为 SparkJob 的过程。这个过程我们只是做了一个 Overview，具体不同的语句会变为怎样的 Job 我们并未一一列举。实际上列举起来是一件相当大工程的事。 在那么多的 SQL 操作中，有那么一个操作十分常用，但又十分耗时，那就是 Join 操作。在这篇文章里，我们将深入探讨 SparkSQL 会对不同的 Join 做出怎样的操作。 什么是 Join ？在 SQL 中，Join 用于根据两个或多个表中的列之间的关系，从这些表中查询数据。表达 Join 的方式有两种： 1234567891011SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM Persons, OrdersWHERE Persons.Id_P = Orders.Id_P;-- 或SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName; 实际上，第一种方式更像是 SQL 的语法糖，理论上而言我们更偏向后一种写法。这种使用关键字 JOIN 的规范写法使用 ON 关键字表明了 Join 的条件，同时在 JOIN 前面加上了一个 INNER 来表明要执行的 Join 的类型。SparkSQL 支持的 SQL 操作有以下几种： Join 类型 效果 Inner Join 使用比较运算符根据每个表共有的列的值匹配两个表中的行 Left Semi Join 对于左表的每个键值，在右表中找到第一个匹配的键值便返回 Left Outer Join 左向外联接的结果集包括 LEFT OUTER 子句中指定的左表的所有行，而不仅仅是联接列所匹配的行。如果左表的某行在右表中没有匹配行，则在相关联的结果集行中右表的所有选择列表列均为空值 Right Outer Join 右向外联接是左向外联接的反向联接。将返回右表的所有行。如果右表的某行在左表中没有匹配行，则将为左表返回空值 Full Outer Join 完整外部联接返回左表和右表中的所有行。当某行在另一个表中没有匹配行时，则另一个表的选择列表列包含空值。如果表之间有匹配行，则整个结果集行包含基表的数据值 接下来我们就开始看看 SparkSQL 会怎么处理这些 JOIN 语句。 Parser首先 JOIN 语句要变成 Logical Plan 就需要先经过 Parser。根据我们之前学习过的内容来判断，JOIN 语句相关的解析规则在 SqlParser 类中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class SqlParser extends AbstractSparkSQLParser with DataTypeParser { // ... // 直接查找关键字 `Join`，发现在 relations 中创建了这样一个实例 protected lazy val relations: Parser[LogicalPlan] = // 我们知道 relation 指代的是一张表，那么在遇到像 `Table1, Table2` 这样的语句就会进入这里 ( relation ~ rep1(&quot;,&quot; ~&gt; relation) ^^ { case r1 ~ joins =&gt; joins.foldLeft(r1) { case(lhs, r) =&gt; Join(lhs, r, Inner, None) } } // 对于这样的语句，这里的做法是 foldLeft 地形成了一个由 Inner Join 组成的单边二叉树 | relation ) protected lazy val select: Parser[LogicalPlan] = SELECT ~&gt; DISTINCT.? ~ repsep(projection, &quot;,&quot;) ~ // FROM 这里引用了上面的 relations，由此可见 SparkSQL 支持我们提到的第一种 SQL 写法，产生的是一个 Inner Join (FROM ~&gt; relations).? ~ (WHERE ~&gt; expression).? ~ (GROUP ~ BY ~&gt; rep1sep(expression, &quot;,&quot;)).? ~ (HAVING ~&gt; expression).? ~ sortType.? ~ (LIMIT ~&gt; expression).? ^^ { case d ~ p ~ r ~ f ~ g ~ h ~ o ~ l =&gt; val base = r.getOrElse(OneRowRelation) val withFilter = f.map(Filter(_, base)).getOrElse(base) val withProjection = g .map(Aggregate(_, assignAliases(p), withFilter)) .getOrElse(Project(assignAliases(p), withFilter)) val withDistinct = d.map(_ =&gt; Distinct(withProjection)).getOrElse(withProjection) val withHaving = h.map(Filter(_, withDistinct)).getOrElse(withDistinct) val withOrder = o.map(_(withHaving)).getOrElse(withHaving) val withLimit = l.map(Limit(_, withOrder)).getOrElse(withOrder) withLimit } // ... // Join 实例另一次出现的位置在这里 protected lazy val joinedRelation: Parser[LogicalPlan] = // 这里对应的语句便是 JOIN `table` [ON ...] relationFactor ~ rep1(joinType.? ~ (JOIN ~&gt; relationFactor) ~ joinConditions.?) ^^ { case r1 ~ joins =&gt; joins.foldLeft(r1) { case (lhs, jt ~ rhs ~ cond) =&gt; Join(lhs, rhs, joinType = jt.getOrElse(Inner), cond) // 注意这里 Join 类型在未指定时为 Inner Join。同时注意这里的 cond 是个 Option[Expression] } // 这里同样是 foldLeft 地形成了一个 Join 的二叉树 } protected lazy val joinConditions: Parser[Expression] = ON ~&gt; expression // 通过在 JOIN 关键字前加入如下关键字可以改变 Join 的类型 protected lazy val joinType: Parser[JoinType] = ( INNER ^^^ Inner | LEFT ~ SEMI ^^^ LeftSemi | LEFT ~ OUTER.? ^^^ LeftOuter | RIGHT ~ OUTER.? ^^^ RightOuter | FULL ~ OUTER.? ^^^ FullOuter ) // ... } 由此，输入到 SparkSQL 中的 SQL 语句与 Join 类型的关系可以总结如下： Join 类型 SQL 语句 Inner Join SELECT ... FROM table1, table2[, ...] ...SELECT ... FROM ... JOIN ... [ON ...] Left Semi Join SELECT ... FROM ... LEFT SEMI JOIN ... [ON ...] Left Outer Join SELECT ... FROM ... LEFT [OUTER] JOIN ... [ON ...] Right Outer Join SELECT ... FROM ... RIGHT [OUTER] JOIN ... [ON ...] Full Outer Join SELECT ... FROM ... FULL [OUTER] JOIN ... [ON ...] 接下来我们来看一下表示 Logical Plan 的 Join 类： 12345678910111213141516171819202122232425262728case class Join( left: LogicalPlan, right: LogicalPlan, joinType: JoinType, // JoinType 包括 5 个 case object，对应 5 个 Join 类型 condition: Option[Expression]) extends BinaryNode { override def output: Seq[Attribute] = { joinType match { case LeftSemi =&gt; left.output case LeftOuter =&gt; left.output ++ right.output.map(_.withNullability(true)) case RightOuter =&gt; left.output.map(_.withNullability(true)) ++ right.output case FullOuter =&gt; left.output.map(_.withNullability(true)) ++ right.output.map(_.withNullability(true)) case _ =&gt; left.output ++ right.output } } // 防止用户构成了一些根本无法 Join 的左右子树 private def selfJoinResolved: Boolean = left.outputSet.intersect(right.outputSet).isEmpty override lazy val resolved: Boolean = { childrenResolved &amp;&amp; !expressions.exists(!_.resolved) &amp;&amp; selfJoinResolved }} Join 的 Logical Plan 本身只有一个类，显得十分简单。 Analyzer在通过 Parser 得到 Unresolved Logical Plan 以后，下一步就轮到 Analyzer 了。经过之前的学习，我们知道 Analyzer 所应用的全部规则都位于 Analyzer.scala 中： 12345678910111213141516171819202122232425262728293031323334353637class Analyzer( catalog: Catalog, registry: FunctionRegistry, conf: CatalystConf, maxIterations: Int = 100) extends RuleExecutor[LogicalPlan] with HiveTypeCoercion with CheckAnalysis { // ... object ResolveReferences extends Rule[LogicalPlan] { def apply(plan: LogicalPlan): LogicalPlan = plan transformUp { // ... // 同样，经过搜索，Join 仅出现在该分支中 // 该处用于处理之前的 selfJoinResolved 为 false 的情况 case j @ Join(left, right, _, _) if left.outputSet.intersect(right.outputSet).nonEmpty =&gt; // 找出冲突的 Attribute val conflictingAttributes = left.outputSet.intersect(right.outputSet) logDebug(s&quot;Conflicting attributes ${conflictingAttributes.mkString(&quot;,&quot;)} in $j&quot;) // ... // 根据右子树类型的不同将右子树进行了替换 val newRight = right transformUp { case r if r == oldRelation =&gt; newRelation } transformUp { case other =&gt; other transformExpressions { case a: Attribute =&gt; attributeRewrites.get(a).getOrElse(a) } } j.copy(right = newRight) // ... } // ...} 看起来，Analyzer 对 Join 树做的操作仅在于解决一些很奇怪的属性冲突。这种问题属于少数派，相信大多数时候 SparkSQL 都不会进入这个分支。 Optimizer接下来我们来看一下 Optimizer 是否有与 Join 相关的优化逻辑： 12345678910111213141516171819202122232425262728293031323334353637383940// Join 首先出现在了这个 Rule 中object ColumnPruning extends Rule[LogicalPlan] { // 对 c 进行剪枝，只需要包含在 allReferences 中的属性 // 通过在 c 之上加上一个 Project 计划来实现 private def prunedChild(c: LogicalPlan, allReferences: AttributeSet) = if ((c.outputSet -- allReferences.filter(c.outputSet.contains)).nonEmpty) { Project(allReferences.filter(c.outputSet.contains).toSeq, c) } else { c } def apply(plan: LogicalPlan): LogicalPlan = plan transform { // ... // Join 后只 SELECT 了少部分属性 case Project(projectList, Join(left, right, joinType, condition)) =&gt; // Collect the list of all references required either above or to evaluate the condition. val allReferences: AttributeSet = AttributeSet( projectList.flatMap(_.references.iterator)) ++ condition.map(_.references).getOrElse(AttributeSet(Seq.empty)) // 包括 SELECT 了的属性以及出现在了 ON 中的属性 /** Applies a projection only when the child is producing unnecessary attributes */ def pruneJoinChild(c: LogicalPlan): LogicalPlan = prunedChild(c, allReferences) // 先对左右子树进行 Project 再 Join Project(projectList, Join(pruneJoinChild(left), pruneJoinChild(right), joinType, condition)) // 消除 LeftSemiJoin 中右子树中不必要的属性 case Join(left, right, LeftSemi, condition) =&gt; // Collect the list of all references required to evaluate the condition. val allReferences: AttributeSet = condition.map(_.references).getOrElse(AttributeSet(Seq.empty)) // 包括出现在 ON 中的属性 Join(left, prunedChild(right, allReferences), LeftSemi, condition) // ... }} 由此可见，Optimizer 对 Join 操作做出的优化，在于将 SELECT 以及 ON 所包含的属性考虑进去后，将左右子树中不需要的属性先删去再 Join，以此来优化 Join 的性能。 至此，Logical Plan 的处理过程就全部完成了。接下来就是重中之重了。 Planner我们知道，Planner 将 Optimized Logical Plan 变为 Physical Plan 的规则全都位于 SparkStrategies 类中，那我们直接看吧： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124private[sql] abstract class SparkStrategies extends QueryPlanner[SparkPlan] { self: SQLContext#SparkPlanner =&gt; object LeftSemiJoin extends Strategy with PredicateHelper { def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match { // ExtractEquiJoinKeys 用于将出现在 condition 的相等条件中的属性拆分出来 // leftKeys 和 rightKeys 分别对应属于左子树和属于右子树的 Attribute // 相同索引值的 leftKey 和 rightKey 构成原本的 condition 中的一对相等条件，即 `leftKey(i) = rightKey(i)` // 剩余的非相等条件会被放入到结果的 condition 中 // 该 unapply 函数当且仅当 leftKeys 和 rightKeys 不为空时会有返回 case ExtractEquiJoinKeys(LeftSemi, leftKeys, rightKeys, condition, left, right) // 该参数默认为 10 * 1024 * 1024，即 10mb if sqlContext.conf.autoBroadcastJoinThreshold &gt; 0 &amp;&amp; right.statistics.sizeInBytes &lt;= sqlContext.conf.autoBroadcastJoinThreshold =&gt; // 右子树 &lt;= 10 MB // 产生一个 BroadcastLeftSemiJoinHash 实例 val semiJoin = joins.BroadcastLeftSemiJoinHash( leftKeys, rightKeys, planLater(left), planLater(right)) // 再把剩下的非相等条件以 Filter 的形式覆盖上去 condition.map(Filter(_, semiJoin)).getOrElse(semiJoin) :: Nil case ExtractEquiJoinKeys(LeftSemi, leftKeys, rightKeys, condition, left, right) =&gt; // 情况基本同上，只是这里改为使用 LeftSemiJoinHash 实例 val semiJoin = joins.LeftSemiJoinHash( leftKeys, rightKeys, planLater(left), planLater(right)) condition.map(Filter(_, semiJoin)).getOrElse(semiJoin) :: Nil // no predicate can be evaluated by matching hash keys case logical.Join(left, right, LeftSemi, condition) =&gt; // 剩下的 Left Semi Join 就直接变成 LeftSemiJoinBNL 实例 joins.LeftSemiJoinBNL(planLater(left), planLater(right), condition) :: Nil case _ =&gt; Nil } } // 到这里，Left Semi Join 已经全部由上面那个 Strategy 变成 Physical Plan 了 object HashJoin extends Strategy with PredicateHelper { private[this] def makeBroadcastHashJoin( leftKeys: Seq[Expression], rightKeys: Seq[Expression], left: LogicalPlan, right: LogicalPlan, condition: Option[Expression], side: joins.BuildSide) = { // 产生一个 BroadcastHashJoin 实例，并用 Filter 把剩余的 condition 盖了上去 val broadcastHashJoin = execution.joins.BroadcastHashJoin( leftKeys, rightKeys, side, planLater(left), planLater(right)) condition.map(Filter(_, broadcastHashJoin)).getOrElse(broadcastHashJoin) :: Nil } def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match { case ExtractEquiJoinKeys(Inner, leftKeys, rightKeys, condition, left, right) if sqlContext.conf.autoBroadcastJoinThreshold &gt; 0 &amp;&amp; right.statistics.sizeInBytes &lt;= sqlContext.conf.autoBroadcastJoinThreshold =&gt; // Inner Join，ON 里有相等条件，右子树不算大 -&gt; BroadcastHashJoin makeBroadcastHashJoin(leftKeys, rightKeys, left, right, condition, joins.BuildRight) case ExtractEquiJoinKeys(Inner, leftKeys, rightKeys, condition, left, right) if sqlContext.conf.autoBroadcastJoinThreshold &gt; 0 &amp;&amp; left.statistics.sizeInBytes &lt;= sqlContext.conf.autoBroadcastJoinThreshold =&gt; // Inner Join，ON 里有相等条件，左子树不算大 -&gt; BroadcastHashJoin makeBroadcastHashJoin(leftKeys, rightKeys, left, right, condition, joins.BuildLeft) case ExtractEquiJoinKeys(Inner, leftKeys, rightKeys, condition, left, right) if sqlContext.conf.sortMergeJoinEnabled =&gt; // Inner Join，ON 里有相等条件，sortMergeJoin 设置被开启 -&gt; SortMergeJoin val mergeJoin = joins.SortMergeJoin(leftKeys, rightKeys, planLater(left), planLater(right)) condition.map(Filter(_, mergeJoin)).getOrElse(mergeJoin) :: Nil case ExtractEquiJoinKeys(Inner, leftKeys, rightKeys, condition, left, right) =&gt; val buildSide = if (right.statistics.sizeInBytes &lt;= left.statistics.sizeInBytes) { joins.BuildRight } else { joins.BuildLeft } // Inner Join，ON 里有相等条件 -&gt; ShuffledHashJoin，以较小的一边作为 buildSide val hashJoin = joins.ShuffledHashJoin( leftKeys, rightKeys, buildSide, planLater(left), planLater(right)) condition.map(Filter(_, hashJoin)).getOrElse(hashJoin) :: Nil case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right) =&gt; // ON 里有相等条件 -&gt; HashOuterJoin joins.HashOuterJoin( leftKeys, rightKeys, joinType, condition, planLater(left), planLater(right)) :: Nil case _ =&gt; Nil } } // ... object CartesianProduct extends Strategy { def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match { case logical.Join(left, right, _, None) =&gt; // 没有 ON 语句 -&gt; CartesianProduct execution.joins.CartesianProduct(planLater(left), planLater(right)) :: Nil case logical.Join(left, right, Inner, Some(condition)) =&gt; // Inner Join，有 ON 语句 -&gt; CartesianProduct 再盖一个 Filter execution.Filter(condition, execution.joins.CartesianProduct(planLater(left), planLater(right))) :: Nil case _ =&gt; Nil } } object BroadcastNestedLoopJoin extends Strategy { def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match { case logical.Join(left, right, joinType, condition) =&gt; val buildSide = if (right.statistics.sizeInBytes &lt;= left.statistics.sizeInBytes) { joins.BuildRight } else { joins.BuildLeft } // 剩下的 JOIN -&gt; 以较小一侧为 buildSide 的 BroadcastNestedLoopJoin joins.BroadcastNestedLoopJoin( planLater(left), planLater(right), buildSide, joinType, condition) :: Nil case _ =&gt; Nil } } // ... } 通过阅读上述代码，我们找到了如下几个与 JOIN 有关的 SparkPlan： `BroadcastLeftSemiJoinHash`：Left Semi Join，ON 中存在相等条件，右子树小于阈值（默认 10MB） `LeftSemiJoinHash`：Left Semi Join，ON 中存在相等条件 `LeftSemiJoinBNL`：Left Semi Join `BroadcastHashJoin`：Inner Join，ON 里有相等条件，左子树或右子树小于阈值（默认 10MB）。以较小的一侧为 BuildSide `SortMergeJoin`：Inner Join，ON 里有相等条件，sortMergeJoin 设置被开启 `ShuffledHashJoin`：Inner Join，ON 里有相等条件。以较小的一侧为 buildSide。 `HashOuterJoin`：ON 里有相等条件 `CartesianProduct`：Inner Join，有 ON 语句 `CartesianProduct`：没有 ON 语句 `BroadcastNestedLoopJoin`：剩下的都是它 足足 10 种用于 Join 的 Physical Plan。看来 SparkSQL 也知道这是最关键的操作。接下来我们逐个解析这些 Plan。 Physical PlanBroadcastLeftSemiJoinHash准入条件：Left Semi Join，ON 中存在相等条件，右子树小于阈值（默认 10MB） 12345678case class BroadcastLeftSemiJoinHash( leftKeys: Seq[Expression], rightKeys: Seq[Expression], left: SparkPlan, right: SparkPlan) extends BinaryNode with HashJoin { // 继承自 HashJoin // ...} 好，在看之前我们先看看 HashJoin： 12345678910111213141516171819202122232425262728293031323334353637383940trait HashJoin { self: SparkPlan =&gt; // 这些成员大部分由子类的构造函数传入 val leftKeys: Seq[Expression] val rightKeys: Seq[Expression] val buildSide: BuildSide // 只有两个子类 case object：BuildLeft 和 BuildRight val left: SparkPlan val right: SparkPlan // buildPlan 为 buildSide 指定的那边的 SparkPlan，streamedPlan 则为剩下那个 protected lazy val (buildPlan, streamedPlan) = buildSide match { case BuildLeft =&gt; (left, right) case BuildRight =&gt; (right, left) } // buildKeys 为 buildSide 指定的那边的 keys，streamedKeys 则为剩下那边的 keys protected lazy val (buildKeys, streamedKeys) = buildSide match { case BuildLeft =&gt; (leftKeys, rightKeys) case BuildRight =&gt; (rightKeys, leftKeys) } override def output: Seq[Attribute] = left.output ++ right.output // abstract class Projection extends (Row =&gt; Row) // 根据 key 和 output 生成了一个 key generator // 子类会使用这个 generator 为每个 Row 生成一个 key(也是一个 Row)并放入到 HashSet 或 HashMap // 想必这个 key 应该实现了比较高效的 hashCode 方法 @transient protected lazy val buildSideKeyGenerator: Projection = newProjection(buildKeys, buildPlan.output) // 同理 @transient protected lazy val streamSideKeyGenerator: () =&gt; MutableProjection = newMutableProjection(streamedKeys, streamedPlan.output) // 直接看比较复杂，等用到时我们再进行解析 protected def hashJoin(streamIter: Iterator[Row], hashedRelation: HashedRelation): Iterator[Row] = { // ... }} 好，我们再回到 BroadcastLeftSemiJoinHash： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546case class BroadcastLeftSemiJoinHash( leftKeys: Seq[Expression], rightKeys: Seq[Expression], left: SparkPlan, right: SparkPlan) extends BinaryNode with HashJoin { // 以右子树为 buildSide override val buildSide: BuildSide = BuildRight // 输出属性集与左子树相同 override def output: Seq[Attribute] = left.output // SparkPlan 入口方法 protected override def doExecute(): RDD[Row] = { // 获取右子树结果集 val buildIter = buildPlan.execute().map(_.copy()).collect().toIterator val hashSet = new java.util.HashSet[Row]() var currentRow: Row = null // 利用右子树结果集构建一个 key 的 HashSet while (buildIter.hasNext) { currentRow = buildIter.next() // 利用 buildSideKeyGenerator 为右子树结果集的每个 Row 都生成一个 key val rowKey = buildSideKeyGenerator(currentRow) if (!rowKey.anyNull) { val keyExists = hashSet.contains(rowKey) if (!keyExists) { // key 们放入到 hashSet 中 hashSet.add(rowKey) } } } // 将 hashSet 广播出去 val broadcastedRelation = sparkContext.broadcast(hashSet) streamedPlan.execute().mapPartitions { streamIter =&gt; // 利用 streamSideKeyGenerator 为左子树的 Row 生成 key val joinKeys = streamSideKeyGenerator() streamIter.filter(current =&gt; { !joinKeys(current).anyNull &amp;&amp; broadcastedRelation.value.contains(joinKeys.currentValue) // 在之前的 hashSet 中包含本 key，则放入到结果集中 }) } }} 首先，在实例化结果 RDD 的时候，右子树的结果就已经计算完毕并被收集回来，将右子树的 Row 变为 key 并放入 HashSet 再广播出去的动作将由 Master 独自完成。在结果 RDD 的 collect 或其他方法被调用的时候，左子树的每个 Partition 同样会将自己的 Row 变为 key，并与之前广播的 HashSet 中的元素进行比对，返回 key 存在于 HashSet 中的记录。 RDD 的计算本该是 lazy 的。诚然，这里左子树的计算确实是 lazy 的，但右子树不是，右子树在 RDD 实例化的时候就已经计算完毕了，因此该方法不太适用于较大的右子树。不过，能产生这种 SparkPlan 本来就要求 LeftSemiJoin 操作右子树的 Statistics 值小于一定的阈值，因此这样做还是合理的。 LeftSemiJoinHash准入条件：Left Semi Join，ON 中存在相等条件 123456789101112131415161718192021222324252627282930313233343536373839404142434445case class LeftSemiJoinHash( leftKeys: Seq[Expression], rightKeys: Seq[Expression], left: SparkPlan, right: SparkPlan) extends BinaryNode with HashJoin { // 同样以右子树作为 BuildSide override val buildSide: BuildSide = BuildRight // 表明对于 leftKeys 以及 rightKeys 的每个属性，具有相同值的 Row 可能分散在不同的 Partition 中 override def requiredChildDistribution: Seq[ClusteredDistribution] = ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil // 同样直接以左子树的输入作为输出 override def output: Seq[Attribute] = left.output protected override def doExecute(): RDD[Row] = { // 先计算出右子树的结果 RDD // 再把左右子树的 Partition 们 zip 起来（意味着左右子树的结果 Partition 数相同） buildPlan.execute().zipPartitions(streamedPlan.execute()) { (buildIter, streamIter) =&gt; // 在 zip 起来的 Partition 内采取了和之前一样的算法 val hashSet = new java.util.HashSet[Row]() var currentRow: Row = null // Create a Hash set of buildKeys // 先构建右子树的 key set while (buildIter.hasNext) { currentRow = buildIter.next() val rowKey = buildSideKeyGenerator(currentRow) if (!rowKey.anyNull) { val keyExists = hashSet.contains(rowKey) if (!keyExists) { hashSet.add(rowKey) } } } // 再从左子树中筛选返回 val joinKeys = streamSideKeyGenerator() streamIter.filter(current =&gt; { !joinKeys(current).anyNull &amp;&amp; hashSet.contains(joinKeys.currentValue) }) } }} 可见，其核心算法本身和 BroadcastLeftSemiJoinHash 并无不同，但却使用了 zipPartitions 方法来计算两个 RDD 的 Join 结果。如果要确保结果完全正确，就需要两个 RDD 的 Partition 数相同，同时在 key 上有着相同值的 Row 必然处于 index 相同的 Partition 内。我暂时无法理解 SparkSQL 要如何保证这两个条件同时满足，只能先放一放了。 LeftSemiJoinBN准入条件：Left Semi Join 12345678910111213141516171819202122232425262728293031323334353637383940414243case class LeftSemiJoinBNL(streamed: SparkPlan, broadcast: SparkPlan, condition: Option[Expression])// 注：实例化时传入的 streamed 为左子树，broadcast 为右子树 extends BinaryNode { // 由于 ON 语句中不再有相等条件，因此该算法也不使用 HashSet 来查找相同元素了 override def left: SparkPlan = streamed override def right: SparkPlan = broadcast // 输出的属性与 Partition 方法与左子树保持一致 override def outputPartitioning: Partitioning = streamed.outputPartitioning override def output: Seq[Attribute] = left.output // 根据传入的 ON condition 生成了一个(Row) =&gt; Boolean @transient private lazy val boundCondition = newPredicate(condition.getOrElse(Literal(true)), left.output ++ right.output) protected override def doExecute(): RDD[Row] = { // 计算右子树并把结果广播出去 val broadcastedRelation = sparkContext.broadcast(broadcast.execute().map(_.copy()).collect().toIndexedSeq) streamed.execute().mapPartitions { streamedIter =&gt; val joinedRow = new JoinedRow // 筛选吻合的行 streamedIter.filter(streamedRow =&gt; { var i = 0 var matched = false // 遍历右子树结果，并在找到第一个匹配结果的时候结束循环 while (i &lt; broadcastedRelation.value.size &amp;&amp; !matched) { val broadcastedRow = broadcastedRelation.value(i) // 利用当前的两个 Row 生成一个 JoinedRow 并验证是否吻合条件 if (boundCondition(joinedRow(streamedRow, broadcastedRow))) { matched = true } i += 1 } matched }) } }} 没什么特别，相当好理解。 BroadcastHashJoin准入条件：Inner Join，ON 里有相等条件，左子树或右子树小于阈值（默认 10MB），以较小的一侧为 BuildSide 12345678910111213141516171819202122232425262728293031323334353637383940414243case class BroadcastHashJoin( leftKeys: Seq[Expression], rightKeys: Seq[Expression], buildSide: BuildSide, left: SparkPlan, right: SparkPlan) extends BinaryNode with HashJoin { val timeout: Duration = { // 默认为 5*60，即 5 分钟 val timeoutValue = sqlContext.conf.broadcastTimeout if (timeoutValue &lt; 0) { Duration.Inf } else { timeoutValue.seconds } } override def outputPartitioning: Partitioning = streamedPlan.outputPartitioning override def requiredChildDistribution: Seq[Distribution] = UnspecifiedDistribution :: UnspecifiedDistribution :: Nil // 启动一个异步计算 @transient private val broadcastFuture = future { // Note that we use .execute().collect() because we don't want to convert data to Scala types // 收集较小子树的结果 val input: Array[Row] = buildPlan.execute().map(_.copy()).collect() // 生成一个 Key 到 Row(s)的 HashMap 并广播出去 val hashed = HashedRelation(input.iterator, buildSideKeyGenerator, input.length) sparkContext.broadcast(hashed) }(BroadcastHashJoin.broadcastHashJoinExecutionContext) protected override def doExecute(): RDD[Row] = { // 等待异步计算完成 val broadcastRelation = Await.result(broadcastFuture, timeout) streamedPlan.execute().mapPartitions { streamedIter =&gt; hashJoin(streamedIter, broadcastRelation.value) } }} 我们看到，在最后 BroadcastHashJoin 调用了父类 HashJoin 的 hashJoin 方法。我们来看看那个方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667trait HashJoin { self: SparkPlan =&gt; // ... // 参考上面，这里传入的 hashedRelation 实际上是 Key 到 Row(s)的 HashMap protected def hashJoin(streamIter: Iterator[Row], hashedRelation: HashedRelation): Iterator[Row] = { new Iterator[Row] { private[this] var currentStreamedRow: Row = _ private[this] var currentHashMatches: CompactBuffer[Row] = _ private[this] var currentMatchPosition: Int = -1 private[this] val joinRow = new JoinedRow2 private[this] val joinKeys = streamSideKeyGenerator() /** * 这里我们需要考虑我们平常使用 Iterator 的方式，基本都是这样： * while (iterator.hasNext) { * sth = iterator.next * .. * } * 意味着 hasNext 和 next 会被交替调用 */ override final def hasNext: Boolean = (currentMatchPosition != -1 &amp;&amp; currentMatchPosition &lt; currentHashMatches.size) || (streamIter.hasNext &amp;&amp; fetchNext()) // 在最初时，我们会先调用 hasNext，这里进入第二条条件判断式，fetchNext 被调用，获取到一个 currentHashMatches // 接下来，hasNext 在第一条条件判断式就会返回 true，第二条被短路。我们通过调用 next，让迭代器遍历 currentHashMatches // 当一个 currentHashMatches 被遍历完毕，第一条条件判断式会返回 false，这里就会进入第二条条件判断式，由 fetchNext 获取下一个 currentHashMatches // 综上，当且仅当 fetchNext 或 streamIter.hasNext 返回 false 时（实际上 fetchNext 也只有在!streamIter.hasNext 时才会返回 false），这里会返回 false override final def next(): Row = { // 遍历在 fetchNext 中拿到的 currentHashMatches，生成 JoinedRow val ret = buildSide match { case BuildRight =&gt; joinRow(currentStreamedRow, currentHashMatches(currentMatchPosition)) case BuildLeft =&gt; joinRow(currentHashMatches(currentMatchPosition), currentStreamedRow) } currentMatchPosition += 1 ret } // 找到下一个 streamSide 中吻合的条目 private final def fetchNext(): Boolean = { currentHashMatches = null currentMatchPosition = -1 // 找到一个吻合的条目并退出循环 while (currentHashMatches == null &amp;&amp; streamIter.hasNext) { currentStreamedRow = streamIter.next() if (!joinKeys(currentStreamedRow).anyNull) { currentHashMatches = hashedRelation.get(joinKeys.currentValue) } } if (currentHashMatches == null) { false // streamIter 已完成遍历，故该迭代器也已完成遍历，返回 false } else { // 找到吻合的条目，迭代器再次初始化 currentMatchPosition = 0 true } } } }} 嗯，这背后确实是个标准的 Hash Join 算法，但我必须得说，这写得实在是太巧妙了。 BroadcastHashJoin 实际上和 BroadcastLeftSemiJoinHash 很像，但后者的 buildSide 结果的收集是在 doExecute 被调用时进行，而前者在实例化时就已经以一个异步计算的形式开始了。考虑到 SparkSQL 的各种 lazy 变量，实际上前者的计算的启动时机比后者要早很多。前者在 planner.plan 的时候就已经开始了，而后者则要等到 QueryExecution#toRDD。 SortMergeJoin准入条件：Inner Join，ON 里有相等条件，sortMergeJoin 设置被开启 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131case class SortMergeJoin( leftKeys: Seq[Expression], rightKeys: Seq[Expression], left: SparkPlan, right: SparkPlan) extends BinaryNode { override def output: Seq[Attribute] = left.output ++ right.output override def outputPartitioning: Partitioning = left.outputPartitioning override def requiredChildDistribution: Seq[Distribution] = ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil // this is to manually construct an ordering that can be used to compare keys from both sides private val keyOrdering: RowOrdering = RowOrdering.forSchema(leftKeys.map(_.dataType)) private def requiredOrders(keys: Seq[Expression]): Seq[SortOrder] = keys.map(SortOrder(_, Ascending)) // 左右子树出现在 ON 相等表达式中的属性按升序排序 override def outputOrdering: Seq[SortOrder] = requiredOrders(leftKeys) override def requiredChildOrdering: Seq[Seq[SortOrder]] = requiredOrders(leftKeys) :: requiredOrders(rightKeys) :: Nil // 类似 HashJoin 的 key generator @transient protected lazy val leftKeyGenerator = newProjection(leftKeys, left.output) @transient protected lazy val rightKeyGenerator = newProjection(rightKeys, right.output) protected override def doExecute(): RDD[Row] = { // 或许到左右子树的结果 RDD val leftResults = left.execute().map(_.copy()) val rightResults = right.execute().map(_.copy()) // 左右子树的 Partition 们 zip 起来 leftResults.zipPartitions(rightResults) { (leftIter, rightIter) =&gt; new Iterator[Row] { // Mutable per row objects. private[this] val joinRow = new JoinedRow5 private[this] var leftElement: Row = _ private[this] var rightElement: Row = _ private[this] var leftKey: Row = _ private[this] var rightKey: Row = _ private[this] var rightMatches: CompactBuffer[Row] = _ private[this] var rightPosition: Int = -1 private[this] var stop: Boolean = false private[this] var matchKey: Row = _ // 迭代器初始化 initialize() // 将 leftElement 和 rightElement 分别指向左右侧第一个元素，并生成对应的 key private def initialize() = { fetchLeft() fetchRight() } // 从左子树获取下一个 Row private def fetchLeft() = { if (leftIter.hasNext) { leftElement = leftIter.next() leftKey = leftKeyGenerator(leftElement) } else { leftElement = null } } // 从右子树获取下一个 Row private def fetchRight() = { if (rightIter.hasNext) { rightElement = rightIter.next() rightKey = rightKeyGenerator(rightElement) } else { rightElement = null } } // 同样考虑刚刚提到的 Iterator 的使用方式 override final def hasNext: Boolean = nextMatchingPair() // 右迭代器搜索下一个与左侧匹配的条目 private def nextMatchingPair(): Boolean = { if (!stop &amp;&amp; rightElement != null) { // 两边的指针一起跑，以找到第一个配对 while (!stop &amp;&amp; leftElement != null &amp;&amp; rightElement != null) { val comparing = keyOrdering.compare(leftKey, rightKey) // 找到配对，则 stop 为 true，退出当前循环 stop = comparing == 0 &amp;&amp; !leftKey.anyNull if (comparing &gt; 0 || rightKey.anyNull) { fetchRight() // 左边比右边大，右边前进一步 } else if (comparing &lt; 0 || leftKey.anyNull) { fetchLeft() // 右边比左边大，左边前进一步 } } rightMatches = new CompactBuffer[Row]() if (stop) { stop = false // 将右侧的所有 key 相同的 Row 放入 rightMatches，直到遇到第一个不同的 key while (!stop &amp;&amp; rightElement != null) { rightMatches += rightElement fetchRight() stop = keyOrdering.compare(leftKey, rightKey) != 0 } if (rightMatches.size &gt; 0) { rightPosition = 0 matchKey = leftKey } } } rightMatches != null &amp;&amp; rightMatches.size &gt; 0 } override final def next(): Row = { if (hasNext) { val joinedRow = joinRow(leftElement, rightMatches(rightPosition)) rightPosition += 1 if (rightPosition &gt;= rightMatches.size) { rightPosition = 0 fetchLeft() // 右侧匹配条目收集完毕，左侧前进一步 if (leftElement == null || keyOrdering.compare(leftKey, matchKey) != 0) { stop = false // stop 置为 false，hasNext 继续寻找下一对配对 rightMatches = null } } joinedRow } else { // no more result throw new NoSuchElementException } } } } }} 虽然算法不相同，但迭代器的设计思想上，SortMergeJoin 和 BroadcastHashJoin 还是很像的，只是前者的迭代器在 next 方法里调用了 hasNext，这样的设计更为安全，而后者如果在 next 之前没有调用过 hasNext 则会直接出错。 ShuffledHashJoin准入条件：Inner Join，ON 里有相等条件。以较小的一侧为 buildSide。 1234567891011121314151617181920case class ShuffledHashJoin( leftKeys: Seq[Expression], rightKeys: Seq[Expression], buildSide: BuildSide, left: SparkPlan, right: SparkPlan) extends BinaryNode with HashJoin { override def outputPartitioning: Partitioning = left.outputPartitioning override def requiredChildDistribution: Seq[ClusteredDistribution] = ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil protected override def doExecute(): RDD[Row] = { buildPlan.execute().zipPartitions(streamedPlan.execute()) { (buildIter, streamIter) =&gt; val hashed = HashedRelation(buildIter, buildSideKeyGenerator) hashJoin(streamIter, hashed) } }} 好像没什么需要说的，十分直观。 HashOuterJoin准入条件：ON 里有相等条件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194@DeveloperApicase class HashOuterJoin( leftKeys: Seq[Expression], rightKeys: Seq[Expression], joinType: JoinType, condition: Option[Expression], left: SparkPlan, right: SparkPlan) extends BinaryNode { // 从这里看得出来，HashOuterJoin 同时接受三种 Outer Join，只要它们的 ON 里有相等条件 override def outputPartitioning: Partitioning = joinType match { case LeftOuter =&gt; left.outputPartitioning case RightOuter =&gt; right.outputPartitioning case FullOuter =&gt; UnknownPartitioning(left.outputPartitioning.numPartitions) case x =&gt; throw new Exception(s&quot;HashOuterJoin should not take $x as the JoinType&quot;) } override def requiredChildDistribution: Seq[ClusteredDistribution] = ClusteredDistribution(leftKeys) :: ClusteredDistribution(rightKeys) :: Nil override def output: Seq[Attribute] = { joinType match { case LeftOuter =&gt; left.output ++ right.output.map(_.withNullability(true)) case RightOuter =&gt; left.output.map(_.withNullability(true)) ++ right.output case FullOuter =&gt; left.output.map(_.withNullability(true)) ++ right.output.map(_.withNullability(true)) case x =&gt; throw new Exception(s&quot;HashOuterJoin should not take $x as the JoinType&quot;) } } @transient private[this] lazy val DUMMY_LIST = Seq[Row](null) @transient private[this] lazy val EMPTY_LIST = Seq.empty[Row] @transient private[this] lazy val leftNullRow = new GenericRow(left.output.length) @transient private[this] lazy val rightNullRow = new GenericRow(right.output.length) @transient private[this] lazy val boundCondition = condition.map(newPredicate(_, left.output ++ right.output)).getOrElse((row: Row) =&gt; true) // 出于性能考虑，SparkSQL 自行实现了三种迭代器 private[this] def leftOuterIterator( // 注意：这里传入的 joinedRow 的 left 已经设定，key 就是 left 的 key。见 doExecute key: Row, joinedRow: JoinedRow, rightIter: Iterable[Row]): Iterator[Row] = { val ret: Iterable[Row] = { if (!key.anyNull) { val temp = rightIter.collect { case r if boundCondition(joinedRow.withRight(r)) =&gt; joinedRow.copy() } // 收集右侧所有匹配的条目 if (temp.size == 0) { joinedRow.withRight(rightNullRow).copy :: Nil // 没有收集到，直接返回个 NULL } else { temp // 收集到了，就全部输出 } } else { joinedRow.withRight(rightNullRow).copy :: Nil } } ret.iterator } // 同上，轴对称一下而已 private[this] def rightOuterIterator( key: Row, leftIter: Iterable[Row], joinedRow: JoinedRow): Iterator[Row] = { val ret: Iterable[Row] = { if (!key.anyNull) { val temp = leftIter.collect { case l if boundCondition(joinedRow.withLeft(l)) =&gt; joinedRow.copy } if (temp.size == 0) { joinedRow.withLeft(leftNullRow).copy :: Nil } else { temp } } else { joinedRow.withLeft(leftNullRow).copy :: Nil } } ret.iterator } // 注意：这里传入的 key 先是左侧的 key，之后才是右侧的 key // leftIter 和 rightIter 则是与该 key 对应的左右侧的 Row。见 doExecute private[this] def fullOuterIterator( key: Row, leftIter: Iterable[Row], rightIter: Iterable[Row], joinedRow: JoinedRow): Iterator[Row] = { if (!key.anyNull) { // 尝试让传入的左右侧条目在 key 上 Join 一下 val rightMatchedSet = scala.collection.mutable.Set[Int]() leftIter.iterator.flatMap[Row] { l =&gt; joinedRow.withLeft(l) var matched = false rightIter.zipWithIndex.collect { // 1. For those matched (satisfy the join condition) records with both sides filled, // append them directly // 尝试找到吻合 Inner Join 的左右条目 case (r, idx) if boundCondition(joinedRow.withRight(r)) =&gt; matched = true // matched 置为 true，意为当前左条目找到对应的右条目了 // if the row satisfy the join condition, add its index into the matched set // 匹配到的右条目 index 放入 set 里，避免重复输出 rightMatchedSet.add(idx) joinedRow.copy() } ++ DUMMY_LIST.filter(_ =&gt; !matched).map( _ =&gt; { // 2. For those unmatched records in left, append additional records with empty right. // DUMMY_LIST.filter(_ =&gt; !matched) is a tricky way to add additional row, // as we don't know whether we need to append it until finish iterating all // of the records in right side. // If we didn't get any proper row, then append a single row with empty right. // 当前左条目没有找到对应的右条目，放入一个 NULL joinedRow.withRight(rightNullRow).copy() }) } ++ rightIter.zipWithIndex.collect { // 3. For those unmatched records in right, append additional records with empty left. // Re-visiting the records in right, and append additional row with empty left, if its not // in the matched set. // 对于剩下的（不在之前那个 set 内）的右条目，也放入一个 NULL case (r, idx) if !rightMatchedSet.contains(idx) =&gt; joinedRow(leftNullRow, r).copy() } } else { // key 本身就是个 NULL，那传入的左右侧条目肯定都不能 Join 起来了，直接输出 leftIter.iterator.map[Row] { l =&gt; joinedRow(l, rightNullRow).copy() } ++ rightIter.iterator.map[Row] { r =&gt; joinedRow(leftNullRow, r).copy() } } } // 根据给定的数据以及 key generator，生成&lt;key, Row(s)&gt;映射。同之前的 hashedRelation private[this] def buildHashTable( iter: Iterator[Row], keyGenerator: Projection): JavaHashMap[Row, CompactBuffer[Row]] = { val hashTable = new JavaHashMap[Row, CompactBuffer[Row]]() while (iter.hasNext) { val currentRow = iter.next() val rowKey = keyGenerator(currentRow) var existingMatchList = hashTable.get(rowKey) if (existingMatchList == null) { existingMatchList = new CompactBuffer[Row]() hashTable.put(rowKey, existingMatchList) } existingMatchList += currentRow.copy() } hashTable } protected override def doExecute(): RDD[Row] = { val joinedRow = new JoinedRow() left.execute().zipPartitions(right.execute()) { (leftIter, rightIter) =&gt; // 根据 Join 类型不同分成不同的处理方式 joinType match { case LeftOuter =&gt; // 左外连接和右外连接的代码可以一起看，感觉就是个逻辑上的轴对称而已 233 val rightHashTable = buildHashTable(rightIter, newProjection(rightKeys, right.output)) // 生成右侧的 key row 映射 val keyGenerator = newProjection(leftKeys, left.output) leftIter.flatMap( currentRow =&gt; { val rowKey = keyGenerator(currentRow) joinedRow.withLeft(currentRow) // 它的 right 由 leftOuterIterator 设定 leftOuterIterator(rowKey, joinedRow, rightHashTable.getOrElse(rowKey, EMPTY_LIST)) }) case RightOuter =&gt; val leftHashTable = buildHashTable(leftIter, newProjection(leftKeys, left.output)) val keyGenerator = newProjection(rightKeys, right.output) rightIter.flatMap ( currentRow =&gt; { val rowKey = keyGenerator(currentRow) joinedRow.withRight(currentRow) rightOuterIterator(rowKey, leftHashTable.getOrElse(rowKey, EMPTY_LIST), joinedRow) }) case FullOuter =&gt; val leftHashTable = buildHashTable(leftIter, newProjection(leftKeys, left.output)) val rightHashTable = buildHashTable(rightIter, newProjection(rightKeys, right.output)) (leftHashTable.keySet ++ rightHashTable.keySet).iterator.flatMap { key =&gt; fullOuterIterator(key, leftHashTable.getOrElse(key, EMPTY_LIST), rightHashTable.getOrElse(key, EMPTY_LIST), joinedRow) } case x =&gt; throw new Exception(s&quot;HashOuterJoin should not take $x as the JoinType&quot;) } } }} 看起来有点费劲，写得很是不面向对象，但总体来说并没有什么特别深奥的地方，慢慢看还是可以看得懂的。 CartesianProduct准入条件：Inner Join，有 ON 语句；没有 ON 语句 12345678910111213case class CartesianProduct(left: SparkPlan, right: SparkPlan) extends BinaryNode { override def output: Seq[Attribute] = left.output ++ right.output protected override def doExecute(): RDD[Row] = { val leftResults = left.execute().map(_.copy()) val rightResults = right.execute().map(_.copy()) leftResults.cartesian(rightResults).mapPartitions { iter =&gt; val joinedRow = new JoinedRow iter.map(r =&gt; joinedRow(r._1, r._2)) } }} 其实就是 RDD 的 cartesian 了。 BroadcastNestedLoopJoin准入条件：剩下的所有 Join。以较小一侧为 buildSide。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394case class BroadcastNestedLoopJoin( left: SparkPlan, right: SparkPlan, buildSide: BuildSide, joinType: JoinType, condition: Option[Expression]) extends BinaryNode { // 最后一个，最 General 的 Join Physical Plan 出现了 // ... protected override def doExecute(): RDD[Row] = { // 收集 buildSide 侧的计算结果并广播 val broadcastedRelation = sparkContext.broadcast(broadcast.execute().map(_.copy()).collect().toIndexedSeq) /** All rows that either match both-way, or rows from streamed joined with nulls. */ val matchesOrStreamedRowsWithNulls = streamed.execute().mapPartitions { streamedIter =&gt; val matchedRows = new CompactBuffer[Row] // TODO: Use Spark's BitSet. val includedBroadcastTuples = new scala.collection.mutable.BitSet(broadcastedRelation.value.size) val joinedRow = new JoinedRow val leftNulls = new GenericMutableRow(left.output.size) val rightNulls = new GenericMutableRow(right.output.size) streamedIter.foreach { streamedRow =&gt; var i = 0 var streamRowMatched = false // 找出所有匹配的 Inner Join 条目 while (i &lt; broadcastedRelation.value.size) { // TODO: One bitset per partition instead of per row. val broadcastedRow = broadcastedRelation.value(i) buildSide match { case BuildRight if boundCondition(joinedRow(streamedRow, broadcastedRow)) =&gt; matchedRows += joinedRow(streamedRow, broadcastedRow).copy() streamRowMatched = true includedBroadcastTuples += i // 记录 broadcast 侧已被匹配的条目 case BuildLeft if boundCondition(joinedRow(broadcastedRow, streamedRow)) =&gt; matchedRows += joinedRow(broadcastedRow, streamedRow).copy() streamRowMatched = true includedBroadcastTuples += i case _ =&gt; } i += 1 } // 根据 Join 类型不同决定是否要把无法匹配的条目放进结果集中 (streamRowMatched, joinType, buildSide) match { case (false, LeftOuter | FullOuter, BuildRight) =&gt; matchedRows += joinedRow(streamedRow, rightNulls).copy() case (false, RightOuter | FullOuter, BuildLeft) =&gt; matchedRows += joinedRow(leftNulls, streamedRow).copy() // 这里还有 LeftOuter + BuildLeft 和 RightOuter + BuildRight 的情况没有处理 case _ =&gt; } } Iterator((matchedRows, includedBroadcastTuples)) } val includedBroadcastTuples = matchesOrStreamedRowsWithNulls.map(_._2) val allIncludedBroadcastTuples = // 所有已被匹配的 broadcast 侧条目 if (includedBroadcastTuples.count == 0) { new scala.collection.mutable.BitSet(broadcastedRelation.value.size) } else { includedBroadcastTuples.reduce(_ ++ _) } val leftNulls = new GenericMutableRow(left.output.size) val rightNulls = new GenericMutableRow(right.output.size) /** Rows from broadcasted joined with nulls. */ val broadcastRowsWithNulls: Seq[Row] = { // 由 broadcast 侧未匹配条目与 NULL 组成的 Row val buf: CompactBuffer[Row] = new CompactBuffer() var i = 0 val rel = broadcastedRelation.value while (i &lt; rel.length) { // 遍历整个 broadcast 侧 if (!allIncludedBroadcastTuples.contains(i)) { (joinType, buildSide) match { // 未匹配的条目根据不同的 Join 类型生成带 NULL 的 Row case (RightOuter | FullOuter, BuildRight) =&gt; buf += new JoinedRow(leftNulls, rel(i)) case (LeftOuter | FullOuter, BuildLeft) =&gt; buf += new JoinedRow(rel(i), rightNulls) case _ =&gt; } } i += 1 } buf.toSeq } // TODO: Breaks lineage. sparkContext.union( matchesOrStreamedRowsWithNulls.flatMap(_._1), sparkContext.makeRDD(broadcastRowsWithNulls)) // 将两个结果集 union 起来并返回 }} 也算是比较直观啦，并没有什么特别神奇的东西。至此，我们就探索完 SparkSQL 为 JOIN 操作设计的 9 种 Physical Plan 了，相信在这个操作上对 SparkSQL 知根知底为以后的工作也能带来莫大的好处。","link":"/sparksql_catalyst_source_8/"}],"tags":[{"name":"分布式存储","slug":"分布式存储","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"},{"name":"分布式系统","slug":"分布式系统","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"},{"name":"Bigtable","slug":"Bigtable","link":"/tags/Bigtable/"},{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"团队协作","slug":"团队协作","link":"/tags/%E5%9B%A2%E9%98%9F%E5%8D%8F%E4%BD%9C/"},{"name":"Go","slug":"Go","link":"/tags/Go/"},{"name":"Groovy","slug":"Groovy","link":"/tags/Groovy/"},{"name":"分布式计算","slug":"分布式计算","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/"},{"name":"MIT 6.824","slug":"MIT-6-824","link":"/tags/MIT-6-824/"},{"name":"Mesos","slug":"Mesos","link":"/tags/Mesos/"},{"name":"集群资源调度","slug":"集群资源调度","link":"/tags/%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/"},{"name":"MongoDB","slug":"MongoDB","link":"/tags/MongoDB/"},{"name":"主从备份","slug":"主从备份","link":"/tags/%E4%B8%BB%E4%BB%8E%E5%A4%87%E4%BB%BD/"},{"name":"分布式共识","slug":"分布式共识","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%85%B1%E8%AF%86/"},{"name":"分布式内存","slug":"分布式内存","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%86%85%E5%AD%98/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"Spark SQL","slug":"Spark-SQL","link":"/tags/Spark-SQL/"},{"name":"SparkSQL","slug":"SparkSQL","link":"/tags/SparkSQL/"},{"name":"Hive ThriftServer","slug":"Hive-ThriftServer","link":"/tags/Hive-ThriftServer/"},{"name":"Raft","slug":"Raft","link":"/tags/Raft/"},{"name":"Paxos","slug":"Paxos","link":"/tags/Paxos/"},{"name":"Windows","slug":"Windows","link":"/tags/Windows/"},{"name":"Yarn","slug":"Yarn","link":"/tags/Yarn/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"ZooKeeper","slug":"ZooKeeper","link":"/tags/ZooKeeper/"},{"name":"etcd","slug":"etcd","link":"/tags/etcd/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"GC","slug":"GC","link":"/tags/GC/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Bash","slug":"Bash","link":"/tags/Bash/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"Gradle","slug":"Gradle","link":"/tags/Gradle/"}],"categories":[{"name":"分布式系统","slug":"分布式系统","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"Go","slug":"Go","link":"/categories/Go/"},{"name":"Groovy","slug":"Groovy","link":"/categories/Groovy/"},{"name":"MongoDB","slug":"MongoDB","link":"/categories/MongoDB/"},{"name":"Spark","slug":"Spark","link":"/categories/Spark/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Gradle","slug":"Gradle","link":"/categories/Gradle/"}]}